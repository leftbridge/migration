diff -Nurp linux/arch/alpha/include/asm/Kbuild sched-deadline-mainline-dl/arch/alpha/include/asm/Kbuild
--- linux/arch/alpha/include/asm/Kbuild	2014-02-17 14:42:19.674380785 -0500
+++ sched-deadline-mainline-dl/arch/alpha/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -11,3 +11,4 @@ header-y += reg.h
 header-y += regdef.h
 header-y += sysinfo.h
 generic-y += exec.h
+generic-y += math128.h
diff -Nurp linux/arch/alpha/kernel/osf_sys.c sched-deadline-mainline-dl/arch/alpha/kernel/osf_sys.c
--- linux/arch/alpha/kernel/osf_sys.c	2014-02-17 14:42:20.456400031 -0500
+++ sched-deadline-mainline-dl/arch/alpha/kernel/osf_sys.c	2012-11-21 15:11:30.000000000 -0500
@@ -445,7 +445,7 @@ struct procfs_args {
  * unhappy with OSF UFS. [CHECKME]
  */
 static int
-osf_ufs_mount(char *dirname, struct ufs_args __user *args, int flags)
+osf_ufs_mount(const char *dirname, struct ufs_args __user *args, int flags)
 {
 	int retval;
 	struct cdfs_args tmp;
@@ -465,7 +465,7 @@ osf_ufs_mount(char *dirname, struct ufs_
 }
 
 static int
-osf_cdfs_mount(char *dirname, struct cdfs_args __user *args, int flags)
+osf_cdfs_mount(const char *dirname, struct cdfs_args __user *args, int flags)
 {
 	int retval;
 	struct cdfs_args tmp;
@@ -485,7 +485,7 @@ osf_cdfs_mount(char *dirname, struct cdf
 }
 
 static int
-osf_procfs_mount(char *dirname, struct procfs_args __user *args, int flags)
+osf_procfs_mount(const char *dirname, struct procfs_args __user *args, int flags)
 {
 	struct procfs_args tmp;
 
diff -Nurp linux/arch/arm/include/asm/Kbuild sched-deadline-mainline-dl/arch/arm/include/asm/Kbuild
--- linux/arch/arm/include/asm/Kbuild	2014-02-17 14:42:23.673479204 -0500
+++ sched-deadline-mainline-dl/arch/arm/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -33,3 +33,4 @@ generic-y += termios.h
 generic-y += timex.h
 generic-y += types.h
 generic-y += unaligned.h
+generic-y += math128.h
diff -Nurp linux/arch/arm/include/asm/unistd.h sched-deadline-mainline-dl/arch/arm/include/asm/unistd.h
--- linux/arch/arm/include/asm/unistd.h	2014-02-17 14:42:24.094489565 -0500
+++ sched-deadline-mainline-dl/arch/arm/include/asm/unistd.h	2012-11-21 15:11:30.000000000 -0500
@@ -15,7 +15,7 @@
 
 #include <uapi/asm/unistd.h>
 
-#define __NR_syscalls  (380)
+#define __NR_syscalls  (384)
 #define __ARM_NR_cmpxchg		(__ARM_NR_BASE+0x00fff0)
 
 #define __ARCH_WANT_STAT64
diff -Nurp linux/arch/arm/include/uapi/asm/unistd.h sched-deadline-mainline-dl/arch/arm/include/uapi/asm/unistd.h
--- linux/arch/arm/include/uapi/asm/unistd.h	2014-02-17 14:42:24.236493059 -0500
+++ sched-deadline-mainline-dl/arch/arm/include/uapi/asm/unistd.h	2012-11-21 15:11:30.000000000 -0500
@@ -405,6 +405,9 @@
 #define __NR_process_vm_readv		(__NR_SYSCALL_BASE+376)
 #define __NR_process_vm_writev		(__NR_SYSCALL_BASE+377)
 					/* 378 for kcmp */
+#define __NR_sched_setscheduler2	(__NR_SYSCALL_BASE+379)
+#define __NR_sched_setparam2		(__NR_SYSCALL_BASE+380)
+#define __NR_sched_getparam2		(__NR_SYSCALL_BASE+381)
 
 /*
  * This may need to be greater than __NR_last_syscall+1 in order to
diff -Nurp linux/arch/arm/kernel/calls.S sched-deadline-mainline-dl/arch/arm/kernel/calls.S
--- linux/arch/arm/kernel/calls.S	2014-02-17 14:42:24.271493921 -0500
+++ sched-deadline-mainline-dl/arch/arm/kernel/calls.S	2012-11-21 15:11:30.000000000 -0500
@@ -388,6 +388,9 @@
 		CALL(sys_process_vm_readv)
 		CALL(sys_process_vm_writev)
 		CALL(sys_ni_syscall)	/* reserved for sys_kcmp */
+		CALL(sys_sched_setscheduler2)
+/* 380 */	CALL(sys_sched_setparam2)
+		CALL(sys_sched_getparam2)
 #ifndef syscalls_counted
 .equ syscalls_padding, ((NR_syscalls + 3) & ~3) - NR_syscalls
 #define syscalls_counted
diff -Nurp linux/arch/avr32/include/asm/Kbuild sched-deadline-mainline-dl/arch/avr32/include/asm/Kbuild
--- linux/arch/avr32/include/asm/Kbuild	2014-02-17 14:42:44.901001440 -0500
+++ sched-deadline-mainline-dl/arch/avr32/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -1,3 +1,4 @@
 
 generic-y	+= clkdev.h
 generic-y	+= exec.h
+generic-y += math128.h
diff -Nurp linux/arch/blackfin/include/asm/Kbuild sched-deadline-mainline-dl/arch/blackfin/include/asm/Kbuild
--- linux/arch/blackfin/include/asm/Kbuild	2014-02-17 14:42:46.983052646 -0500
+++ sched-deadline-mainline-dl/arch/blackfin/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -47,3 +47,4 @@ generic-y += xor.h
 header-y += bfin_sport.h
 header-y += cachectl.h
 header-y += fixed_code.h
+generic-y += math128.h
diff -Nurp linux/arch/c6x/include/asm/Kbuild sched-deadline-mainline-dl/arch/c6x/include/asm/Kbuild
--- linux/arch/c6x/include/asm/Kbuild	2014-02-17 14:42:55.740267997 -0500
+++ sched-deadline-mainline-dl/arch/c6x/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -53,3 +53,4 @@ generic-y += types.h
 generic-y += ucontext.h
 generic-y += user.h
 generic-y += vga.h
+generic-y += math128.h
diff -Nurp linux/arch/cris/include/asm/Kbuild sched-deadline-mainline-dl/arch/cris/include/asm/Kbuild
--- linux/arch/cris/include/asm/Kbuild	2014-02-17 14:43:02.197426758 -0500
+++ sched-deadline-mainline-dl/arch/cris/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -11,3 +11,4 @@ header-y += sync_serial.h
 generic-y += clkdev.h
 generic-y += exec.h
 generic-y += module.h
+generic-y += math128.h
diff -Nurp linux/arch/frv/include/asm/Kbuild sched-deadline-mainline-dl/arch/frv/include/asm/Kbuild
--- linux/arch/frv/include/asm/Kbuild	2014-02-17 14:43:04.946494342 -0500
+++ sched-deadline-mainline-dl/arch/frv/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -1,3 +1,4 @@
 
 generic-y += clkdev.h
 generic-y += exec.h
+generic-y += math128.h
diff -Nurp linux/arch/h8300/include/asm/Kbuild sched-deadline-mainline-dl/arch/h8300/include/asm/Kbuild
--- linux/arch/h8300/include/asm/Kbuild	2014-02-17 14:43:06.673536799 -0500
+++ sched-deadline-mainline-dl/arch/h8300/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -3,3 +3,4 @@ include include/asm-generic/Kbuild.asm
 generic-y += clkdev.h
 generic-y += exec.h
 generic-y += module.h
+generic-y += math128.h
diff -Nurp linux/arch/hexagon/include/asm/Kbuild sched-deadline-mainline-dl/arch/hexagon/include/asm/Kbuild
--- linux/arch/hexagon/include/asm/Kbuild	2014-02-17 14:43:13.017692746 -0500
+++ sched-deadline-mainline-dl/arch/hexagon/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -52,3 +52,4 @@ generic-y += types.h
 generic-y += ucontext.h
 generic-y += unaligned.h
 generic-y += xor.h
+generic-y += math128.h
diff -Nurp linux/arch/ia64/include/asm/Kbuild sched-deadline-mainline-dl/arch/ia64/include/asm/Kbuild
--- linux/arch/ia64/include/asm/Kbuild	2014-02-17 14:43:14.123719931 -0500
+++ sched-deadline-mainline-dl/arch/ia64/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -2,3 +2,4 @@
 generic-y += clkdev.h
 generic-y += exec.h
 generic-y += kvm_para.h
+generic-y += math128.h
diff -Nurp linux/arch/m32r/include/asm/Kbuild sched-deadline-mainline-dl/arch/m32r/include/asm/Kbuild
--- linux/arch/m32r/include/asm/Kbuild	2014-02-17 14:43:15.844762232 -0500
+++ sched-deadline-mainline-dl/arch/m32r/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -3,3 +3,4 @@ include include/asm-generic/Kbuild.asm
 generic-y += clkdev.h
 generic-y += exec.h
 generic-y += module.h
+generic-y += math128.h
diff -Nurp linux/arch/m68k/include/asm/Kbuild sched-deadline-mainline-dl/arch/m68k/include/asm/Kbuild
--- linux/arch/m68k/include/asm/Kbuild	2014-02-17 14:43:17.577804826 -0500
+++ sched-deadline-mainline-dl/arch/m68k/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -27,3 +27,4 @@ generic-y += topology.h
 generic-y += types.h
 generic-y += word-at-a-time.h
 generic-y += xor.h
+generic-y += math128.h
diff -Nurp linux/arch/m68k/include/asm/signal.h sched-deadline-mainline-dl/arch/m68k/include/asm/signal.h
--- linux/arch/m68k/include/asm/signal.h	2014-02-17 14:43:18.599829945 -0500
+++ sched-deadline-mainline-dl/arch/m68k/include/asm/signal.h	2012-11-21 15:11:30.000000000 -0500
@@ -41,7 +41,7 @@ struct k_sigaction {
 static inline void sigaddset(sigset_t *set, int _sig)
 {
 	asm ("bfset %0{%1,#1}"
-		: "+od" (*set)
+		: "+o" (*set)
 		: "id" ((_sig - 1) ^ 31)
 		: "cc");
 }
@@ -49,7 +49,7 @@ static inline void sigaddset(sigset_t *s
 static inline void sigdelset(sigset_t *set, int _sig)
 {
 	asm ("bfclr %0{%1,#1}"
-		: "+od" (*set)
+		: "+o" (*set)
 		: "id" ((_sig - 1) ^ 31)
 		: "cc");
 }
@@ -65,7 +65,7 @@ static inline int __gen_sigismember(sigs
 	int ret;
 	asm ("bfextu %1{%2,#1},%0"
 		: "=d" (ret)
-		: "od" (*set), "id" ((_sig-1) ^ 31)
+		: "o" (*set), "id" ((_sig-1) ^ 31)
 		: "cc");
 	return ret;
 }
diff -Nurp linux/arch/microblaze/include/asm/Kbuild sched-deadline-mainline-dl/arch/microblaze/include/asm/Kbuild
--- linux/arch/microblaze/include/asm/Kbuild	2014-02-17 14:43:20.350872980 -0500
+++ sched-deadline-mainline-dl/arch/microblaze/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -3,3 +3,4 @@ include include/asm-generic/Kbuild.asm
 header-y  += elf.h
 generic-y += clkdev.h
 generic-y += exec.h
+generic-y += math128.h
diff -Nurp linux/arch/mips/include/asm/Kbuild sched-deadline-mainline-dl/arch/mips/include/asm/Kbuild
--- linux/arch/mips/include/asm/Kbuild	2014-02-17 14:43:23.480949903 -0500
+++ sched-deadline-mainline-dl/arch/mips/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -1 +1,3 @@
 # MIPS headers
+
+generic-y += math128.h
diff -Nurp linux/arch/mn10300/include/asm/Kbuild sched-deadline-mainline-dl/arch/mn10300/include/asm/Kbuild
--- linux/arch/mn10300/include/asm/Kbuild	2014-02-17 14:43:29.454096683 -0500
+++ sched-deadline-mainline-dl/arch/mn10300/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -1,3 +1,4 @@
 
 generic-y += clkdev.h
 generic-y += exec.h
+generic-y += math128.h
diff -Nurp linux/arch/openrisc/include/asm/Kbuild sched-deadline-mainline-dl/arch/openrisc/include/asm/Kbuild
--- linux/arch/openrisc/include/asm/Kbuild	2014-02-17 14:43:29.546098944 -0500
+++ sched-deadline-mainline-dl/arch/openrisc/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -64,3 +64,4 @@ generic-y += types.h
 generic-y += ucontext.h
 generic-y += user.h
 generic-y += word-at-a-time.h
+generic-y += math128.h
diff -Nurp linux/arch/parisc/include/asm/Kbuild sched-deadline-mainline-dl/arch/parisc/include/asm/Kbuild
--- linux/arch/parisc/include/asm/Kbuild	2014-02-17 14:43:29.647101426 -0500
+++ sched-deadline-mainline-dl/arch/parisc/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -2,4 +2,4 @@
 generic-y += word-at-a-time.h auxvec.h user.h cputime.h emergency-restart.h \
 	  segment.h topology.h vga.h device.h percpu.h hw_irq.h mutex.h \
 	  div64.h irq_regs.h kdebug.h kvm_para.h local64.h local.h param.h \
-	  poll.h xor.h clkdev.h exec.h
+	  poll.h xor.h clkdev.h exec.h math128.h
diff -Nurp linux/arch/powerpc/include/asm/Kbuild sched-deadline-mainline-dl/arch/powerpc/include/asm/Kbuild
--- linux/arch/powerpc/include/asm/Kbuild	2014-02-17 14:43:30.708127497 -0500
+++ sched-deadline-mainline-dl/arch/powerpc/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -2,3 +2,4 @@
 
 generic-y += clkdev.h
 generic-y += rwsem.h
+generic-y += math128.h
diff -Nurp linux/arch/s390/include/asm/Kbuild sched-deadline-mainline-dl/arch/s390/include/asm/Kbuild
--- linux/arch/s390/include/asm/Kbuild	2014-02-17 14:43:35.617248117 -0500
+++ sched-deadline-mainline-dl/arch/s390/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -1,3 +1,4 @@
 
 
 generic-y += clkdev.h
+generic-y += math128.h
diff -Nurp linux/arch/score/include/asm/Kbuild sched-deadline-mainline-dl/arch/score/include/asm/Kbuild
--- linux/arch/score/include/asm/Kbuild	2014-02-17 14:43:36.180261950 -0500
+++ sched-deadline-mainline-dl/arch/score/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -3,3 +3,4 @@ include include/asm-generic/Kbuild.asm
 header-y +=
 
 generic-y += clkdev.h
+generic-y += math128.h
diff -Nurp linux/arch/sh/include/asm/Kbuild sched-deadline-mainline-dl/arch/sh/include/asm/Kbuild
--- linux/arch/sh/include/asm/Kbuild	2014-02-17 14:43:36.370266618 -0500
+++ sched-deadline-mainline-dl/arch/sh/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -33,3 +33,4 @@ generic-y += termbits.h
 generic-y += termios.h
 generic-y += ucontext.h
 generic-y += xor.h
+generic-y += math128.h
diff -Nurp linux/arch/sparc/include/asm/Kbuild sched-deadline-mainline-dl/arch/sparc/include/asm/Kbuild
--- linux/arch/sparc/include/asm/Kbuild	2014-02-17 14:43:37.105284677 -0500
+++ sched-deadline-mainline-dl/arch/sparc/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -9,3 +9,4 @@ generic-y += irq_regs.h
 generic-y += local.h
 generic-y += module.h
 generic-y += word-at-a-time.h
+generic-y += math128.h
diff -Nurp linux/arch/tile/include/asm/Kbuild sched-deadline-mainline-dl/arch/tile/include/asm/Kbuild
--- linux/arch/tile/include/asm/Kbuild	2014-02-17 14:43:38.031307428 -0500
+++ sched-deadline-mainline-dl/arch/tile/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -36,3 +36,4 @@ generic-y += termbits.h
 generic-y += termios.h
 generic-y += types.h
 generic-y += xor.h
+generic-y += math128.h
diff -Nurp linux/arch/um/include/asm/Kbuild sched-deadline-mainline-dl/arch/um/include/asm/Kbuild
--- linux/arch/um/include/asm/Kbuild	2014-02-17 14:43:39.327339269 -0500
+++ sched-deadline-mainline-dl/arch/um/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -1,4 +1,4 @@
 generic-y += bug.h cputime.h device.h emergency-restart.h futex.h hardirq.h
 generic-y += hw_irq.h irq_regs.h kdebug.h percpu.h sections.h topology.h xor.h
 generic-y += ftrace.h pci.h io.h param.h delay.h mutex.h current.h exec.h
-generic-y += switch_to.h clkdev.h
+generic-y += switch_to.h clkdev.h math128.h
diff -Nurp linux/arch/unicore32/include/asm/Kbuild sched-deadline-mainline-dl/arch/unicore32/include/asm/Kbuild
--- linux/arch/unicore32/include/asm/Kbuild	2014-02-17 14:43:39.603346050 -0500
+++ sched-deadline-mainline-dl/arch/unicore32/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -59,3 +59,4 @@ generic-y += unaligned.h
 generic-y += user.h
 generic-y += vga.h
 generic-y += xor.h
+generic-y += math128.h
diff -Nurp linux/arch/x86/include/asm/Kbuild sched-deadline-mainline-dl/arch/x86/include/asm/Kbuild
--- linux/arch/x86/include/asm/Kbuild	2014-02-17 14:43:40.143359317 -0500
+++ sched-deadline-mainline-dl/arch/x86/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -28,3 +28,4 @@ genhdr-y += unistd_64.h
 genhdr-y += unistd_x32.h
 
 generic-y += clkdev.h
+generic-y += math128.h
diff -Nurp linux/arch/x86/include/asm/math128.h sched-deadline-mainline-dl/arch/x86/include/asm/math128.h
--- linux/arch/x86/include/asm/math128.h	1969-12-31 19:00:00.000000000 -0500
+++ sched-deadline-mainline-dl/arch/x86/include/asm/math128.h	2012-11-21 15:11:30.000000000 -0500
@@ -0,0 +1,39 @@
+#ifndef _ASM_MATH128_H
+#define _ASM_MATH128_H
+
+#ifdef CONFIG_X86_64
+
+#ifdef __SIZEOF_INT128__
+#define ARCH_HAS_INT128
+#endif
+
+#ifndef ARCH_HAS_INT128
+
+static inline u128 mul_u64_u64(u64 a, u64 b)
+{
+       u128 res;
+
+       asm("mulq %2"
+               : "=a" (res.lo), "=d" (res.hi)
+               :  "rm" (b), "0" (a));
+
+       return res;
+}
+#define mul_u64_u64 mul_u64_u64
+
+static inline u128 add_u128(u128 a, u128 b)
+{
+       u128 res;
+
+       asm("addq %2,%0;\n"
+           "adcq %3,%1;\n"
+               : "=rm" (res.lo), "=rm" (res.hi)
+               : "r" (b.lo), "r" (b.hi), "0" (a.lo), "1" (a.hi));
+
+       return res;
+}
+#define add_u128 add_u128
+
+#endif /* ARCH_HAS_INT128 */
+#endif /* CONFIG_X86_64 */
+#endif /* _ASM_MATH128_H */
diff -Nurp linux/arch/x86/syscalls/syscall_32.tbl sched-deadline-mainline-dl/arch/x86/syscalls/syscall_32.tbl
--- linux/arch/x86/syscalls/syscall_32.tbl	2014-02-17 14:43:42.424415357 -0500
+++ sched-deadline-mainline-dl/arch/x86/syscalls/syscall_32.tbl	2012-11-21 15:11:30.000000000 -0500
@@ -356,3 +356,6 @@
 347	i386	process_vm_readv	sys_process_vm_readv		compat_sys_process_vm_readv
 348	i386	process_vm_writev	sys_process_vm_writev		compat_sys_process_vm_writev
 349	i386	kcmp			sys_kcmp
+350	i386	sched_setparam2		sys_sched_setparam2
+351	i386	sched_getparam2		sys_sched_getparam2
+352	i386	sched_setscheduler2	sys_sched_setscheduler2
diff -Nurp linux/arch/x86/syscalls/syscall_64.tbl sched-deadline-mainline-dl/arch/x86/syscalls/syscall_64.tbl
--- linux/arch/x86/syscalls/syscall_64.tbl	2014-02-17 14:43:42.424415357 -0500
+++ sched-deadline-mainline-dl/arch/x86/syscalls/syscall_64.tbl	2012-11-21 15:11:30.000000000 -0500
@@ -319,7 +319,9 @@
 310	64	process_vm_readv	sys_process_vm_readv
 311	64	process_vm_writev	sys_process_vm_writev
 312	common	kcmp			sys_kcmp
-
+313	common	sched_setparam2		sys_sched_setparam2
+314	common	sched_getparam2		sys_sched_getparam2
+315	common	sched_setscheduler2	sys_sched_setscheduler2
 #
 # x32-specific system call numbers start at 512 to avoid cache impact
 # for native 64-bit operation.
diff -Nurp linux/arch/xtensa/include/asm/Kbuild sched-deadline-mainline-dl/arch/xtensa/include/asm/Kbuild
--- linux/arch/xtensa/include/asm/Kbuild	2014-02-17 14:43:42.845425699 -0500
+++ sched-deadline-mainline-dl/arch/xtensa/include/asm/Kbuild	2012-11-21 15:11:30.000000000 -0500
@@ -26,3 +26,4 @@ generic-y += statfs.h
 generic-y += termios.h
 generic-y += topology.h
 generic-y += xor.h
+generic-y += math128.h
diff -Nurp linux/Documentation/scheduler/sched-deadline.txt sched-deadline-mainline-dl/Documentation/scheduler/sched-deadline.txt
--- linux/Documentation/scheduler/sched-deadline.txt	1969-12-31 19:00:00.000000000 -0500
+++ sched-deadline-mainline-dl/Documentation/scheduler/sched-deadline.txt	2012-11-21 15:11:30.000000000 -0500
@@ -0,0 +1,164 @@
+			Deadline Task and Group Scheduling
+			----------------------------------
+
+CONTENTS
+========
+
+0. WARNING
+1. Overview
+2. Task scheduling
+2. The Interface
+3. Bandwidth management
+  3.1 System-wide settings
+  2.2 Task interface
+  2.4 Default behavior
+3. Future plans
+
+
+0. WARNING
+==========
+
+ Fiddling with these settings can result in an unpredictable or even unstable
+ system behavior. As for -rt (group) scheduling, it is assumed that root users
+ know what they're doing.
+
+
+1. Overview
+===========
+
+ The SCHED_DEADLINE policy contained inside the sched_dl scheduling class is
+ basically an implementation of the Earliest Deadline First (EDF) scheduling
+ algorithm, augmented with a mechanism (called Constant Bandwidth Server, CBS)
+ that makes it possible to isolate the behavior of tasks between each other.
+
+
+2. Task scheduling
+==================
+
+ The typical -deadline task is composed of a computation phase (instance)
+ which is activated on a periodic or sporadic fashion. The expected (maximum)
+ duration of such computation is called the task's runtime; the time interval
+ by which each instance needs to be completed is called the task's relative
+ deadline. The task's absolute deadline is dynamically calculated as the
+ time instant a task (or, more properly) activates plus the relative
+ deadline.
+
+ The EDF[1] algorithm selects the task with the smallest absolute deadline as
+ the one to be executed first, while the CBS[2,3] ensures that each task runs
+ for at most its runtime every period, avoiding any interference between
+ different tasks (bandwidth isolation).
+ Thanks to this feature, also tasks that do not strictly comply with the
+ computational model described above can effectively use the new policy.
+ IOW, there are no limitations on what kind of task can exploit this new
+ scheduling discipline, even if it must be said that it is particularly
+ suited for periodic or sporadic tasks that need guarantees on their
+ timing behavior, e.g., multimedia, streaming, control applications, etc.
+
+ References:
+  1 - C. L. Liu and J. W. Layland. Scheduling algorithms for multiprogram-
+      ming in a hard-real-time environment. Journal of the Association for
+      Computing Machinery, 20(1), 1973.
+  2 - L. Abeni , G. Buttazzo. Integrating Multimedia Applications in Hard
+      Real-Time Systems. Proceedings of the 19th IEEE Real-time Systems
+      Symposium, 1998. http://retis.sssup.it/~giorgio/paps/1998/rtss98-cbs.pdf
+  3 - L. Abeni. Server Mechanisms for Multimedia Applications. ReTiS Lab
+      Technical Report. http://xoomer.virgilio.it/lucabe72/pubs/tr-98-01.ps
+
+3. Bandwidth management
+=======================
+
+ In order for the -deadline scheduling to be effective and useful, it is
+ important to have some method to keep the allocation of the available CPU
+ bandwidth to the tasks under control.
+ This is usually called "admission control" and if it is not performed at all,
+ no guarantee can be given on the actual scheduling of the -deadline tasks.
+
+ Since when RT-throttling has been introduced each task group has a bandwidth
+ associated, calculated as a certain amount of runtime over a period.
+ Moreover, to make it possible to manipulate such bandwidth, readable/writable
+ controls have been added to both procfs (for system wide settings) and cgroupfs
+ (for per-group settings).
+ Therefore, the same interface is being used for controlling the bandwidth
+ distrubution to -deadline tasks and task groups, i.e., new controls but with
+ similar names, equivalent meaning and with the same usage paradigm are added.
+
+ However, more discussion is needed in order to figure out how we want to manage
+ SCHED_DEADLINE bandwidth at the task group level. Therefore, SCHED_DEADLINE
+ uses (for now) a less sophisticated, but actually very sensible, mechanism to
+ ensure that a certain utilization cap is not overcome per each root_domain.
+
+ Another main difference between deadline bandwidth management and RT-throttling
+ is that -deadline tasks have bandwidth on their own (while -rt ones don't!),
+ and thus we don't need an higher level throttling mechanism to enforce the
+ desired bandwidth.
+
+3.1 System wide settings
+------------------------
+
+The system wide settings are configured under the /proc virtual file system:
+
+ The per-group controls that are added to the cgroupfs virtual file system are:
+  * /proc/sys/kernel/sched_dl_runtime_us,
+  * /proc/sys/kernel/sched_dl_period_us,
+
+ They accept (if written) and provides (if read) the new runtime and period,
+ respectively, for each CPU in each root_domain.
+
+ This means that, for a root_domain comprising M CPUs, -deadline tasks
+ can be created until the sum of their bandwidths stay below:
+
+   M * (sched_dl_runtime_us / sched_dl_period_us)
+
+ It is also possible to disable this bandwidth management logic, and
+ be thus free of oversubscribing the system up to any arbitrary level.
+ This is done by writing -1 in /proc/sys/kernel/sched_dl_runtime_us.
+
+
+2.2 Task interface
+------------------
+
+ Specifying a periodic/sporadic task that executes for a given amount of
+ runtime at each instance, and that is scheduled according to the urgency of
+ its own timing constraints needs, in general, a way of declaring:
+  - a (maximum/typical) instance execution time,
+  - a minimum interval between consecutive instances,
+  - a time constraint by which each instance must be completed.
+
+ Therefore:
+  * a new struct sched_param2, containing all the necessary fields is
+    provided;
+  * the new scheduling related syscalls that manipulate it, i.e.,
+    sched_setscheduler2(), sched_setparam2() and sched_getparam2()
+    are implemented.
+
+
+2.4 Default behavior
+---------------------
+
+The default values for SCHED_DEADLINE bandwidth is to have dl_runtime and
+dl_period equal to 500000 and 1000000, respectively. This means -deadline
+tasks can use at most 5%, multiplied by the number of CPUs that compose the
+root_domain, for each root_domain.
+
+When a -deadline task fork a child, its dl_runtime is set to 0, which means
+someone must call sched_setscheduler2() on it, or it won't even start.
+
+
+3. Future plans
+===============
+
+Still missing:
+
+ - refinements to deadline inheritance, especially regarding the possibility
+   of retaining bandwidth isolation among non-interacting tasks. This is
+   being studied from both theoretical and practical points of view, and
+   hopefully we should be able to produce some demonstrative code soon.
+ - (c)group based bandwidth management, and maybe scheduling;
+ - access control for non-root users (and related security concerns to
+   address), which is the best way to allow unprivileged use of the mechanisms
+   and how to prevent non-root users "cheat" the system?
+
+As already discussed, we are planning also to merge this work with the EDF
+throttling patches [https://lkml.org/lkml/2010/2/23/239] but we still are in
+the preliminary phases of the merge and we really seek feedback that would help us
+decide on the direction it should take.
diff -Nurp linux/drivers/ata/ahci_platform.c sched-deadline-mainline-dl/drivers/ata/ahci_platform.c
--- linux/drivers/ata/ahci_platform.c	2014-02-17 14:43:45.353487313 -0500
+++ sched-deadline-mainline-dl/drivers/ata/ahci_platform.c	2012-11-21 15:11:30.000000000 -0500
@@ -238,7 +238,7 @@ static int __devexit ahci_remove(struct
 	return 0;
 }
 
-#ifdef CONFIG_PM
+#ifdef CONFIG_PM_SLEEP
 static int ahci_suspend(struct device *dev)
 {
 	struct ahci_platform_data *pdata = dev_get_platdata(dev);
diff -Nurp linux/drivers/ata/libata-acpi.c sched-deadline-mainline-dl/drivers/ata/libata-acpi.c
--- linux/drivers/ata/libata-acpi.c	2014-02-17 14:43:45.378487927 -0500
+++ sched-deadline-mainline-dl/drivers/ata/libata-acpi.c	2012-11-21 15:11:30.000000000 -0500
@@ -1105,10 +1105,15 @@ static int ata_acpi_bind_device(struct a
 	struct acpi_device *acpi_dev;
 	struct acpi_device_power_state *states;
 
-	if (ap->flags & ATA_FLAG_ACPI_SATA)
-		ata_dev = &ap->link.device[sdev->channel];
-	else
+	if (ap->flags & ATA_FLAG_ACPI_SATA) {
+		if (!sata_pmp_attached(ap))
+			ata_dev = &ap->link.device[sdev->id];
+		else
+			ata_dev = &ap->pmp_link[sdev->channel].device[sdev->id];
+	}
+	else {
 		ata_dev = &ap->link.device[sdev->id];
+	}
 
 	*handle = ata_dev_acpi_handle(ata_dev);
 
diff -Nurp linux/drivers/ata/libata-core.c sched-deadline-mainline-dl/drivers/ata/libata-core.c
--- linux/drivers/ata/libata-core.c	2014-02-17 14:43:45.395488345 -0500
+++ sched-deadline-mainline-dl/drivers/ata/libata-core.c	2012-11-21 15:11:30.000000000 -0500
@@ -2942,6 +2942,10 @@ const struct ata_timing *ata_timing_find
 
 	if (xfer_mode == t->mode)
 		return t;
+
+	WARN_ONCE(true, "%s: unable to find timing for xfer_mode 0x%x\n",
+			__func__, xfer_mode);
+
 	return NULL;
 }
 
diff -Nurp linux/drivers/ata/pata_arasan_cf.c sched-deadline-mainline-dl/drivers/ata/pata_arasan_cf.c
--- linux/drivers/ata/pata_arasan_cf.c	2014-02-17 14:43:45.429489180 -0500
+++ sched-deadline-mainline-dl/drivers/ata/pata_arasan_cf.c	2012-11-21 15:11:30.000000000 -0500
@@ -317,6 +317,12 @@ static int cf_init(struct arasan_cf_dev
 		return ret;
 	}
 
+	ret = clk_set_rate(acdev->clk, 166000000);
+	if (ret) {
+		dev_warn(acdev->host->dev, "clock set rate failed");
+		return ret;
+	}
+
 	spin_lock_irqsave(&acdev->host->lock, flags);
 	/* configure CF interface clock */
 	writel((pdata->cf_if_clk <= CF_IF_CLK_200M) ? pdata->cf_if_clk :
@@ -908,7 +914,7 @@ static int __devexit arasan_cf_remove(st
 	return 0;
 }
 
-#ifdef CONFIG_PM
+#ifdef CONFIG_PM_SLEEP
 static int arasan_cf_suspend(struct device *dev)
 {
 	struct ata_host *host = dev_get_drvdata(dev);
diff -Nurp linux/drivers/ata/sata_highbank.c sched-deadline-mainline-dl/drivers/ata/sata_highbank.c
--- linux/drivers/ata/sata_highbank.c	2014-02-17 14:43:45.652494658 -0500
+++ sched-deadline-mainline-dl/drivers/ata/sata_highbank.c	2012-11-21 15:11:30.000000000 -0500
@@ -260,7 +260,7 @@ static const struct of_device_id ahci_of
 };
 MODULE_DEVICE_TABLE(of, ahci_of_match);
 
-static int __init ahci_highbank_probe(struct platform_device *pdev)
+static int __devinit ahci_highbank_probe(struct platform_device *pdev)
 {
 	struct device *dev = &pdev->dev;
 	struct ahci_host_priv *hpriv;
@@ -378,7 +378,7 @@ static int __devexit ahci_highbank_remov
 	return 0;
 }
 
-#ifdef CONFIG_PM
+#ifdef CONFIG_PM_SLEEP
 static int ahci_highbank_suspend(struct device *dev)
 {
 	struct ata_host *host = dev_get_drvdata(dev);
diff -Nurp linux/drivers/ata/sata_svw.c sched-deadline-mainline-dl/drivers/ata/sata_svw.c
--- linux/drivers/ata/sata_svw.c	2014-02-17 14:43:45.774497655 -0500
+++ sched-deadline-mainline-dl/drivers/ata/sata_svw.c	2012-11-21 15:11:30.000000000 -0500
@@ -142,6 +142,39 @@ static int k2_sata_scr_write(struct ata_
 	return 0;
 }
 
+static int k2_sata_softreset(struct ata_link *link,
+			     unsigned int *class, unsigned long deadline)
+{
+	u8 dmactl;
+	void __iomem *mmio = link->ap->ioaddr.bmdma_addr;
+
+	dmactl = readb(mmio + ATA_DMA_CMD);
+
+	/* Clear the start bit */
+	if (dmactl & ATA_DMA_START) {
+		dmactl &= ~ATA_DMA_START;
+		writeb(dmactl, mmio + ATA_DMA_CMD);
+	}
+
+	return ata_sff_softreset(link, class, deadline);
+}
+
+static int k2_sata_hardreset(struct ata_link *link,
+			     unsigned int *class, unsigned long deadline)
+{
+	u8 dmactl;
+	void __iomem *mmio = link->ap->ioaddr.bmdma_addr;
+
+	dmactl = readb(mmio + ATA_DMA_CMD);
+
+	/* Clear the start bit */
+	if (dmactl & ATA_DMA_START) {
+		dmactl &= ~ATA_DMA_START;
+		writeb(dmactl, mmio + ATA_DMA_CMD);
+	}
+
+	return sata_sff_hardreset(link, class, deadline);
+}
 
 static void k2_sata_tf_load(struct ata_port *ap, const struct ata_taskfile *tf)
 {
@@ -346,6 +379,8 @@ static struct scsi_host_template k2_sata
 
 static struct ata_port_operations k2_sata_ops = {
 	.inherits		= &ata_bmdma_port_ops,
+	.softreset              = k2_sata_softreset,
+	.hardreset              = k2_sata_hardreset,
 	.sff_tf_load		= k2_sata_tf_load,
 	.sff_tf_read		= k2_sata_tf_read,
 	.sff_check_status	= k2_stat_check_status,
diff -Nurp linux/drivers/gpio/gpio-mcp23s08.c sched-deadline-mainline-dl/drivers/gpio/gpio-mcp23s08.c
--- linux/drivers/gpio/gpio-mcp23s08.c	2014-02-17 14:43:50.594616060 -0500
+++ sched-deadline-mainline-dl/drivers/gpio/gpio-mcp23s08.c	2012-11-21 15:11:30.000000000 -0500
@@ -77,7 +77,7 @@ struct mcp23s08_driver_data {
 
 /*----------------------------------------------------------------------*/
 
-#ifdef CONFIG_I2C
+#if IS_ENABLED(CONFIG_I2C)
 
 static int mcp23008_read(struct mcp23s08 *mcp, unsigned reg)
 {
@@ -399,7 +399,7 @@ static int mcp23s08_probe_one(struct mcp
 		break;
 #endif /* CONFIG_SPI_MASTER */
 
-#ifdef CONFIG_I2C
+#if IS_ENABLED(CONFIG_I2C)
 	case MCP_TYPE_008:
 		mcp->ops = &mcp23008_ops;
 		mcp->chip.ngpio = 8;
@@ -473,7 +473,7 @@ fail:
 
 /*----------------------------------------------------------------------*/
 
-#ifdef CONFIG_I2C
+#if IS_ENABLED(CONFIG_I2C)
 
 static int __devinit mcp230xx_probe(struct i2c_client *client,
 				    const struct i2c_device_id *id)
diff -Nurp linux/drivers/gpio/gpio-mvebu.c sched-deadline-mainline-dl/drivers/gpio/gpio-mvebu.c
--- linux/drivers/gpio/gpio-mvebu.c	2014-02-17 14:43:50.643617263 -0500
+++ sched-deadline-mainline-dl/drivers/gpio/gpio-mvebu.c	2012-11-21 15:11:30.000000000 -0500
@@ -92,6 +92,11 @@ static inline void __iomem *mvebu_gpiore
 	return mvchip->membase + GPIO_OUT_OFF;
 }
 
+static inline void __iomem *mvebu_gpioreg_blink(struct mvebu_gpio_chip *mvchip)
+{
+	return mvchip->membase + GPIO_BLINK_EN_OFF;
+}
+
 static inline void __iomem *mvebu_gpioreg_io_conf(struct mvebu_gpio_chip *mvchip)
 {
 	return mvchip->membase + GPIO_IO_CONF_OFF;
@@ -206,6 +211,23 @@ static int mvebu_gpio_get(struct gpio_ch
 	return (u >> pin) & 1;
 }
 
+static void mvebu_gpio_blink(struct gpio_chip *chip, unsigned pin, int value)
+{
+	struct mvebu_gpio_chip *mvchip =
+		container_of(chip, struct mvebu_gpio_chip, chip);
+	unsigned long flags;
+	u32 u;
+
+	spin_lock_irqsave(&mvchip->lock, flags);
+	u = readl_relaxed(mvebu_gpioreg_blink(mvchip));
+	if (value)
+		u |= 1 << pin;
+	else
+		u &= ~(1 << pin);
+	writel_relaxed(u, mvebu_gpioreg_blink(mvchip));
+	spin_unlock_irqrestore(&mvchip->lock, flags);
+}
+
 static int mvebu_gpio_direction_input(struct gpio_chip *chip, unsigned pin)
 {
 	struct mvebu_gpio_chip *mvchip =
@@ -244,6 +266,7 @@ static int mvebu_gpio_direction_output(s
 	if (ret)
 		return ret;
 
+	mvebu_gpio_blink(chip, pin, 0);
 	mvebu_gpio_set(chip, pin, value);
 
 	spin_lock_irqsave(&mvchip->lock, flags);
diff -Nurp linux/drivers/gpio/Kconfig sched-deadline-mainline-dl/drivers/gpio/Kconfig
--- linux/drivers/gpio/Kconfig	2014-02-17 14:43:50.517614168 -0500
+++ sched-deadline-mainline-dl/drivers/gpio/Kconfig	2012-11-21 15:11:30.000000000 -0500
@@ -466,7 +466,7 @@ config GPIO_ADP5588_IRQ
 
 config GPIO_ADNP
 	tristate "Avionic Design N-bit GPIO expander"
-	depends on I2C && OF
+	depends on I2C && OF_GPIO
 	help
 	  This option enables support for N GPIOs found on Avionic Design
 	  I2C GPIO expanders. The register space will be extended by powers
diff -Nurp linux/drivers/net/phy/mdio-mux-mmioreg.c sched-deadline-mainline-dl/drivers/net/phy/mdio-mux-mmioreg.c
--- linux/drivers/net/phy/mdio-mux-mmioreg.c	2014-02-17 14:45:04.223423841 -0500
+++ sched-deadline-mainline-dl/drivers/net/phy/mdio-mux-mmioreg.c	1969-12-31 19:00:00.000000000 -0500
@@ -1,171 +0,0 @@
-/*
- * Simple memory-mapped device MDIO MUX driver
- *
- * Author: Timur Tabi <timur@freescale.com>
- *
- * Copyright 2012 Freescale Semiconductor, Inc.
- *
- * This file is licensed under the terms of the GNU General Public License
- * version 2.  This program is licensed "as is" without any warranty of any
- * kind, whether express or implied.
- */
-
-#include <linux/platform_device.h>
-#include <linux/device.h>
-#include <linux/of_address.h>
-#include <linux/of_mdio.h>
-#include <linux/module.h>
-#include <linux/init.h>
-#include <linux/phy.h>
-#include <linux/mdio-mux.h>
-
-struct mdio_mux_mmioreg_state {
-	void *mux_handle;
-	phys_addr_t phys;
-	uint8_t mask;
-};
-
-/*
- * MDIO multiplexing switch function
- *
- * This function is called by the mdio-mux layer when it thinks the mdio bus
- * multiplexer needs to switch.
- *
- * 'current_child' is the current value of the mux register (masked via
- * s->mask).
- *
- * 'desired_child' is the value of the 'reg' property of the target child MDIO
- * node.
- *
- * The first time this function is called, current_child == -1.
- *
- * If current_child == desired_child, then the mux is already set to the
- * correct bus.
- */
-static int mdio_mux_mmioreg_switch_fn(int current_child, int desired_child,
-				      void *data)
-{
-	struct mdio_mux_mmioreg_state *s = data;
-
-	if (current_child ^ desired_child) {
-		void *p = ioremap(s->phys, 1);
-		uint8_t x, y;
-
-		if (!p)
-			return -ENOMEM;
-
-		x = ioread8(p);
-		y = (x & ~s->mask) | desired_child;
-		if (x != y) {
-			iowrite8((x & ~s->mask) | desired_child, p);
-			pr_debug("%s: %02x -> %02x\n", __func__, x, y);
-		}
-
-		iounmap(p);
-	}
-
-	return 0;
-}
-
-static int __devinit mdio_mux_mmioreg_probe(struct platform_device *pdev)
-{
-	struct device_node *np2, *np = pdev->dev.of_node;
-	struct mdio_mux_mmioreg_state *s;
-	struct resource res;
-	const __be32 *iprop;
-	int len, ret;
-
-	dev_dbg(&pdev->dev, "probing node %s\n", np->full_name);
-
-	s = devm_kzalloc(&pdev->dev, sizeof(*s), GFP_KERNEL);
-	if (!s)
-		return -ENOMEM;
-
-	ret = of_address_to_resource(np, 0, &res);
-	if (ret) {
-		dev_err(&pdev->dev, "could not obtain memory map for node %s\n",
-			np->full_name);
-		return ret;
-	}
-	s->phys = res.start;
-
-	if (resource_size(&res) != sizeof(uint8_t)) {
-		dev_err(&pdev->dev, "only 8-bit registers are supported\n");
-		return -EINVAL;
-	}
-
-	iprop = of_get_property(np, "mux-mask", &len);
-	if (!iprop || len != sizeof(uint32_t)) {
-		dev_err(&pdev->dev, "missing or invalid mux-mask property\n");
-		return -ENODEV;
-	}
-	if (be32_to_cpup(iprop) > 255) {
-		dev_err(&pdev->dev, "only 8-bit registers are supported\n");
-		return -EINVAL;
-	}
-	s->mask = be32_to_cpup(iprop);
-
-	/*
-	 * Verify that the 'reg' property of each child MDIO bus does not
-	 * set any bits outside of the 'mask'.
-	 */
-	for_each_available_child_of_node(np, np2) {
-		iprop = of_get_property(np2, "reg", &len);
-		if (!iprop || len != sizeof(uint32_t)) {
-			dev_err(&pdev->dev, "mdio-mux child node %s is "
-				"missing a 'reg' property\n", np2->full_name);
-			return -ENODEV;
-		}
-		if (be32_to_cpup(iprop) & ~s->mask) {
-			dev_err(&pdev->dev, "mdio-mux child node %s has "
-				"a 'reg' value with unmasked bits\n",
-				np2->full_name);
-			return -ENODEV;
-		}
-	}
-
-	ret = mdio_mux_init(&pdev->dev, mdio_mux_mmioreg_switch_fn,
-			    &s->mux_handle, s);
-	if (ret) {
-		dev_err(&pdev->dev, "failed to register mdio-mux bus %s\n",
-			np->full_name);
-		return ret;
-	}
-
-	pdev->dev.platform_data = s;
-
-	return 0;
-}
-
-static int __devexit mdio_mux_mmioreg_remove(struct platform_device *pdev)
-{
-	struct mdio_mux_mmioreg_state *s = dev_get_platdata(&pdev->dev);
-
-	mdio_mux_uninit(s->mux_handle);
-
-	return 0;
-}
-
-static struct of_device_id mdio_mux_mmioreg_match[] = {
-	{
-		.compatible = "mdio-mux-mmioreg",
-	},
-	{},
-};
-MODULE_DEVICE_TABLE(of, mdio_mux_mmioreg_match);
-
-static struct platform_driver mdio_mux_mmioreg_driver = {
-	.driver = {
-		.name		= "mdio-mux-mmioreg",
-		.owner		= THIS_MODULE,
-		.of_match_table = mdio_mux_mmioreg_match,
-	},
-	.probe		= mdio_mux_mmioreg_probe,
-	.remove		= __devexit_p(mdio_mux_mmioreg_remove),
-};
-
-module_platform_driver(mdio_mux_mmioreg_driver);
-
-MODULE_AUTHOR("Timur Tabi <timur@freescale.com>");
-MODULE_DESCRIPTION("Memory-mapped device MDIO MUX driver");
-MODULE_LICENSE("GPL v2");
diff -Nurp linux/fs/file.c sched-deadline-mainline-dl/fs/file.c
--- linux/fs/file.c	2014-02-17 14:45:45.338432789 -0500
+++ sched-deadline-mainline-dl/fs/file.c	2012-11-21 15:11:30.000000000 -0500
@@ -685,7 +685,6 @@ void do_close_on_exec(struct files_struc
 	struct fdtable *fdt;
 
 	/* exec unshares first */
-	BUG_ON(atomic_read(&files->count) != 1);
 	spin_lock(&files->file_lock);
 	for (i = 0; ; i++) {
 		unsigned long set;
diff -Nurp linux/fs/notify/fanotify/fanotify_user.c sched-deadline-mainline-dl/fs/notify/fanotify/fanotify_user.c
--- linux/fs/notify/fanotify/fanotify_user.c	2014-02-17 14:45:47.163477569 -0500
+++ sched-deadline-mainline-dl/fs/notify/fanotify/fanotify_user.c	2012-11-21 15:11:30.000000000 -0500
@@ -258,7 +258,8 @@ static ssize_t copy_event_to_user(struct
 	if (ret)
 		goto out_close_fd;
 
-	fd_install(fd, f);
+	if (fd != FAN_NOFD)
+		fd_install(fd, f);
 	return fanotify_event_metadata.event_len;
 
 out_close_fd:
diff -Nurp linux/fs/xfs/xfs_aops.c sched-deadline-mainline-dl/fs/xfs/xfs_aops.c
--- linux/fs/xfs/xfs_aops.c	2014-02-17 14:45:48.315505835 -0500
+++ sched-deadline-mainline-dl/fs/xfs/xfs_aops.c	2012-11-21 15:11:30.000000000 -0500
@@ -481,11 +481,17 @@ static inline int bio_add_buffer(struct
  *
  * The fix is two passes across the ioend list - one to start writeback on the
  * buffer_heads, and then submit them for I/O on the second pass.
+ *
+ * If @fail is non-zero, it means that we have a situation where some part of
+ * the submission process has failed after we have marked paged for writeback
+ * and unlocked them. In this situation, we need to fail the ioend chain rather
+ * than submit it to IO. This typically only happens on a filesystem shutdown.
  */
 STATIC void
 xfs_submit_ioend(
 	struct writeback_control *wbc,
-	xfs_ioend_t		*ioend)
+	xfs_ioend_t		*ioend,
+	int			fail)
 {
 	xfs_ioend_t		*head = ioend;
 	xfs_ioend_t		*next;
@@ -506,6 +512,18 @@ xfs_submit_ioend(
 		next = ioend->io_list;
 		bio = NULL;
 
+		/*
+		 * If we are failing the IO now, just mark the ioend with an
+		 * error and finish it. This will run IO completion immediately
+		 * as there is only one reference to the ioend at this point in
+		 * time.
+		 */
+		if (fail) {
+			ioend->io_error = -fail;
+			xfs_finish_ioend(ioend);
+			continue;
+		}
+
 		for (bh = ioend->io_buffer_head; bh; bh = bh->b_private) {
 
 			if (!bio) {
@@ -1060,7 +1078,18 @@ xfs_vm_writepage(
 
 	xfs_start_page_writeback(page, 1, count);
 
-	if (ioend && imap_valid) {
+	/* if there is no IO to be submitted for this page, we are done */
+	if (!ioend)
+		return 0;
+
+	ASSERT(iohead);
+
+	/*
+	 * Any errors from this point onwards need tobe reported through the IO
+	 * completion path as we have marked the initial page as under writeback
+	 * and unlocked it.
+	 */
+	if (imap_valid) {
 		xfs_off_t		end_index;
 
 		end_index = imap.br_startoff + imap.br_blockcount;
@@ -1079,20 +1108,15 @@ xfs_vm_writepage(
 				  wbc, end_index);
 	}
 
-	if (iohead) {
-		/*
-		 * Reserve log space if we might write beyond the on-disk
-		 * inode size.
-		 */
-		if (ioend->io_type != XFS_IO_UNWRITTEN &&
-		    xfs_ioend_is_append(ioend)) {
-			err = xfs_setfilesize_trans_alloc(ioend);
-			if (err)
-				goto error;
-		}
 
-		xfs_submit_ioend(wbc, iohead);
-	}
+	/*
+	 * Reserve log space if we might write beyond the on-disk inode size.
+	 */
+	err = 0;
+	if (ioend->io_type != XFS_IO_UNWRITTEN && xfs_ioend_is_append(ioend))
+		err = xfs_setfilesize_trans_alloc(ioend);
+
+	xfs_submit_ioend(wbc, iohead, err);
 
 	return 0;
 
diff -Nurp linux/fs/xfs/xfs_attr_leaf.c sched-deadline-mainline-dl/fs/xfs/xfs_attr_leaf.c
--- linux/fs/xfs/xfs_attr_leaf.c	2014-02-17 14:45:48.332506252 -0500
+++ sched-deadline-mainline-dl/fs/xfs/xfs_attr_leaf.c	2012-11-21 15:11:30.000000000 -0500
@@ -1291,6 +1291,7 @@ xfs_attr_leaf_rebalance(xfs_da_state_t *
 	leaf2 = blk2->bp->b_addr;
 	ASSERT(leaf1->hdr.info.magic == cpu_to_be16(XFS_ATTR_LEAF_MAGIC));
 	ASSERT(leaf2->hdr.info.magic == cpu_to_be16(XFS_ATTR_LEAF_MAGIC));
+	ASSERT(leaf2->hdr.count == 0);
 	args = state->args;
 
 	trace_xfs_attr_leaf_rebalance(args);
@@ -1361,6 +1362,7 @@ xfs_attr_leaf_rebalance(xfs_da_state_t *
 		 * I assert that since all callers pass in an empty
 		 * second buffer, this code should never execute.
 		 */
+		ASSERT(0);
 
 		/*
 		 * Figure the total bytes to be added to the destination leaf.
@@ -1422,10 +1424,24 @@ xfs_attr_leaf_rebalance(xfs_da_state_t *
 			args->index2 = 0;
 			args->blkno2 = blk2->blkno;
 		} else {
+			/*
+			 * On a double leaf split, the original attr location
+			 * is already stored in blkno2/index2, so don't
+			 * overwrite it overwise we corrupt the tree.
+			 */
 			blk2->index = blk1->index
 				    - be16_to_cpu(leaf1->hdr.count);
-			args->index = args->index2 = blk2->index;
-			args->blkno = args->blkno2 = blk2->blkno;
+			args->index = blk2->index;
+			args->blkno = blk2->blkno;
+			if (!state->extravalid) {
+				/*
+				 * set the new attr location to match the old
+				 * one and let the higher level split code
+				 * decide where in the leaf to place it.
+				 */
+				args->index2 = blk2->index;
+				args->blkno2 = blk2->blkno;
+			}
 		}
 	} else {
 		ASSERT(state->inleaf == 1);
diff -Nurp linux/fs/xfs/xfs_buf.c sched-deadline-mainline-dl/fs/xfs/xfs_buf.c
--- linux/fs/xfs/xfs_buf.c	2014-02-17 14:45:48.478509834 -0500
+++ sched-deadline-mainline-dl/fs/xfs/xfs_buf.c	2012-11-21 15:11:30.000000000 -0500
@@ -1197,9 +1197,14 @@ xfs_buf_bio_end_io(
 {
 	xfs_buf_t		*bp = (xfs_buf_t *)bio->bi_private;
 
-	xfs_buf_ioerror(bp, -error);
+	/*
+	 * don't overwrite existing errors - otherwise we can lose errors on
+	 * buffers that require multiple bios to complete.
+	 */
+	if (!bp->b_error)
+		xfs_buf_ioerror(bp, -error);
 
-	if (!error && xfs_buf_is_vmapped(bp) && (bp->b_flags & XBF_READ))
+	if (!bp->b_error && xfs_buf_is_vmapped(bp) && (bp->b_flags & XBF_READ))
 		invalidate_kernel_vmap_range(bp->b_addr, xfs_buf_vmap_len(bp));
 
 	_xfs_buf_ioend(bp, 1);
@@ -1279,6 +1284,11 @@ next_chunk:
 		if (size)
 			goto next_chunk;
 	} else {
+		/*
+		 * This is guaranteed not to be the last io reference count
+		 * because the caller (xfs_buf_iorequest) holds a count itself.
+		 */
+		atomic_dec(&bp->b_io_remaining);
 		xfs_buf_ioerror(bp, EIO);
 		bio_put(bio);
 	}
diff -Nurp linux/.git/config sched-deadline-mainline-dl/.git/config
--- linux/.git/config	2014-02-17 14:27:41.815672202 -0500
+++ sched-deadline-mainline-dl/.git/config	1969-12-31 19:00:00.000000000 -0500
@@ -1,11 +0,0 @@
-[core]
-	repositoryformatversion = 0
-	filemode = true
-	bare = false
-	logallrefupdates = true
-[remote "origin"]
-	url = https://github.com/torvalds/linux
-	fetch = +refs/heads/*:refs/remotes/origin/*
-[branch "master"]
-	remote = origin
-	merge = refs/heads/master
diff -Nurp linux/.git/description sched-deadline-mainline-dl/.git/description
--- linux/.git/description	2014-02-17 13:27:38.875240195 -0500
+++ sched-deadline-mainline-dl/.git/description	1969-12-31 19:00:00.000000000 -0500
@@ -1 +0,0 @@
-Unnamed repository; edit this file 'description' to name the repository.
diff -Nurp linux/.git/HEAD sched-deadline-mainline-dl/.git/HEAD
--- linux/.git/HEAD	2014-02-17 14:46:45.368905484 -0500
+++ sched-deadline-mainline-dl/.git/HEAD	1969-12-31 19:00:00.000000000 -0500
@@ -1 +0,0 @@
-ref: refs/heads/bv3.7-rc6
diff -Nurp linux/.git/hooks/applypatch-msg.sample sched-deadline-mainline-dl/.git/hooks/applypatch-msg.sample
--- linux/.git/hooks/applypatch-msg.sample	2014-02-17 13:27:38.870240074 -0500
+++ sched-deadline-mainline-dl/.git/hooks/applypatch-msg.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,15 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to check the commit log message taken by
-# applypatch from an e-mail message.
-#
-# The hook should exit with non-zero status after issuing an
-# appropriate message if it wants to stop the commit.  The hook is
-# allowed to edit the commit message file.
-#
-# To enable this hook, rename this file to "applypatch-msg".
-
-. git-sh-setup
-test -x "$GIT_DIR/hooks/commit-msg" &&
-	exec "$GIT_DIR/hooks/commit-msg" ${1+"$@"}
-:
diff -Nurp linux/.git/hooks/commit-msg.sample sched-deadline-mainline-dl/.git/hooks/commit-msg.sample
--- linux/.git/hooks/commit-msg.sample	2014-02-17 13:27:38.872240122 -0500
+++ sched-deadline-mainline-dl/.git/hooks/commit-msg.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,24 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to check the commit log message.
-# Called by "git commit" with one argument, the name of the file
-# that has the commit message.  The hook should exit with non-zero
-# status after issuing an appropriate message if it wants to stop the
-# commit.  The hook is allowed to edit the commit message file.
-#
-# To enable this hook, rename this file to "commit-msg".
-
-# Uncomment the below to add a Signed-off-by line to the message.
-# Doing this in a hook is a bad idea in general, but the prepare-commit-msg
-# hook is more suited to it.
-#
-# SOB=$(git var GIT_AUTHOR_IDENT | sed -n 's/^\(.*>\).*$/Signed-off-by: \1/p')
-# grep -qs "^$SOB" "$1" || echo "$SOB" >> "$1"
-
-# This example catches duplicate Signed-off-by lines.
-
-test "" = "$(grep '^Signed-off-by: ' "$1" |
-	 sort | uniq -c | sed -e '/^[ 	]*1[ 	]/d')" || {
-	echo >&2 Duplicate Signed-off-by lines.
-	exit 1
-}
diff -Nurp linux/.git/hooks/post-update.sample sched-deadline-mainline-dl/.git/hooks/post-update.sample
--- linux/.git/hooks/post-update.sample	2014-02-17 13:27:38.869240050 -0500
+++ sched-deadline-mainline-dl/.git/hooks/post-update.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,8 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to prepare a packed repository for use over
-# dumb transports.
-#
-# To enable this hook, rename this file to "post-update".
-
-exec git update-server-info
diff -Nurp linux/.git/hooks/pre-applypatch.sample sched-deadline-mainline-dl/.git/hooks/pre-applypatch.sample
--- linux/.git/hooks/pre-applypatch.sample	2014-02-17 13:27:38.869240050 -0500
+++ sched-deadline-mainline-dl/.git/hooks/pre-applypatch.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,14 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to verify what is about to be committed
-# by applypatch from an e-mail message.
-#
-# The hook should exit with non-zero status after issuing an
-# appropriate message if it wants to stop the commit.
-#
-# To enable this hook, rename this file to "pre-applypatch".
-
-. git-sh-setup
-test -x "$GIT_DIR/hooks/pre-commit" &&
-	exec "$GIT_DIR/hooks/pre-commit" ${1+"$@"}
-:
diff -Nurp linux/.git/hooks/pre-commit.sample sched-deadline-mainline-dl/.git/hooks/pre-commit.sample
--- linux/.git/hooks/pre-commit.sample	2014-02-17 13:27:38.871240098 -0500
+++ sched-deadline-mainline-dl/.git/hooks/pre-commit.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,50 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to verify what is about to be committed.
-# Called by "git commit" with no arguments.  The hook should
-# exit with non-zero status after issuing an appropriate message if
-# it wants to stop the commit.
-#
-# To enable this hook, rename this file to "pre-commit".
-
-if git rev-parse --verify HEAD >/dev/null 2>&1
-then
-	against=HEAD
-else
-	# Initial commit: diff against an empty tree object
-	against=4b825dc642cb6eb9a060e54bf8d69288fbee4904
-fi
-
-# If you want to allow non-ascii filenames set this variable to true.
-allownonascii=$(git config hooks.allownonascii)
-
-# Redirect output to stderr.
-exec 1>&2
-
-# Cross platform projects tend to avoid non-ascii filenames; prevent
-# them from being added to the repository. We exploit the fact that the
-# printable range starts at the space character and ends with tilde.
-if [ "$allownonascii" != "true" ] &&
-	# Note that the use of brackets around a tr range is ok here, (it's
-	# even required, for portability to Solaris 10's /usr/bin/tr), since
-	# the square bracket bytes happen to fall in the designated range.
-	test $(git diff --cached --name-only --diff-filter=A -z $against |
-	  LC_ALL=C tr -d '[ -~]\0' | wc -c) != 0
-then
-	echo "Error: Attempt to add a non-ascii file name."
-	echo
-	echo "This can cause problems if you want to work"
-	echo "with people on other platforms."
-	echo
-	echo "To be portable it is advisable to rename the file ..."
-	echo
-	echo "If you know what you are doing you can disable this"
-	echo "check using:"
-	echo
-	echo "  git config hooks.allownonascii true"
-	echo
-	exit 1
-fi
-
-# If there are whitespace errors, print the offending file names and fail.
-exec git diff-index --check --cached $against --
diff -Nurp linux/.git/hooks/prepare-commit-msg.sample sched-deadline-mainline-dl/.git/hooks/prepare-commit-msg.sample
--- linux/.git/hooks/prepare-commit-msg.sample	2014-02-17 13:27:38.869240050 -0500
+++ sched-deadline-mainline-dl/.git/hooks/prepare-commit-msg.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,36 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to prepare the commit log message.
-# Called by "git commit" with the name of the file that has the
-# commit message, followed by the description of the commit
-# message's source.  The hook's purpose is to edit the commit
-# message file.  If the hook fails with a non-zero status,
-# the commit is aborted.
-#
-# To enable this hook, rename this file to "prepare-commit-msg".
-
-# This hook includes three examples.  The first comments out the
-# "Conflicts:" part of a merge commit.
-#
-# The second includes the output of "git diff --name-status -r"
-# into the message, just before the "git status" output.  It is
-# commented because it doesn't cope with --amend or with squashed
-# commits.
-#
-# The third example adds a Signed-off-by line to the message, that can
-# still be edited.  This is rarely a good idea.
-
-case "$2,$3" in
-  merge,)
-    /usr/bin/perl -i.bak -ne 's/^/# /, s/^# #/#/ if /^Conflicts/ .. /#/; print' "$1" ;;
-
-# ,|template,)
-#   /usr/bin/perl -i.bak -pe '
-#      print "\n" . `git diff --cached --name-status -r`
-#	 if /^#/ && $first++ == 0' "$1" ;;
-
-  *) ;;
-esac
-
-# SOB=$(git var GIT_AUTHOR_IDENT | sed -n 's/^\(.*>\).*$/Signed-off-by: \1/p')
-# grep -qs "^$SOB" "$1" || echo "$SOB" >> "$1"
diff -Nurp linux/.git/hooks/pre-push.sample sched-deadline-mainline-dl/.git/hooks/pre-push.sample
--- linux/.git/hooks/pre-push.sample	2014-02-17 13:27:38.869240050 -0500
+++ sched-deadline-mainline-dl/.git/hooks/pre-push.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,53 +0,0 @@
-#!/bin/sh
-
-# An example hook script to verify what is about to be pushed.  Called by "git
-# push" after it has checked the remote status, but before anything has been
-# pushed.  If this script exits with a non-zero status nothing will be pushed.
-#
-# This hook is called with the following parameters:
-#
-# $1 -- Name of the remote to which the push is being done
-# $2 -- URL to which the push is being done
-#
-# If pushing without using a named remote those arguments will be equal.
-#
-# Information about the commits which are being pushed is supplied as lines to
-# the standard input in the form:
-#
-#   <local ref> <local sha1> <remote ref> <remote sha1>
-#
-# This sample shows how to prevent push of commits where the log message starts
-# with "WIP" (work in progress).
-
-remote="$1"
-url="$2"
-
-z40=0000000000000000000000000000000000000000
-
-IFS=' '
-while read local_ref local_sha remote_ref remote_sha
-do
-	if [ "$local_sha" = $z40 ]
-	then
-		# Handle delete
-	else
-		if [ "$remote_sha" = $z40 ]
-		then
-			# New branch, examine all commits
-			range="$local_sha"
-		else
-			# Update to existing branch, examine new commits
-			range="$remote_sha..$local_sha"
-		fi
-
-		# Check for WIP commit
-		commit=`git rev-list -n 1 --grep '^WIP' "$range"`
-		if [ -n "$commit" ]
-		then
-			echo "Found WIP commit in $local_ref, not pushing"
-			exit 1
-		fi
-	fi
-done
-
-exit 0
diff -Nurp linux/.git/hooks/pre-rebase.sample sched-deadline-mainline-dl/.git/hooks/pre-rebase.sample
--- linux/.git/hooks/pre-rebase.sample	2014-02-17 13:27:38.870240074 -0500
+++ sched-deadline-mainline-dl/.git/hooks/pre-rebase.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,169 +0,0 @@
-#!/bin/sh
-#
-# Copyright (c) 2006, 2008 Junio C Hamano
-#
-# The "pre-rebase" hook is run just before "git rebase" starts doing
-# its job, and can prevent the command from running by exiting with
-# non-zero status.
-#
-# The hook is called with the following parameters:
-#
-# $1 -- the upstream the series was forked from.
-# $2 -- the branch being rebased (or empty when rebasing the current branch).
-#
-# This sample shows how to prevent topic branches that are already
-# merged to 'next' branch from getting rebased, because allowing it
-# would result in rebasing already published history.
-
-publish=next
-basebranch="$1"
-if test "$#" = 2
-then
-	topic="refs/heads/$2"
-else
-	topic=`git symbolic-ref HEAD` ||
-	exit 0 ;# we do not interrupt rebasing detached HEAD
-fi
-
-case "$topic" in
-refs/heads/??/*)
-	;;
-*)
-	exit 0 ;# we do not interrupt others.
-	;;
-esac
-
-# Now we are dealing with a topic branch being rebased
-# on top of master.  Is it OK to rebase it?
-
-# Does the topic really exist?
-git show-ref -q "$topic" || {
-	echo >&2 "No such branch $topic"
-	exit 1
-}
-
-# Is topic fully merged to master?
-not_in_master=`git rev-list --pretty=oneline ^master "$topic"`
-if test -z "$not_in_master"
-then
-	echo >&2 "$topic is fully merged to master; better remove it."
-	exit 1 ;# we could allow it, but there is no point.
-fi
-
-# Is topic ever merged to next?  If so you should not be rebasing it.
-only_next_1=`git rev-list ^master "^$topic" ${publish} | sort`
-only_next_2=`git rev-list ^master           ${publish} | sort`
-if test "$only_next_1" = "$only_next_2"
-then
-	not_in_topic=`git rev-list "^$topic" master`
-	if test -z "$not_in_topic"
-	then
-		echo >&2 "$topic is already up-to-date with master"
-		exit 1 ;# we could allow it, but there is no point.
-	else
-		exit 0
-	fi
-else
-	not_in_next=`git rev-list --pretty=oneline ^${publish} "$topic"`
-	/usr/bin/perl -e '
-		my $topic = $ARGV[0];
-		my $msg = "* $topic has commits already merged to public branch:\n";
-		my (%not_in_next) = map {
-			/^([0-9a-f]+) /;
-			($1 => 1);
-		} split(/\n/, $ARGV[1]);
-		for my $elem (map {
-				/^([0-9a-f]+) (.*)$/;
-				[$1 => $2];
-			} split(/\n/, $ARGV[2])) {
-			if (!exists $not_in_next{$elem->[0]}) {
-				if ($msg) {
-					print STDERR $msg;
-					undef $msg;
-				}
-				print STDERR " $elem->[1]\n";
-			}
-		}
-	' "$topic" "$not_in_next" "$not_in_master"
-	exit 1
-fi
-
-exit 0
-
-################################################################
-
-This sample hook safeguards topic branches that have been
-published from being rewound.
-
-The workflow assumed here is:
-
- * Once a topic branch forks from "master", "master" is never
-   merged into it again (either directly or indirectly).
-
- * Once a topic branch is fully cooked and merged into "master",
-   it is deleted.  If you need to build on top of it to correct
-   earlier mistakes, a new topic branch is created by forking at
-   the tip of the "master".  This is not strictly necessary, but
-   it makes it easier to keep your history simple.
-
- * Whenever you need to test or publish your changes to topic
-   branches, merge them into "next" branch.
-
-The script, being an example, hardcodes the publish branch name
-to be "next", but it is trivial to make it configurable via
-$GIT_DIR/config mechanism.
-
-With this workflow, you would want to know:
-
-(1) ... if a topic branch has ever been merged to "next".  Young
-    topic branches can have stupid mistakes you would rather
-    clean up before publishing, and things that have not been
-    merged into other branches can be easily rebased without
-    affecting other people.  But once it is published, you would
-    not want to rewind it.
-
-(2) ... if a topic branch has been fully merged to "master".
-    Then you can delete it.  More importantly, you should not
-    build on top of it -- other people may already want to
-    change things related to the topic as patches against your
-    "master", so if you need further changes, it is better to
-    fork the topic (perhaps with the same name) afresh from the
-    tip of "master".
-
-Let's look at this example:
-
-		   o---o---o---o---o---o---o---o---o---o "next"
-		  /       /           /           /
-		 /   a---a---b A     /           /
-		/   /               /           /
-	       /   /   c---c---c---c B         /
-	      /   /   /             \         /
-	     /   /   /   b---b C     \       /
-	    /   /   /   /             \     /
-    ---o---o---o---o---o---o---o---o---o---o---o "master"
-
-
-A, B and C are topic branches.
-
- * A has one fix since it was merged up to "next".
-
- * B has finished.  It has been fully merged up to "master" and "next",
-   and is ready to be deleted.
-
- * C has not merged to "next" at all.
-
-We would want to allow C to be rebased, refuse A, and encourage
-B to be deleted.
-
-To compute (1):
-
-	git rev-list ^master ^topic next
-	git rev-list ^master        next
-
-	if these match, topic has not merged in next at all.
-
-To compute (2):
-
-	git rev-list master..topic
-
-	if this is empty, it is fully merged to "master".
diff -Nurp linux/.git/hooks/update.sample sched-deadline-mainline-dl/.git/hooks/update.sample
--- linux/.git/hooks/update.sample	2014-02-17 13:27:38.871240098 -0500
+++ sched-deadline-mainline-dl/.git/hooks/update.sample	1969-12-31 19:00:00.000000000 -0500
@@ -1,128 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to blocks unannotated tags from entering.
-# Called by "git receive-pack" with arguments: refname sha1-old sha1-new
-#
-# To enable this hook, rename this file to "update".
-#
-# Config
-# ------
-# hooks.allowunannotated
-#   This boolean sets whether unannotated tags will be allowed into the
-#   repository.  By default they won't be.
-# hooks.allowdeletetag
-#   This boolean sets whether deleting tags will be allowed in the
-#   repository.  By default they won't be.
-# hooks.allowmodifytag
-#   This boolean sets whether a tag may be modified after creation. By default
-#   it won't be.
-# hooks.allowdeletebranch
-#   This boolean sets whether deleting branches will be allowed in the
-#   repository.  By default they won't be.
-# hooks.denycreatebranch
-#   This boolean sets whether remotely creating branches will be denied
-#   in the repository.  By default this is allowed.
-#
-
-# --- Command line
-refname="$1"
-oldrev="$2"
-newrev="$3"
-
-# --- Safety check
-if [ -z "$GIT_DIR" ]; then
-	echo "Don't run this script from the command line." >&2
-	echo " (if you want, you could supply GIT_DIR then run" >&2
-	echo "  $0 <ref> <oldrev> <newrev>)" >&2
-	exit 1
-fi
-
-if [ -z "$refname" -o -z "$oldrev" -o -z "$newrev" ]; then
-	echo "usage: $0 <ref> <oldrev> <newrev>" >&2
-	exit 1
-fi
-
-# --- Config
-allowunannotated=$(git config --bool hooks.allowunannotated)
-allowdeletebranch=$(git config --bool hooks.allowdeletebranch)
-denycreatebranch=$(git config --bool hooks.denycreatebranch)
-allowdeletetag=$(git config --bool hooks.allowdeletetag)
-allowmodifytag=$(git config --bool hooks.allowmodifytag)
-
-# check for no description
-projectdesc=$(sed -e '1q' "$GIT_DIR/description")
-case "$projectdesc" in
-"Unnamed repository"* | "")
-	echo "*** Project description file hasn't been set" >&2
-	exit 1
-	;;
-esac
-
-# --- Check types
-# if $newrev is 0000...0000, it's a commit to delete a ref.
-zero="0000000000000000000000000000000000000000"
-if [ "$newrev" = "$zero" ]; then
-	newrev_type=delete
-else
-	newrev_type=$(git cat-file -t $newrev)
-fi
-
-case "$refname","$newrev_type" in
-	refs/tags/*,commit)
-		# un-annotated tag
-		short_refname=${refname##refs/tags/}
-		if [ "$allowunannotated" != "true" ]; then
-			echo "*** The un-annotated tag, $short_refname, is not allowed in this repository" >&2
-			echo "*** Use 'git tag [ -a | -s ]' for tags you want to propagate." >&2
-			exit 1
-		fi
-		;;
-	refs/tags/*,delete)
-		# delete tag
-		if [ "$allowdeletetag" != "true" ]; then
-			echo "*** Deleting a tag is not allowed in this repository" >&2
-			exit 1
-		fi
-		;;
-	refs/tags/*,tag)
-		# annotated tag
-		if [ "$allowmodifytag" != "true" ] && git rev-parse $refname > /dev/null 2>&1
-		then
-			echo "*** Tag '$refname' already exists." >&2
-			echo "*** Modifying a tag is not allowed in this repository." >&2
-			exit 1
-		fi
-		;;
-	refs/heads/*,commit)
-		# branch
-		if [ "$oldrev" = "$zero" -a "$denycreatebranch" = "true" ]; then
-			echo "*** Creating a branch is not allowed in this repository" >&2
-			exit 1
-		fi
-		;;
-	refs/heads/*,delete)
-		# delete branch
-		if [ "$allowdeletebranch" != "true" ]; then
-			echo "*** Deleting a branch is not allowed in this repository" >&2
-			exit 1
-		fi
-		;;
-	refs/remotes/*,commit)
-		# tracking branch
-		;;
-	refs/remotes/*,delete)
-		# delete tracking branch
-		if [ "$allowdeletebranch" != "true" ]; then
-			echo "*** Deleting a tracking branch is not allowed in this repository" >&2
-			exit 1
-		fi
-		;;
-	*)
-		# Anything else (is there anything else?)
-		echo "*** Update hook: unknown type of update to ref $refname of type $newrev_type" >&2
-		exit 1
-		;;
-esac
-
-# --- Finished
-exit 0
Binary files linux/.git/index and sched-deadline-mainline-dl/.git/index differ
diff -Nurp linux/.git/info/exclude sched-deadline-mainline-dl/.git/info/exclude
--- linux/.git/info/exclude	2014-02-17 13:27:38.877240243 -0500
+++ sched-deadline-mainline-dl/.git/info/exclude	1969-12-31 19:00:00.000000000 -0500
@@ -1,6 +0,0 @@
-# git ls-files --others --exclude-from=.git/info/exclude
-# Lines that start with '#' are comments.
-# For a project mostly in C, the following would be a good set of
-# exclude patterns (uncomment them if you want to use them):
-# *.[oa]
-# *~
diff -Nurp linux/.git/logs/HEAD sched-deadline-mainline-dl/.git/logs/HEAD
--- linux/.git/logs/HEAD	2014-02-17 14:46:45.381905803 -0500
+++ sched-deadline-mainline-dl/.git/logs/HEAD	1969-12-31 19:00:00.000000000 -0500
@@ -1,2 +0,0 @@
-0000000000000000000000000000000000000000 6d0abeca3242a88cab8232e4acd7e2bf088f3bc2 qguan <qguan@localhost.localdomain> 1392665261 -0500	clone: from https://github.com/torvalds/linux
-6d0abeca3242a88cab8232e4acd7e2bf088f3bc2 f4a75d2eb7b1e2206094b901be09adb31ba63681 qguan <qguan@localhost.localdomain> 1392666405 -0500	checkout: moving from master to bv3.7-rc6
diff -Nurp linux/.git/logs/refs/heads/bv3.7-rc6 sched-deadline-mainline-dl/.git/logs/refs/heads/bv3.7-rc6
--- linux/.git/logs/refs/heads/bv3.7-rc6	2014-02-17 14:46:45.345904920 -0500
+++ sched-deadline-mainline-dl/.git/logs/refs/heads/bv3.7-rc6	1969-12-31 19:00:00.000000000 -0500
@@ -1 +0,0 @@
-0000000000000000000000000000000000000000 f4a75d2eb7b1e2206094b901be09adb31ba63681 qguan <qguan@localhost.localdomain> 1392666405 -0500	branch: Created from v3.7-rc6
diff -Nurp linux/.git/logs/refs/heads/master sched-deadline-mainline-dl/.git/logs/refs/heads/master
--- linux/.git/logs/refs/heads/master	2014-02-17 14:27:41.802671887 -0500
+++ sched-deadline-mainline-dl/.git/logs/refs/heads/master	1969-12-31 19:00:00.000000000 -0500
@@ -1 +0,0 @@
-0000000000000000000000000000000000000000 6d0abeca3242a88cab8232e4acd7e2bf088f3bc2 qguan <qguan@localhost.localdomain> 1392665261 -0500	clone: from https://github.com/torvalds/linux
diff -Nurp linux/.git/logs/refs/remotes/origin/HEAD sched-deadline-mainline-dl/.git/logs/refs/remotes/origin/HEAD
--- linux/.git/logs/refs/remotes/origin/HEAD	2014-02-17 14:27:41.801671863 -0500
+++ sched-deadline-mainline-dl/.git/logs/refs/remotes/origin/HEAD	1969-12-31 19:00:00.000000000 -0500
@@ -1 +0,0 @@
-0000000000000000000000000000000000000000 6d0abeca3242a88cab8232e4acd7e2bf088f3bc2 qguan <qguan@localhost.localdomain> 1392665261 -0500	clone: from https://github.com/torvalds/linux
Binary files linux/.git/objects/pack/pack-231a164501e78670545e1eac2321441519bce916.idx and sched-deadline-mainline-dl/.git/objects/pack/pack-231a164501e78670545e1eac2321441519bce916.idx differ
Binary files linux/.git/objects/pack/pack-231a164501e78670545e1eac2321441519bce916.pack and sched-deadline-mainline-dl/.git/objects/pack/pack-231a164501e78670545e1eac2321441519bce916.pack differ
diff -Nurp linux/.git/packed-refs sched-deadline-mainline-dl/.git/packed-refs
--- linux/.git/packed-refs	2014-02-17 14:27:41.709669630 -0500
+++ sched-deadline-mainline-dl/.git/packed-refs	1969-12-31 19:00:00.000000000 -0500
@@ -1,720 +0,0 @@
-# pack-refs with: peeled fully-peeled 
-6d0abeca3242a88cab8232e4acd7e2bf088f3bc2 refs/remotes/origin/master
-5dc01c595e6c6ec9ccda4f6f69c131c0dd945f8c refs/tags/v2.6.11
-^c39ae07f393806ccf406ef966e9a15afc43cc36a
-5dc01c595e6c6ec9ccda4f6f69c131c0dd945f8c refs/tags/v2.6.11-tree
-^c39ae07f393806ccf406ef966e9a15afc43cc36a
-26791a8bcf0e6d33f43aef7682bdb555236d56de refs/tags/v2.6.12
-^9ee1c939d1cb936b1f98e8d81aeffab57bae46ab
-9e734775f7c22d2f89943ad6c745571f1930105f refs/tags/v2.6.12-rc2
-^1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
-0397236d43e48e821cce5bbe6a80a1a56bb7cc3a refs/tags/v2.6.12-rc3
-^a2755a80f40e5794ddc20e00f781af9d6320fafb
-ebb5573ea8beaf000d4833735f3e53acb9af844c refs/tags/v2.6.12-rc4
-^88d7bd8cb9eb8d64bf7997600b0d64f7834047c5
-06f6d9e2f140466eeb41e494e14167f90210f89d refs/tags/v2.6.12-rc5
-^2a24ab628aa7b190be32f63dfb6d96f3fb61580a
-701d7ecec3e0c6b4ab9bb824fd2b34be4da63b7e refs/tags/v2.6.12-rc6
-^7cef5677ef3a8084f2588ce0a129dc95d65161f6
-0da688d20078783b23f99b232b272b027d6c3f59 refs/tags/v2.6.13
-^02b3e4e2d71b6058ec11cc01c72ac651eb3ded2b
-733ad933f62e82ebc92fed988c7f0795e64dea62 refs/tags/v2.6.13-rc1
-^4c91aedb75d1b87deccf16d58f67fb46402d7d44
-c521cb0f10ef2bf28a18e1cc8adf378ccbbe5a19 refs/tags/v2.6.13-rc2
-^a18bcb7450840f07a772a45229de4811d930f461
-a339981ec18d304f9efeb9ccf01b1f04302edf32 refs/tags/v2.6.13-rc3
-^c32511e2718618f0b53479eb36e07439aa363a74
-7eab951de91d95875ba34ec4c599f37e1208db93 refs/tags/v2.6.13-rc4
-^63953523341bcafe5928bf6e99bffd7db94b471e
-2ef68d3fdfacf92ac5ff84be969b4b8acb5c0723 refs/tags/v2.6.13-rc5
-^9a351e30d72d409ec62c83f380e330e0baa584b4
-50f0e20a16496a9feb52cf1d79a0c33cce1a682d refs/tags/v2.6.13-rc6
-^6fc32179de9e14c542e0b1760e412bc670611c53
-3e6f704488332c74e7defec13f22bf2e2bccb444 refs/tags/v2.6.13-rc7
-^0572e3da3ff5c3744b2f606ecf296d5f89a4bbdf
-2b10839e32c4c476e9d94492756bb1a3e1ec4aa8 refs/tags/v2.6.14
-^741b2252a5e14d6c60a913c77a6099abe73a854a
-1f9d1e3248d4eb96b229eecf0e5d9445d3529e85 refs/tags/v2.6.14-rc1
-^2f4ba45a75d6383b4a1201169a808ffea416ffa0
-c2bbf523f1d454649897b3e4bcd71778e4fa5913 refs/tags/v2.6.14-rc2
-^676d55ae30ea3b688f0386f70553489f25f24d55
-f92737b18abac90af30ac26a050fda879c9b238b refs/tags/v2.6.14-rc3
-^1c9426e8a59461688bb451e006456987b198e4c0
-a1aeb6f9e4306b1a62629c56197eea65afd372b3 refs/tags/v2.6.14-rc4
-^907a42617970a159361f17ef9a63f04d276995ab
-c08359b174c3b3a175979f81cc786c791f74c852 refs/tags/v2.6.14-rc5
-^93918e9afc76717176e9e114e79cdbb602a45ae8
-dab47a31f42a23d2b374e1cd7d0b797e8e08b23d refs/tags/v2.6.15
-^88026842b0a760145aa71d69e74fbc9ec118ca44
-1400e40b758e4099e4d758d8fc38981b33b71897 refs/tags/v2.6.15-rc1
-^cd52d1ee9a92587b242d946a2300a3245d3b885a
-7305b5cb045e2c71250b5b7472771ed2620bc514 refs/tags/v2.6.15-rc2
-^3bedff1d73b86e0cf52634efb447e9ada08f2cc6
-00d7a7358e3f9f2575501674e604fe4c6700b365 refs/tags/v2.6.15-rc3
-^624f54be206adf970cd8eece16446b027913e533
-8c27e9cccbc96548646606a2eb52c27e72abf5e1 refs/tags/v2.6.15-rc4
-^5666c0947ede0432ba5148570aa66ffb9febff5b
-b58cb01b0f7eff054d1246e63f0eebfbdf8e6686 refs/tags/v2.6.15-rc5
-^436b0f76f2cee6617f27a649637766628909dd5d
-20f065bc620db2d6157c95b117910c4ed59acc46 refs/tags/v2.6.15-rc6
-^df7addbb45874f0f992266003155de5a22e1872f
-373540886574ecf8b864fa7b8a5cf879b1d7b054 refs/tags/v2.6.15-rc7
-^f89f5948fc10bb973cd452d2e334da207828e228
-414ad0ded83f088608f7c0e774df8cccbba4e229 refs/tags/v2.6.16
-^7705a8792b0fc82fd7d4dd923724606bbfd9fb20
-f3bcf72eb85aba88a7bd0a6116dd0b5418590dbe refs/tags/v2.6.16-rc1
-^2664b25051f7ab96b22b199aa2f5ef6a949a4296
-e4f9aae0d74cb7d2fd5f0eb315cf9de1118fe260 refs/tags/v2.6.16-rc2
-^826eeb53a6f264842200d3311d69107d2eb25f5e
-a3adf278bcd5852d8c6345c9b0a44a0cc71c7454 refs/tags/v2.6.16-rc3
-^e9bb4c9929a63b23dcc637fae312b36b038bdc61
-d94ef3bc908905e8bc0f1560a90df76a4c4b60bb refs/tags/v2.6.16-rc4
-^bd71c2b17468a2531fb4c81ec1d73520845e97e1
-afdf555c717a6ace9e5446dda4d169eaa5ca8062 refs/tags/v2.6.16-rc5
-^b9a33cebac70d6f67a769ce8d4078fee2b254ada
-76dbe3f496eeaadab8977409b930e7d522dcf750 refs/tags/v2.6.16-rc6
-^535744878e34d01a53f946f26dfbca37186f2cf8
-8ba130df4b67fa40878ccf80d54615132d24bc68 refs/tags/v2.6.17
-^427abfa28afedffadfca9dd8b067eb6d36bac53f
-d882e0c80e6e3c60640492b83395e6fbbae04276 refs/tags/v2.6.17-rc1
-^6246b6128bbe34d0752f119cf7c5111c85fe481d
-f61c8059ffbc29bd8a1ffbd5a87e5135bc28a752 refs/tags/v2.6.17-rc2
-^8bbde0e6d52265158ee9625f383500c1a7d09ba9
-6716c37ec2dbf78b85a55cc5605677b6cf2299a0 refs/tags/v2.6.17-rc3
-^2be4d50295e2b6f62c07b614e1b103e280dddb84
-90b92312eeebc70e61415394be3cc03b08a74945 refs/tags/v2.6.17-rc4
-^d8c3291c73b958243b33f8509d4507e76dafd055
-39beb382e4e11ed01cb5e73022f18bbed2aefd8b refs/tags/v2.6.17-rc5
-^a8bd60705aa17a998516837d9c1e503ad4cbd7fc
-831695cbeb7a0e8f9ddb9c0203a22723da2c3f2f refs/tags/v2.6.17-rc6
-^1def630a6a49dda5bc89dfbd86656293640456f0
-119248f4578ca60b09c20893724e10f19806e6f1 refs/tags/v2.6.18
-^e478bec0ba0a83a48a0f6982934b6de079e7e6b3
-7df8ea909888d4856d3aff1c41192739d715a393 refs/tags/v2.6.18-rc1
-^120bda20c6f64b32e8bfbdd7b34feafaa5f5332e
-f4035c6284f7de0b47ccf68264f289e7e0b47da2 refs/tags/v2.6.18-rc2
-^82d6897fefca6206bca7153805b4c5359ce97fc4
-5031a46c0d098346e4c9cd56f7858def243da096 refs/tags/v2.6.18-rc3
-^b6ff50833ad43a8ebd9b16bf53c334f7aaf33c41
-06abe048193087f81ee67c225a1b685d937f220d refs/tags/v2.6.18-rc4
-^9f737633e6ee54fc174282d49b2559bd2208391d
-69b7510036d06c6aabcdc0bb03f35cfc62160302 refs/tags/v2.6.18-rc5
-^60d4684068ff1eec78f55b5888d0bd2d4cca1520
-12c8b6ff70a17bfcb59bcca55fa8e79d809c1c70 refs/tags/v2.6.18-rc6
-^c336923b668fdcf0312efbec3b44895d713f4d81
-3d3f47c98b3696f5bd677f89a755c239f4d52a4c refs/tags/v2.6.18-rc7
-^95064a75ebf8744e1ff595e8cd7ff9b6c851523e
-c3fe6924620fd733ffe8bc8a9da1e9cde08402b3 refs/tags/v2.6.19
-^0215ffb08ce99e2bb59eca114a99499a4d06e704
-008616c979c90b1a4ea2d2cebf4996743463b001 refs/tags/v2.6.19-rc1
-^d223a60106891bfe46febfacf46b20cd8509aaad
-5e2c92804ed42d0be784e4dc4041dc3ca001810b refs/tags/v2.6.19-rc2
-^b4bd8c66435a8cdf8c90334fb3b517a23ff2ab95
-aaa751e8aa7dd8b1c1f787728d279d9dadbb5938 refs/tags/v2.6.19-rc3
-^7059abedd2f04b68bd7e1a79c9c72f7aeee134c0
-7a9d289b6650bf78df77ab463bedc2919df89833 refs/tags/v2.6.19-rc4
-^ae99a78af33f00565a05dbbc6ca9b247fed002c5
-2f4871018917f13c7cd01254335561a152decc00 refs/tags/v2.6.19-rc5
-^80c218812786f619c9a1ce50d0e7c32c7afde4de
-82471364173618c5a97b6c02bf6e72deddde9632 refs/tags/v2.6.19-rc6
-^44597f65f6af3c692560a639f61d25398d13d1b6
-fa285a3d7924a0e3782926e51f16865c5129a2f7 refs/tags/v2.6.20
-^62d0cfcb27cf755cebdc93ca95dabc83608007cd
-a7ecdd29e85a7d51179c6a4507a4c25a87ab5c0e refs/tags/v2.6.20-rc1
-^cc016448b0bf0764928275d034e367753bde8162
-8a2d17a56a71c5c796b0a5378ee76a105f21fdd9 refs/tags/v2.6.20-rc2
-^3bf8ba38f38d3647368e4edcf7d019f9f8d9184a
-9d37e643a198d4232d2a12f1107358eb02b6cece refs/tags/v2.6.20-rc3
-^669df1b478803f49a356528d290af7bf442eb3be
-cb15cca892ac09f7cb99618a09304c5f5620e6ee refs/tags/v2.6.20-rc4
-^bf81b46482c0fa8ea638e409d39768ea92a6b0f0
-c81c70d32732b7abc644ec7aca24f3597aa921ea refs/tags/v2.6.20-rc5
-^a8b3485287731978899ced11f24628c927890e78
-0909eebced6eadf15ea6aadce0d79c08676de3fe refs/tags/v2.6.20-rc6
-^99abfeafb5f2eea1bb481330ff37343e1133c924
-97c617f72b58983017f32da97fc228597c0dae8f refs/tags/v2.6.20-rc7
-^f56df2f4db6e4af87fb8e941cff69f4501a111df
-d1be341dba5521506d9e6dccfd66179080705bea refs/tags/v2.6.21
-^de46c33745f5e2ad594c72f2cf5f490861b16ce1
-2eb1ae149a28c1b8ade687c5fbab3c37da4c0fba refs/tags/v2.6.21-rc1
-^c8f71b01a50597e298dc3214a2f2be7b8d31170c
-fa18364691754673df97824d81bfe8524a3a0595 refs/tags/v2.6.21-rc2
-^606135a3081e045b677cde164a296c51f66c4633
-44e05067b4b767fa3137335818c399b9b198c925 refs/tags/v2.6.21-rc3
-^08e15e81a40e3241ce93b4a43886f3abda184aa6
-bac6eefe96204d0ad67d144f2511a6fc487aa594 refs/tags/v2.6.21-rc4
-^db98e0b434a6265c451ffe94ec0a29b8d0aaf587
-6fb04ccf5c5e054c4107090bed6e866489f1089f refs/tags/v2.6.21-rc5
-^e0f2e3a06be513352cb4955313ed7e55909acd84
-a5d693edb558469a6f72bfda1253c7ba2278d657 refs/tags/v2.6.21-rc6
-^a21bd69e1509b43823c317c3bf3f7ffa99884356
-60afa917fea11f7cc93281b2dfd64b19b6521a93 refs/tags/v2.6.21-rc7
-^94a05509a9e11806acd797153d03019706e466f1
-098fd16f00005f665d3baa7e682d8cb3d7c0fe6f refs/tags/v2.6.22
-^7dcca30a32aadb0520417521b0c44f42d09fe05c
-cb22632a37a5d797da988453924206d1638e4e6c refs/tags/v2.6.22-rc1
-^39403865d2e4590802553370a56c9ab93131e4ee
-5a1b8597cd250efd5bda1cba08417c95b6b314d7 refs/tags/v2.6.22-rc2
-^55b637c6a003a8c4850b41a2c2fd6942d8a7f530
-0119a8416e0827bf2b937a1cf21d4909db3fa111 refs/tags/v2.6.22-rc3
-^c420bc9f09a0926b708c3edb27eacba434a4f4ba
-5b78c77092a64e253fe1fde9fbbe818b49330ffc refs/tags/v2.6.22-rc4
-^5ecd3100e695228ac5e0ce0e325e252c0f11806f
-aec07c7abc280bd5d0ca33b7cda3eb7b9b6e89c1 refs/tags/v2.6.22-rc5
-^188e1f81ba31af1b65a2f3611df4c670b092bbac
-953e420db7c599f7db00548243f2afddc8440329 refs/tags/v2.6.22-rc6
-^189548642c5962e60c3667bdb3a703fe0bed12a6
-087ea061253277de2b27e82d8572a386835a1b7e refs/tags/v2.6.22-rc7
-^a38d6181ff27824c79fc7df825164a212eff6a3f
-0b8bc8b91cf6befea20fe78b90367ca7b61cfa0d refs/tags/v2.6.23
-^bbf25010f1a6b761914430f5fca081ec8c7accd1
-7d57c74238cdf570bca20b711b2c0b31a553c1e5 refs/tags/v2.6.23-rc1
-^f695baf2df9e0413d3521661070103711545207a
-2c7522b19c386ed601d27b2aed3e7b84ac7852f0 refs/tags/v2.6.23-rc2
-^d4ac2477fad0f2680e84ec12e387ce67682c5c13
-39e7e2ec80646a62a9d61871bee8d2736088a86f refs/tags/v2.6.23-rc3
-^39d3520c92cf7a28c07229ca00cc35a1e8026c77
-e7afef45b41f5f5e2211322f083247e42ba13a78 refs/tags/v2.6.23-rc4
-^b07d68b5ca4d55a16fab223d63d5fb36f89ff42f
-f3cfc7abf10379d926dd04008059d2b04eaf4499 refs/tags/v2.6.23-rc5
-^40ffbfad6bb79a99cc7627bdaca0ee22dec526f6
-a33969a68b624d98356398af0a59856cc52f47a5 refs/tags/v2.6.23-rc6
-^0d4cbb5e7f60b2f1a4d8b7f6ea4cc264262c7a01
-c3e1edc6a6b420f81a6bc1ea47c5b3dd157e76aa refs/tags/v2.6.23-rc7
-^81cfe79b9c577139a873483654640eb3f6e78c39
-fc4a2ad046f06bed41eda33142c5767149a72fe7 refs/tags/v2.6.23-rc8
-^4942de4a0e914f205d351a81873f4f63986bcc3c
-da0a81e98c06aa0d1e05b9012c2b2facb1807e12 refs/tags/v2.6.23-rc9
-^3146b39c185f8a436d430132457e84fa1d8f8208
-0d733ddb2026683da26c1722847b99911c43ccb5 refs/tags/v2.6.24
-^49914084e797530d9baaf51df9eda77babc98fa8
-cebdeed27b068dcc3e7c311d7ec0d9c33b5138c2 refs/tags/v2.6.24-rc1
-^c9927c2bf4f45bb85e8b502ab3fb79ad6483c244
-9aae299f7fd1888ea3a195cfe0edef17bb647415 refs/tags/v2.6.24-rc2
-^dbeeb816e805091e7cfc03baf36dc40b4adb2bbd
-f05092637dc0d9a3f2249c9b283b973e6e96b7d2 refs/tags/v2.6.24-rc3
-^d9f8bcbf67a0ee67c8cb0734f003dfe916bb5248
-b6fa40f5916811c6aad6625c384d26fd01135014 refs/tags/v2.6.24-rc4
-^09b56adc98e0f8a21644fcb4d20ad367c3fceb55
-9f11d5919577129413e8389e43e5b6e8413dff53 refs/tags/v2.6.24-rc5
-^82d29bf6dc7317aeb0a3a13c2348ca8591965875
-f49e4e249d57ddfa97e046bc5c994ef72c93e63b refs/tags/v2.6.24-rc6
-^ea67db4cdbbf7f4e74150e71da0984e25121f500
-fcb31af14662059db467201ec73dfbb6f3300342 refs/tags/v2.6.24-rc7
-^3ce54450461bad18bbe1f9f5aa3ecd2f8e8d1235
-c9ba0caa9650a1898c839a79f6ff96a8a982424c refs/tags/v2.6.24-rc8
-^cbd9c883696da72b2b1f03f909dbacc04bbf8b58
-20b8df8e5501bac243e64c0c8c52907735a0041b refs/tags/v2.6.25
-^4b119e21d0c66c22e8ca03df05d9de623d0eb50f
-abf6976c818c553eb2209fe32028a4c5eecab0cb refs/tags/v2.6.25-rc1
-^19af35546de68c872dcb687613e0902a602cb20e
-b74415eac8d3f1fcb39ad4bcef0c829635a3bc9f refs/tags/v2.6.25-rc2
-^101142c37be8e5af9b847860219217e6b958c739
-d622f5379e88a3bac4f8decfa49c0a04a8e209d3 refs/tags/v2.6.25-rc3
-^bfa274e2436fc7ef72ef51c878083647f1cfd429
-c6c155e032361b0031943141b1a6f231e4f63817 refs/tags/v2.6.25-rc4
-^29e8c3c304b62f31b799565c9ee85d42bd163f80
-cd81f35c48b7e0c2a871f88e1973f391f8330449 refs/tags/v2.6.25-rc5
-^cdeeeae056a429e729ae9e914fa8142ee45bee93
-b22f07f908a648c864b16d2ba71f03aba4b684c9 refs/tags/v2.6.25-rc6
-^a978b30af3bab0dd9af9350eeda25e76123fa28e
-f4281310b609edd587922b7d4afa63e4b9a1ffd4 refs/tags/v2.6.25-rc7
-^05dda977f2574c3341abef9b74c27d2b362e1e3a
-e39586f39c2829d30f4ea6680a846dfe4aad2f2e refs/tags/v2.6.25-rc8
-^0e81a8ae37687845f7cdfa2adce14ea6a5f1dd34
-3df83da958163beeca00d1254f512fafd79a19ed refs/tags/v2.6.25-rc9
-^120dd64cacd4fb796bca0acba3665553f1d9ecaa
-14650d6ec137e70b6c1918cdef235027c5156020 refs/tags/v2.6.26
-^bce7f793daec3e65ec5c5705d2457b81fe7b5725
-d6b7f73ed134769c86966697e61b235b200cc4ae refs/tags/v2.6.26-rc1
-^2ddcca36c8bcfa251724fe342c8327451988be0d
-b67fc588ce611ca847620bd1353bf2d68fc3027f refs/tags/v2.6.26-rc2
-^492c2e476eac010962850006c49df326919b284c
-b041a30258df00c90ac1ed532cec3f25c00a3ce8 refs/tags/v2.6.26-rc3
-^b8291ad07a7f3b5b990900f0001198ac23ba893e
-e21868a4cdd93e5883ff61579d4cd799d1a3c244 refs/tags/v2.6.26-rc4
-^e490517a039a99d692cb3a5561941b0a5f576172
-9ab8267ac47ce50b932cc4b1cbd9b05e2faac8b7 refs/tags/v2.6.26-rc5
-^53c8ba95402be65d412a806cda3430f0e72cd107
-c047413a582cbf2c7e1b012458c1665d959703be refs/tags/v2.6.26-rc6
-^5dd34572ad9a3be430632dd42e4af2ea370b397b
-f40883d058ed196976285fc1fd5fd6c85dcb5bef refs/tags/v2.6.26-rc7
-^d70ac829b7f42d7ef4f879635c6a772b0b4ed0a2
-496a3db2bfb98f1e9c7b73514d8d25790f69f5fb refs/tags/v2.6.26-rc8
-^543cf4cb3fe6f6cae3651ba918b9c56200b257d0
-c22689a6f45beff21b97df566e0da17b4fa9ec19 refs/tags/v2.6.26-rc9
-^b7279469d66b55119784b8b9529c99c1955fe747
-4b5127df968616dee2f4775d795198878ef1638b refs/tags/v2.6.27
-^3fa8749e584b55f1180411ab1b51117190bac1e5
-78e28361b194c98eaa987e368264c2209ca08976 refs/tags/v2.6.27-rc1
-^6e86841d05f371b5b9b86ce76c02aaee83352298
-b27c893faffea2a0c49cf6170d89c2a7aeba6598 refs/tags/v2.6.27-rc2
-^0967d61ea0d8e8a7826bd8949cd93dd1e829ac55
-4aca83081f7ddf04d74989cc9b5e149160096324 refs/tags/v2.6.27-rc3
-^30a2f3c60a84092c8084dfe788b710f8d0768cd4
-0572569f90ba4175e0400f3be7b9f42e1c803e55 refs/tags/v2.6.27-rc4
-^6a55617ed5d1aa62b850de2cf66f5ede2eef4825
-d17d499628a7d4a34c11f54a203281773885a3e7 refs/tags/v2.6.27-rc5
-^24342c34a022ee90839873d91396045e12ef1090
-89c44b4a5ad50f8b85846b16af5f977f3861d197 refs/tags/v2.6.27-rc6
-^adee14b2e1557d0a8559f29681732d05a89dfc35
-cb1d73008533d7c4a949fc0867de2612dab23572 refs/tags/v2.6.27-rc7
-^72d31053f62c4bc464c2783974926969614a8649
-7b4e06aa7c0952353096105f87be67615e4382f5 refs/tags/v2.6.27-rc8
-^94aca1dac6f6d21f4b07e4864baf7768cabcc6e7
-5a97794d66909dbe3282062d7637705bcd352815 refs/tags/v2.6.27-rc9
-^4330ed8ed4da360ac1ca14b0fddff4c05b10de16
-8a38e7fd7a30cd44be954f9a3b062e607cec5d41 refs/tags/v2.6.28
-^4a6908a3a050aacc9c3a2f36b276b46c0629ad91
-cb50773491b0066d0e55f31f8875d5678fa3f8ad refs/tags/v2.6.28-rc1
-^57f8f7b60db6f1ed2c6918ab9230c4623a9dbe37
-5eb14db1f80df4eb0ecb0976e47e8e287e3175fc refs/tags/v2.6.28-rc2
-^0173a3265b228da319ceb9c1ec6a5682fd1b2d92
-31cb515c75388d457c2f318a0ee9606b3527852f refs/tags/v2.6.28-rc3
-^45beca08dd8b6d6a65c5ffd730af2eac7a2c7a03
-b65a80a5ee7923355cbca669cead08e067fc7ada refs/tags/v2.6.28-rc4
-^f7160c7573615ec82c691e294cf80d920b5d588d
-68185b00cf91c1c4dcc761a2f3a1631562ed52f3 refs/tags/v2.6.28-rc5
-^9bf1a2445f3c569098b8de7097ca324e65abecc2
-b503092a16bdba0a418e155fe592521fc20855af refs/tags/v2.6.28-rc6
-^13d428afc007fcfcd6deeb215618f54cf9c0cae6
-1a0bff987b27da5181f112bcc60f34d6fbb7e67e refs/tags/v2.6.28-rc7
-^061e41fdb5047b1fb161e89664057835935ca1d2
-6fa7003fe34e9a8a31fb91754f3c289cc045564b refs/tags/v2.6.28-rc8
-^8b1fae4e4200388b64dd88065639413cb3f1051c
-7d4b1bcc5e7411fc9e63f610c16e5de8fe6dfde8 refs/tags/v2.6.28-rc9
-^929096fe9ff1f4b3645cf3919527ab47e8d5e17c
-5dfd736f95b3d84a18b5bb8e50ac71f245438acf refs/tags/v2.6.29
-^8e0ee43bc2c3e19db56a4adaa9a9b04ce885cd84
-7a3862d6e9934ffe107fe7ddfbe2c63dba321793 refs/tags/v2.6.29-rc1
-^c59765042f53a79a7a65585042ff463b69cb248c
-d31ce8060b0e875179ba5ca1d40475dc2a082cc7 refs/tags/v2.6.29-rc2
-^1de9e8e70f5acc441550ca75433563d91b269bbe
-8be00154b8e949bf4b89ac198aef9a247532ac2d refs/tags/v2.6.29-rc3
-^18e352e4a73465349711a9324767e1b2453383e2
-87c16e9e8bb74f14f4504305957e4346e7fc46ea refs/tags/v2.6.29-rc4
-^8e4921515c1a379539607eb443d51c30f4f7f338
-1dcda2df87ba4ecc7988be7a45d01645e11c9f4c refs/tags/v2.6.29-rc5
-^d2f8d7ee1a9b4650b4e43325b321801264f7c37a
-0715562512ca6cf14c1b8f08e09d5907118deaf0 refs/tags/v2.6.29-rc6
-^20f4d6c3a2a23c5d7d9cc7f42fbb943ca7a03d1f
-b21232ea962bbaf0e909365f4964f6cceb2ba8ce refs/tags/v2.6.29-rc7
-^fec6c6fec3e20637bee5d276fb61dd8b49a3f9cc
-73e37758f6b500a67d918528204832cc8f256516 refs/tags/v2.6.29-rc8
-^041b62374c7fedc11a8a1eeda2868612d3d1436c
-fa9c4d0983f98945b32d6bd0dfc1ba1b02d3773c refs/tags/v2.6.30
-^07a2039b8eb0af4ff464efd3dfd95de5c02648c6
-42ae7400074d449189d41fceb6d6f871490d7842 refs/tags/v2.6.30-rc1
-^577c9c456f0e1371cbade38eaf91ae8e8a308555
-7c941a7798a5169ee0dd69a9e8d5c40ceb702023 refs/tags/v2.6.30-rc2
-^0882e8dd3aad33eca41696d463bb896e6c8817eb
-b2fdb301af8f488952aaab7de3ff8d3294c3274f refs/tags/v2.6.30-rc3
-^091069740304c979f957ceacec39c461d0192158
-176c5e45fe4f1c83df9429b7c2668b41446baac2 refs/tags/v2.6.30-rc4
-^091438dd5668396328a3419abcbc6591159eb8d1
-4266b00c9dc3e1e071bde0ebfeadc40bbc1e8316 refs/tags/v2.6.30-rc5
-^091bf7624d1c90cec9e578a18529f615213ff847
-6492b02a211a5fd0fc92e68d171fc3644cda71a7 refs/tags/v2.6.30-rc6
-^1406de8e11eb043681297adf86d6892ff8efc27a
-efa3e68c670b745894255af9827b3902bbc9376e refs/tags/v2.6.30-rc7
-^59a3759d0fe8d969888c741bb33f4946e4d3750d
-e6c72abc9d239d788b0cdb20cb3d20ba04c33707 refs/tags/v2.6.30-rc8
-^9fa7eb283c5cdc2b0f4a8cfe6387ed82e5e9a3d3
-a271b16ba5478acead8773ebe01ee9b6365154d8 refs/tags/v2.6.31
-^74fca6a42863ffacaf7ba6f1936a9f228950f657
-eeadf87f8411b42b9803312d2870aa424602a99c refs/tags/v2.6.31-rc1
-^28d0325ce6e0a52f53d8af687e6427fee59004d3
-c5d511255186f0bba081f11cbe11c856cadaedf7 refs/tags/v2.6.31-rc2
-^8e4a718ff38d8539938ec3421935904c27e00c39
-c361304f4a17c05992b1cf68172a5ea8389649ee refs/tags/v2.6.31-rc3
-^6847e154e3cd74fca6084124c097980a7634285a
-12ae5c63b8a8ac621ddfc810f774d00cad44765a refs/tags/v2.6.31-rc4
-^4be3bd7849165e7efa6b0b35a23d6a3598d97465
-948e7a0b0e000a8d646b72fcfd8ccaa047e046ab refs/tags/v2.6.31-rc5
-^ed680c4ad478d0fee9740f7d029087f181346564
-73e5ade3a45e2096db80f0e87d6d838d0499f0fe refs/tags/v2.6.31-rc6
-^64f1607ffbbc772685733ea63e6f7f4183df1b16
-dcd5e628b7dc7f393dd728999fb0fb73a96d5e1b refs/tags/v2.6.31-rc7
-^422bef879e84104fee6dc68ded0e371dbeb5f88e
-da1ee8b71154887fe5298ae683c089050f2c49a5 refs/tags/v2.6.31-rc8
-^326ba5010a5429a5a528b268b36a5900d4ab0eba
-0827eb5df631ed1756653faf74248a9524f202cd refs/tags/v2.6.31-rc9
-^e07cccf4046978df10f2e13fe2b99b2f9b3a65db
-459b3d520991ec1b8e5ba68fbc4b206d602fee6e refs/tags/v2.6.32
-^22763c5cf3690a681551162c15d34d935308c8d7
-1016bf08944977a33d3a48edc15ee34b425f6d8a refs/tags/v2.6.32-rc1
-^17d857be649a21ca90008c6dc425d849fa83db5c
-1016bf08944977a33d3a48edc15ee34b425f6d8a refs/tags/v2.6.32-rc2
-^17d857be649a21ca90008c6dc425d849fa83db5c
-910eff4ec30f648f297700d43784b2159d35fb4f refs/tags/v2.6.32-rc3
-^374576a8b6f865022c0fd1ca62396889b23d66dd
-742a213497d587595f23674eafad1e520c5af6bd refs/tags/v2.6.32-rc4
-^161291396e76e0832c08f617eb9bd364d1648148
-c6add0a844533aeaa7bf86dcd4f924dca085d287 refs/tags/v2.6.32-rc5
-^012abeea669ea49636cf952d13298bb68654146a
-53a1963436f23ee8b6fa29a5ebcd925a9912594b refs/tags/v2.6.32-rc6
-^b419148e567728f6af0c3b01965c1cc141e3e13a
-5946e9740318d61b3d1c3d7bfdc3b54fc3ac181c refs/tags/v2.6.32-rc7
-^156171c71a0dc4bce12b4408bb1591f8fe32dc1a
-de8a4af91ab6fa3cde2618f4021b5faabdcd95ea refs/tags/v2.6.32-rc8
-^648f4e3e50c4793d9dbf9a09afa193631f76fa26
-4ac8e07ee3f251ae32329a24e0b01a316b21ead9 refs/tags/v2.6.33
-^60b341b778cc2929df16c0a504c91621b3c6a4ad
-d14e040a3592de665407269688d70296955c5f14 refs/tags/v2.6.33-rc1
-^55639353a0035052d9ea6cfe4dde0ac7fcbb2c9f
-331ce84170c8ebba5f0fadac64f66d6f00a438e4 refs/tags/v2.6.33-rc2
-^6b7b284958d47b77d06745b36bc7f36dab769d9b
-31fdc15b99788540d0ee8b8b337242e38489f603 refs/tags/v2.6.33-rc3
-^74d2e4f8d79ae0c4b6ec027958d5b18058662eea
-5ba7808eabc37cb2464096077d0df55f33148245 refs/tags/v2.6.33-rc4
-^7284ce6c9f6153d1777df5f310c959724d1bd446
-76efcb71c910774213480cdfe20b73e07c6a00aa refs/tags/v2.6.33-rc5
-^92dcffb916d309aa01778bf8963a6932e4014d07
-b02c43040da3d3e4f56d34d443c4c2a0d41da367 refs/tags/v2.6.33-rc6
-^abe94c756c08d50566c09a65b9c7fe72f83071c5
-600255d9e9ec0eecc49be78197c630504cf8c263 refs/tags/v2.6.33-rc7
-^29275254caedfedce960cfe6df24b90cb04fe431
-22ebef85968508e596dc60d970b716024cb9a34e refs/tags/v2.6.33-rc8
-^724e6d3fe8003c3f60bf404bf22e4e331327c596
-dba2e709efc365df385a762e763b51365403bc0f refs/tags/v2.6.34
-^e40152ee1e1c7a63f4777791863215e3faa37a86
-fe2098f5dff78881162f71ae028384435681a90a refs/tags/v2.6.34-rc1
-^57d54889cd00db2752994b389ba714138652e60c
-2fc56a2a7aa32adeddf7efe074b38cbdbb41894a refs/tags/v2.6.34-rc2
-^220bf991b0366cc50a94feede3d7341fa5710ee4
-fc2199137e85558e1f8f2bf44e0d3fa2b5cc4371 refs/tags/v2.6.34-rc3
-^2eaa9cfdf33b8d7fb7aff27792192e0019ae8fc6
-cb0cbeb16ff949783023da2270d5af36af416865 refs/tags/v2.6.34-rc4
-^0d0fb0f9c5fddef4a10242fe3337f00f528a3099
-d1744e136396b363e5844ed5f928e40067b5784a refs/tags/v2.6.34-rc5
-^01bf0b64579ead8a82e7cfc32ae44bc667e7ad0f
-1b5265c5905fdc68873d37c902adc4aec2cbd6a3 refs/tags/v2.6.34-rc6
-^66f41d4c5c8a5deed66fdcc84509376c9a0bf9d8
-3884cbfac5cf610a7116ca90a970a11cf495bb83 refs/tags/v2.6.34-rc7
-^b57f95a38233a2e73b679bea4a5453a1cc2a1cc9
-d786bf1b8352facdde8cc28725e1e6e067e1854d refs/tags/v2.6.35
-^9fe6206f400646a2322096b56c59891d530e8d51
-77cb4411c86825cf693e338fada52dfca3345668 refs/tags/v2.6.35-rc1
-^67a3e12b05e055c0415c556a315a3d3eb637e29e
-06006d039fbf390b87b5ee76d16187b5cb9b3f4d refs/tags/v2.6.35-rc2
-^e44a21b7268a022c7749f521c06214145bd161e4
-f51882c32dcbec0befa48e12682f1651c4772b7b refs/tags/v2.6.35-rc3
-^7e27d6e778cd87b6f2415515d7127eba53fe5d02
-55acf6b6533581f03df07ddd166c6631bc304845 refs/tags/v2.6.35-rc4
-^815c4163b6c8ebf8152f42b0a5fd015cfdcedc78
-1dd55af06d281ffbc945cd7bb21cfb98d6e84a99 refs/tags/v2.6.35-rc5
-^1c5474a65bf15a4cb162dfff86d6d0b5a08a740c
-37a3bd34ba0892f0f1c1a0a03b06d61ed3c554c1 refs/tags/v2.6.35-rc6
-^b37fa16e78d6f9790462b3181602a26b5af36260
-25427f38d3b791d986812cb81c68df38e8249ef8 refs/tags/v2.6.36
-^f6f94e2ab1b33f0082ac22d71f66385a60d8157f
-8ed88d401f908a594cd74a4f2513b0fabd32b699 refs/tags/v2.6.36-rc1
-^da5cabf80e2433131bf0ed8993abc0f7ea618c73
-58d3707b8891f71d4891e6b36129eeacd3ba63f4 refs/tags/v2.6.36-rc2
-^76be97c1fc945db08aae1f1b746012662d643e97
-40f7ec041a61c6b6d419e418818c79f7c23a1007 refs/tags/v2.6.36-rc3
-^2bfc96a127bc1cc94d26bfaa40159966064f9c8c
-8607f6908a65fbd41d8eee6d0572425182eced69 refs/tags/v2.6.36-rc4
-^49553c2ef88749dd502687f4eb9c258bb10a4f44
-f4d2c86897046fb2dd9680b3446dfcc17a11e7f4 refs/tags/v2.6.36-rc5
-^b30a3f6257ed2105259b404d419b4964e363928c
-93590f17a2e3ba2aed400c7608263b97da62b6d4 refs/tags/v2.6.36-rc6
-^899611ee7d373e5eeda08e9a8632684e1ebbbf00
-f3f2d2543afa76bcc13a58fc6a1ff723f28890da refs/tags/v2.6.36-rc7
-^cb655d0f3d57c23db51b981648e452988c0223f9
-7619e63f48822b2c68d0e108677340573873fb93 refs/tags/v2.6.36-rc8
-^cd07202cc8262e1669edff0d97715f3dd9260917
-4a7895f41220ed60e97b736ac1b92e589e67b263 refs/tags/v2.6.37
-^3c0eee3fe6a3a1c745379547c7e7c904aa64f6d5
-579cc3d8b5840f3355bad58e7ab23eae04ff9cb6 refs/tags/v2.6.37-rc1
-^c8ddb2713c624f432fa5fe3c7ecffcdda46ea0d4
-a1e4aaecda4a52e850e90d4f95ed2f59955ed057 refs/tags/v2.6.37-rc2
-^e53beacd23d9cb47590da6a7a7f6d417b941a994
-8a198c8196ac3716e1856978d3e18cca5d800ef3 refs/tags/v2.6.37-rc3
-^3561d43fd289f590fdae672e5eb831b8d5cf0bf6
-78d3494aa9e5cbd28600b5440655b263ab1c0efd refs/tags/v2.6.37-rc4
-^e8a7e48bb248a1196484d3f8afa53bded2b24e71
-f9459ed00525f440d25d7c88fe8d52abd3746066 refs/tags/v2.6.37-rc5
-^cf7d7e5a1980d1116ee152d25dac382b112b9c17
-3f4ddf273ce92382ad3ce55fde3d773bd9e4bddd refs/tags/v2.6.37-rc6
-^b0c3844d8af6b9f3f18f31e1b0502fbefa2166be
-6261d1ea5bdbc9baf84c192242c82e63cdb02788 refs/tags/v2.6.37-rc7
-^90a8a73c06cc32b609a880d48449d7083327e11a
-5720e551140dd586156e9875b0d1ef528e9c5f59 refs/tags/v2.6.37-rc8
-^387c31c7e5c9805b0aef8833d1731a5fe7bdea14
-fbeb94b65cf784ed8bf852131e28c9fb5c4c760f refs/tags/v2.6.38
-^521cb40b0c44418a4fd36dc633f575813d59a43d
-aa6741cead5660406a453c01b6e37cbe06d52433 refs/tags/v2.6.38-rc1
-^c56eb8fb6dccb83d9fe62fd4dc00c834de9bc470
-182a43159a3ea400594ca05ce7ff5052950a1010 refs/tags/v2.6.38-rc2
-^1bae4ce27c9c90344f23c65ea6966c50ffeae2f5
-70f1e06182fab9290a1f7775ade996e4854dec3a refs/tags/v2.6.38-rc3
-^ebf53826e105f488f4f628703a108e98940d1dc5
-901069c5c5b155322539a94cf337c378848e435a refs/tags/v2.6.38-rc4
-^100b33c8bd8a3235fd0b7948338d6cbb3db3c63d
-656372ebed8acf941bb63d08abb250c9896785a8 refs/tags/v2.6.38-rc5
-^85e2efbb1db9a18d218006706d6e4fbeb0216213
-86483dddb24b8d5624c38362f820211c694473ba refs/tags/v2.6.38-rc6
-^f5412be599602124d2bdd49947b231dd77c0bf99
-295dd79b22916ed71a641dd80ee4d8b07c624feb refs/tags/v2.6.38-rc7
-^dd9c1549edef02290edced639f67b54a25abbe0e
-44986a8d6d6d024d9422dbb91635600862921ed3 refs/tags/v2.6.38-rc8
-^a5abba989deceb731047425812d268daf7536575
-8b0753a3df28c21b0570fa21362c5f1b3b4f59bf refs/tags/v2.6.39
-^61c4f2c81c61f73549928dfd9f3e8f26aa36a8cf
-5cb75c01f17f69d6ac102d58766f7fb30269a5c9 refs/tags/v2.6.39-rc1
-^0ce790e7d736cedc563e1fb4e998babf5a4dbc3d
-76af3715abf44176ae1ac9cf5cb9f3861d39eee1 refs/tags/v2.6.39-rc2
-^6221f222c0ebf1acdf7abcf927178f40e1a65e2a
-074bd9a1615dd4c0108b0424a50770d1b605b53e refs/tags/v2.6.39-rc3
-^a6360dd37e1a144ed11e6548371bade559a1e4df
-c8e761132679d935b5d3edd06e48db2bd3eb918a refs/tags/v2.6.39-rc4
-^f0e615c3cb72b42191b558c130409335812621d8
-149b78658628901a6e578566a45f159d0b38ce2f refs/tags/v2.6.39-rc5
-^8e10cd74342c7f5ce259cceca36f6eba084f5d58
-8630f22a089f0d777893ee6a53eb3e6acef06044 refs/tags/v2.6.39-rc6
-^0ee5623f9a6e52df90a78bd21179f8ab370e102e
-2c2138749b9dc99a1f26b212eb373458ea179e06 refs/tags/v2.6.39-rc7
-^693d92a1bbc9e42681c42ed190bd42b636ca876f
-4204bcde7c0b93c5e127eb868e17b337a513cf34 refs/tags/v3.0
-^02f8c6aee8df3cdc935e9bdd4f2d020306035dbe
-2a23a510142a1ab597f0214e4fadb3c7350bbb8d refs/tags/v3.0-rc1
-^55922c9d1b84b89cb946c777fddccb3247e7df2c
-eb73a032155ba51adb229be1a2ad3ac08951b485 refs/tags/v3.0-rc2
-^59c5f46fbe01a00eedf54a23789634438bb80603
-a73e7344f7a7712628f40f851fa835ea82025d7b refs/tags/v3.0-rc3
-^2c53b436a30867eb6b47dd7bab23ba638d1fb0d2
-4921f03412bd0a91cb242b259618b13dfae1acf6 refs/tags/v3.0-rc4
-^56299378726d5f2ba8d3c8cbbd13cb280ba45e4f
-c699212fde57ea66926c4a0b6ebbe4d62c52e0b9 refs/tags/v3.0-rc5
-^b0af8dfdd67699e25083478c63eedef2e72ebd85
-8a79d68b5a847beef3f70b2ed16b2a757572cc5e refs/tags/v3.0-rc6
-^fe0d42203cb5616eeff68b14576a0f7e2dd56625
-394d6f903ac6889fa50ca5c19111ae659d524b4c refs/tags/v3.0-rc7
-^620917de59eeb934b9f8cf35cc2d95c1ac8ed0fc
-a732360658fc100add6ded23ac6c8013b50cef1f refs/tags/v3.1
-^c3b92c8787367a8bb53d57d9789b558f1295cc96
-dd073a5b13967fa83038ed1594ecae5faaed71a2 refs/tags/v3.1-rc1
-^322a8b034003c0d46d39af85bf24fee27b902f48
-bc9dac81d1d3442713e5b91ed7cda1646df9730e refs/tags/v3.1-rc10
-^899e3ee404961a90b828ad527573aaaac39f0ab1
-fbba4cec036b7027b84c00547361bd8127254b1e refs/tags/v3.1-rc2
-^93ee7a9340d64f20295aacc3fb6a22b759323280
-ae30e7e9c4d8cd135ec429191f9eec746709eccc refs/tags/v3.1-rc3
-^fcb8ce5cfe30ca9ca5c9a79cdfe26d1993e65e0c
-897e5ed317d229e937731d6389f8f51c7e29e62e refs/tags/v3.1-rc4
-^c6a389f123b9f68d605bb7e0f9b32ec1e3e14132
-dba5cf0231a4c3091c0f3b7236c6978dab8cbc97 refs/tags/v3.1-rc5
-^ddf28352b80c86754a6424e3a61e8bdf9213b3c7
-8ff02915cf6ca8f5ecda04e0a2150507df89846b refs/tags/v3.1-rc6
-^b6fd41e29dea9c6753b1843a77e50433e6123bcb
-4cf670b4bbdb31291ffa52ad3f65acd8cd2eb20d refs/tags/v3.1-rc7
-^d93dc5c4478c1fd5de85a3e8aece9aad7bbae044
-e7f2a88548df497544d4fafca79836bbd8b6342e refs/tags/v3.1-rc8
-^a102a9ece5489e1718cd7543aa079082450ac3a2
-38a181c9f494c81d7d2327861621c0e04018bc6a refs/tags/v3.1-rc9
-^976d167615b64e14bc1491ca51d424e2ba9a5e84
-bc1b510b8979ecc322f8d930dde56658967b7355 refs/tags/v3.10
-^8bb495e3f02401ee6f76d1b1d77f3ac9f079e376
-1fd7e50e9cbdabce5a0ff00cfb8767b948d86eb7 refs/tags/v3.10-rc1
-^f722406faae2d073cc1d01063d1123c35425939e
-a8c6d53c4d84b3a5eb182758a9cdceceba4691da refs/tags/v3.10-rc2
-^c7788792a5e7b0d5d7f96d0766b4cb6112d47d75
-74f675aa9f9c2f2d52de4a00144b64f5a36637ac refs/tags/v3.10-rc3
-^e4aa937ec75df0eea0bee03bffa3303ad36c986b
-846830e71929e8728b0209e04259ca3e2b2f99c9 refs/tags/v3.10-rc4
-^d683b96b072dc4680fc74964eca77e6a23d1fa6e
-ee3e35b1aa057646c166780d13bfe8e57ed5d4cf refs/tags/v3.10-rc5
-^317ddd256b9c24b0d78fa8018f80f1e495481a10
-c598c40bcc18e93e63e3f251d40234478195d03a refs/tags/v3.10-rc6
-^7d132055814ef17a6c7b69f342244c410a5e000f
-e6995de3dfc1b122c2ce08522909ced160c960d9 refs/tags/v3.10-rc7
-^9e895ace5d82df8929b16f58e9f515f6d54ab82d
-bedaca4e311e2c2abe0a215ee2b25c133e435211 refs/tags/v3.11
-^6e4664525b1db28f8c4e1130957f70a94c19213e
-8d339e724c3dc1ff81ad341b201e5b273c594d75 refs/tags/v3.11-rc1
-^ad81f0545ef01ea651886dddac4bef6cec930092
-c97e1dd9202cbfdf214208c934a021ac73b5a7d4 refs/tags/v3.11-rc2
-^3b2f64d00c46e1e4e9bd0bb9bb12619adac27a4b
-92f65bf9283c618ce157027b2fbcee82156b4c5c refs/tags/v3.11-rc3
-^5ae90d8e467e625e447000cb4335c4db973b1095
-458a49f2e1c379ab07a3fd59894ae7050360b1fc refs/tags/v3.11-rc4
-^c095ba7224d8edc71dcef0d655911399a8bd4a3f
-e49ee8a9d47753e7cd0f6e1827a95c436bc7544c refs/tags/v3.11-rc5
-^d4e4ab86bcba5a72779c43dc1459f71fea3d89c8
-2ea699d98cd6f9e9b813c24542d581dedacdc659 refs/tags/v3.11-rc6
-^b36f4be3de1b123d8601de062e7dbfc904f305fb
-99c0aead9dbef77b6a09f11d34b6867f3b352efe refs/tags/v3.11-rc7
-^d8dfad3876e4386666b759da3c833d62fb8b2267
-71572a0f766f454071277744c98cab00dad5efc9 refs/tags/v3.12
-^5e01dc7b26d9f24f39abace5da98ccbd6a5ceb52
-c59c557446a6ddb3260df207048d7b0a876d981c refs/tags/v3.12-rc1
-^272b98c6455f00884f0350f775c5342358ebb73f
-e271e342a6202340459998d713188d1ad6d32cb6 refs/tags/v3.12-rc2
-^4a10c2ac2f368583138b774ca41fac4207911983
-59ca6c45dd68f79fc0470aafcf98fc64efc0d24d refs/tags/v3.12-rc3
-^15c03dd4859ab16f9212238f29dd315654aa94f6
-3b6a2139d8b69bfc3a95a7b5252967a548c88597 refs/tags/v3.12-rc4
-^d0e639c9e06d44e713170031fe05fb60ebe680af
-3ec4b24ffb064e6fa91b61471e874027b8e9fbd2 refs/tags/v3.12-rc5
-^61e6cfa80de5760bbe406f4e815b7739205754d2
-19ef23a1f211ed17cad3bfbe0539ec6f38c8ff63 refs/tags/v3.12-rc6
-^31d141e3a666269a3b6fcccddb0351caf7454240
-af03676423fe9b4beefafcb8cd4a80164d4b0bd7 refs/tags/v3.12-rc7
-^959f58544b7f20c92d5eb43d1232c96c15c01bfb
-0e2c4c21a56b8f820bab218566b02ea728909f45 refs/tags/v3.13
-^d8ec26d7f8287f5788a494f56e8814210f0e64be
-1c2570ad1851fffa1e34d8b74d2349b09a3e8c2a refs/tags/v3.13-rc1
-^6ce4eac1f600b34f2f7f58f9cd8f0503d79e42ae
-74c87395184969ce9f20e902f756cc7256ad2c20 refs/tags/v3.13-rc2
-^dc1ccc48159d63eca5089e507c82c7d22ef60839
-1ae260b036bb223c11cfcd46698ea46a90cd336c refs/tags/v3.13-rc3
-^374b105797c3d4f29c685f3be535c35f5689b30e
-00c58b88ff7f7f6503e2021def13498a6846a849 refs/tags/v3.13-rc4
-^319e2e3f63c348a9b66db4667efa73178e18b17d
-5be6b0c63d1d45c8ad134b19053aaf79e9161218 refs/tags/v3.13-rc5
-^413541dd66d51f791a0b169d9b9014e4f56be13c
-13ecbebf2553bb6c5f19457e78a6c8f2a1732151 refs/tags/v3.13-rc6
-^802eee95bde72fd0cd0f3a5b2098375a487d1eda
-66d8d623aca0b26934afe85f5b05eb930d0166d0 refs/tags/v3.13-rc7
-^d6e0a2dd12f4067a5bcefb8bbd8ddbeff800afbc
-210fd8c5c324253b0d6c852d161017aeb65f70bd refs/tags/v3.13-rc8
-^7e22e91102c6b9df7c4ae2168910e19d2bb14cd6
-08499ec1a4c8d859c71564d56890442f2f1ac8ed refs/tags/v3.14-rc1
-^38dbfb59d1175ef458d006556061adeaa8751b72
-97112cce341931fb992ce6afc9d3a9beff9ad81c refs/tags/v3.14-rc2
-^b28a960c42fcd9cfc987441fa6d1c1a471f0f9ed
-0f0e0c41b94db3caa68d5111347e699f6cf84e70 refs/tags/v3.14-rc3
-^6d0abeca3242a88cab8232e4acd7e2bf088f3bc2
-d018bf5252f1041f8839022e3ab38a1bc40d3868 refs/tags/v3.2
-^805a6af8dba5dfdd35ec35dc52ec0122400b2610
-4ea8b996e5dd18145d945667fbcd9a6af234a30c refs/tags/v3.2-rc1
-^1ea6b8f48918282bdca0b32a34095504ee65bab5
-68b7d2ca89b1ce708f40ebd45d19fcf982cc6b38 refs/tags/v3.2-rc2
-^cfcfc9eca2bcbd26a8e206baeb005b055dbf8e37
-e0f79b438798057b03d1a06704d63c8e4297b053 refs/tags/v3.2-rc3
-^caca6a03d365883564885f2c1da3e88dcf65d139
-96c50634c8fc57b7a4a5d84691a18c173268251e refs/tags/v3.2-rc4
-^5611cc4572e889b62a7b4c72a413536bf6a9c416
-10fc295c8cd3c5b9d7d271b4875679ee294afbae refs/tags/v3.2-rc5
-^dc47ce90c3a822cd7c9e9339fe4d5f61dcb26b50
-5292d98194346528607da828962c7e8677c27e15 refs/tags/v3.2-rc6
-^384703b8e6cd4c8ef08512e596024e028c91c339
-e33abea7c57a228c8fdf77c892c230cb840425ac refs/tags/v3.2-rc7
-^5f0a6e2d503896062f641639dacfe5055c2f593b
-353f301f06485635f3a7789c2787da50a34c49b5 refs/tags/v3.3
-^c16fa4f2ad19908a47c63d8fa436a1178438c7e7
-0803b590443c714345e41c77b6e2f1d6c92243ec refs/tags/v3.3-rc1
-^dcd6c92267155e70a94b3927bce681ce74b80d1f
-8bd4e2a2ba139fcdab2ab9bbdd76e4df4d66d7fd refs/tags/v3.3-rc2
-^62aa2b537c6f5957afd98e29f96897419ed5ebab
-72cdd65517c6463e5578040976c166b33bcfc14a refs/tags/v3.3-rc3
-^d65b4e98d7ea3038b767b70fe8be959b2913f16d
-b87f9498998dbefd82fa7b3d6414454600481889 refs/tags/v3.3-rc4
-^b01543dfe67bb1d191998e90d20534dc354de059
-13aae9ead734a49a5486907f7dc290b2ea50a6b9 refs/tags/v3.3-rc5
-^6b21d18ed50c7d145220b0724ea7f2613abf0f95
-d9234c2ff6f2a50075dd37fb859e17bfb185d17c refs/tags/v3.3-rc6
-^192cfd58774b4d17b2fe8bdc77d89c2ef4e0591d
-b2e4909898710f070173a108f25e6e135b5bdebd refs/tags/v3.3-rc7
-^fde7d9049e55ab85a390be7f415d74c9f62dd0f9
-27406e978f596a646b87941cba5247eb0ccc8916 refs/tags/v3.4
-^76e10d158efb6d4516018846f60c2ab5501900bc
-b84516a747a9754fc77668a23cbf6203cf2c05d2 refs/tags/v3.4-rc1
-^dd775ae2549217d3ae09363e3edb305d0fa19928
-4029716364bee5d89323c24dc343c0dade05c5e8 refs/tags/v3.4-rc2
-^0034102808e0dbbf3a2394b82b1bb40b5778de9e
-0690bdc7e124d9ef5da67cc75f0ea9952a5427d7 refs/tags/v3.4-rc3
-^e816b57a337ea3b755de72bec38c10c864f23015
-2263a9c8844371b175e061728bd7acb8fd188c4f refs/tags/v3.4-rc4
-^66f75a5d028beaf67c931435fdc3e7823125730c
-c3bf00ad560c0ccf0e6a36d922cc99e9de10fb47 refs/tags/v3.4-rc5
-^69964ea4c7b68c9399f7977aa5b9aa6539a6a98a
-2b1e9fec303b3f2f138edcefc5f048d666e56166 refs/tags/v3.4-rc6
-^d48b97b403d23f6df0b990cee652bdf9a52337a3
-5842651b3b71b4873d020e3e16e238b76fdfa90c refs/tags/v3.4-rc7
-^36be50515fe2aef61533b516fa2576a2c7fe7664
-f34fad35283333561c88f4507c4fa1849f3451a1 refs/tags/v3.5
-^28a33cbc24e4256c143dce96c7d93bf423229f92
-04f4dfa6bbb4df376ed50599de43bf5181241602 refs/tags/v3.5-rc1
-^f8f5701bdaf9134b1f90e5044a82c66324d2073f
-41827bc48b4ead01918d9a8ee2f7307a886c3a1a refs/tags/v3.5-rc2
-^cfaf025112d3856637ff34a767ef785ef5cf2ca9
-111ecbd538d2d3542e2e54a666f04c11bb14ed83 refs/tags/v3.5-rc3
-^485802a6c524e62b5924849dd727ddbb1497cc71
-379f9f503b99360d6fa269947eefc67492641c28 refs/tags/v3.5-rc4
-^6b16351acbd415e66ba16bf7d473ece1574cf0bc
-b26e78e3ef49a32fe2267fb194d740cabec35b78 refs/tags/v3.5-rc5
-^6887a4131da3adaab011613776d865f4bcfb5678
-a94b851ad29264f83dd48229090cf95e6394b0d5 refs/tags/v3.5-rc6
-^bd0a521e88aa7a06ae7aabaed7ae196ed4ad867a
-ac143858efa89977a70ff375ac9c1bedba17691d refs/tags/v3.5-rc7
-^84a1caf1453c3d44050bd22db958af4a7f99315c
-255595ebd4692ca21dcdbeef2b648204d0b8ee63 refs/tags/v3.6
-^a0d271cbfed1dd50278c6b06bead3d00ba0a88f9
-f85125cc169d2379a55f170270ef3f0adf260409 refs/tags/v3.6-rc1
-^0d7614f09c1ebdbaa1599a5aba7593f147bf96ee
-d1a4d2f3a903dbd3b274cac81d53c138748ef309 refs/tags/v3.6-rc2
-^d9875690d9b89a866022ff49e3fcea892345ad92
-c8d47765843dba9357904ee626a46f949e1ee2ca refs/tags/v3.6-rc3
-^fea7a08acb13524b47711625eebea40a0ede69a0
-f8042a627ac90fb99c89a89cc7a38650528ab76f refs/tags/v3.6-rc4
-^4cbe5a555fa58a79b6ecbb6c531b8bab0650778d
-4678fc83dcdd811ec2b893b087d2a08c8530d7b2 refs/tags/v3.6-rc5
-^55d512e245bc7699a8800e23df1a24195dd08217
-895fdd1a25e399348c1fdc9be85dd847c302c275 refs/tags/v3.6-rc6
-^5698bd757d55b1bb87edd1a9744ab09c142abfc2
-cbdc1ccf2692ee9d0f50fe4911fa474b7ac2882c refs/tags/v3.6-rc7
-^979570e02981d4a8fc20b3cc8fd651856c98ee9d
-d4f28c153f820851f9c765ac9bc12dffe90accae refs/tags/v3.7
-^29594404d7fe73cd80eaa4ee8c43dcc53970c60e
-b93229f2cc1b8bb260135c3da61733ed92568fd3 refs/tags/v3.7-rc1
-^ddffeb8c4d0331609ef2581d84de4d763607bd37
-966b4b0842a6cbbfe8ce8165865b7c4fa993df32 refs/tags/v3.7-rc2
-^6f0c0580b70c89094b3422ba81118c7b959c7556
-47193149164f56a5b2cebe803cdf6bde56971323 refs/tags/v3.7-rc3
-^8f0d8163b50e01f398b14bcd4dc039ac5ab18d64
-cf5c3c9cee716beddb7f2539a7f7bc0415239ce6 refs/tags/v3.7-rc4
-^3d70f8c617a436c7146ecb81df2265b4626dfe89
-167e404cd31e98ab4fa56d8a4d774e4d9fdb5191 refs/tags/v3.7-rc5
-^77b67063bb6bce6d475e910d3b886a606d0d91f7
-60c890ec8f52b5d6369164dbb2212b2273786866 refs/tags/v3.7-rc6
-^f4a75d2eb7b1e2206094b901be09adb31ba63681
-c4f5fad00840768d96c6999947ca50b8c91e70a6 refs/tags/v3.7-rc7
-^9489e9dcae718d5fde988e4a684a0f55b5f94d17
-61cbb5be523d9b12da6596d5db90965d6ecd70d4 refs/tags/v3.7-rc8
-^b69f0859dc8e633c5d8c06845811588fe17e68b3
-a788f85fbb206352319787d9dc1a5e62d87f74d4 refs/tags/v3.8
-^19f949f52599ba7c3f67a5897ac6be14bfcb1200
-6391acbd01acb5cde61dd54baf9caef2ae58d98c refs/tags/v3.8-rc1
-^a49f0d1ea3ec94fc7cf33a7c36a16343b74bd565
-bfad7d20d7080bdba4c0e864fcbdf55baa2d1de2 refs/tags/v3.8-rc2
-^d1c3ed669a2d452cacfb48c2d171a1f364dae2ed
-d7150026d19ce56fb285b3d812cd6d5e0bfbe262 refs/tags/v3.8-rc3
-^9931faca02c604c22335f5a935a501bb2ace6e20
-16fc6c0d406528d25f8ef0ec2c2a543b46a5fba1 refs/tags/v3.8-rc4
-^7d1f9aeff1ee4a20b1aeb377dd0f579fe9647619
-b67f19a416919ae48c98105a7e32886b15f6e0d8 refs/tags/v3.8-rc5
-^949db153b6466c6f7cad5a427ecea94985927311
-442d6ffcf0920b0631fb9b50d665a991c9e16e73 refs/tags/v3.8-rc6
-^88b62b915b0b7e25870eb0604ed9a92ba4bfc9f7
-afa17d15e4cf8ee70ad475627721bf4385a4a387 refs/tags/v3.8-rc7
-^836dc9e3fbbab0c30aa6e664417225f5c1fb1c39
-f73366b472547dd3fe6f4fb71d003773bb19d1d6 refs/tags/v3.9
-^c1be5a5b1b355d40e6cf79cc979eb66dafa24ad1
-d29d73e064b457fbb6077e01e9516c96bc075279 refs/tags/v3.9-rc1
-^6dbe51c251a327e012439c4772097a13df43c5b8
-e4ae8588b9e3b14b60d746ec1c9381a565c2baa3 refs/tags/v3.9-rc2
-^f6161aa153581da4a3867a2d1a7caf4be19b6ec9
-c7664f420fc8d902676e1c8a34acfc5777bf8ce8 refs/tags/v3.9-rc3
-^a937536b868b8369b98967929045f1df54234323
-d3318b414811d5393b4535ab6cb5aeddb57a4e5b refs/tags/v3.9-rc4
-^8bb9660418e05bb1845ac1a2428444d78e322cc7
-c67bf5361e7e66a0ff1f4caf95f89347d55dfb89 refs/tags/v3.9-rc5
-^07961ac7c0ee8b546658717034fe692fd12eefa9
-2c8edfa75a6f7de3abc6061c5f665735de07dbee refs/tags/v3.9-rc6
-^31880c37c11e28cb81c70757e38392b42e695dc6
-c03c4b22b8b29c522d986a1e5440e35eb2d08aa7 refs/tags/v3.9-rc7
-^41ef2d5678d83af030125550329b6ae8b74618fa
-eec112d8fed9b29bd17c023975efd68b7f8b020b refs/tags/v3.9-rc8
-^60d509fa6a9c4653a86ad830e4c4b30360b23f0e
diff -Nurp linux/.git/refs/heads/bv3.7-rc6 sched-deadline-mainline-dl/.git/refs/heads/bv3.7-rc6
--- linux/.git/refs/heads/bv3.7-rc6	2014-02-17 14:46:45.338904749 -0500
+++ sched-deadline-mainline-dl/.git/refs/heads/bv3.7-rc6	1969-12-31 19:00:00.000000000 -0500
@@ -1 +0,0 @@
-f4a75d2eb7b1e2206094b901be09adb31ba63681
diff -Nurp linux/.git/refs/heads/master sched-deadline-mainline-dl/.git/refs/heads/master
--- linux/.git/refs/heads/master	2014-02-17 14:27:41.802671887 -0500
+++ sched-deadline-mainline-dl/.git/refs/heads/master	1969-12-31 19:00:00.000000000 -0500
@@ -1 +0,0 @@
-6d0abeca3242a88cab8232e4acd7e2bf088f3bc2
diff -Nurp linux/.git/refs/remotes/origin/HEAD sched-deadline-mainline-dl/.git/refs/remotes/origin/HEAD
--- linux/.git/refs/remotes/origin/HEAD	2014-02-17 14:27:41.757670795 -0500
+++ sched-deadline-mainline-dl/.git/refs/remotes/origin/HEAD	1969-12-31 19:00:00.000000000 -0500
@@ -1 +0,0 @@
-ref: refs/remotes/origin/master
diff -Nurp linux/include/asm-generic/math128.h sched-deadline-mainline-dl/include/asm-generic/math128.h
--- linux/include/asm-generic/math128.h	1969-12-31 19:00:00.000000000 -0500
+++ sched-deadline-mainline-dl/include/asm-generic/math128.h	2012-11-21 15:11:30.000000000 -0500
@@ -0,0 +1,4 @@
+#ifndef _ASM_GENERIC_MATH128_H
+#define _ASM_GENERIC_MATH128_H
+
+#endif /*_ASM_GENERIC_MATH128_H */
diff -Nurp linux/include/asm-generic/resource.h sched-deadline-mainline-dl/include/asm-generic/resource.h
--- linux/include/asm-generic/resource.h	2014-02-17 14:29:20.628070288 -0500
+++ sched-deadline-mainline-dl/include/asm-generic/resource.h	2012-11-21 15:11:30.000000000 -0500
@@ -3,7 +3,6 @@
 
 #include <uapi/asm-generic/resource.h>
 
-
 /*
  * boot-time rlimit defaults for the init task:
  */
diff -Nurp linux/include/linux/init_task.h sched-deadline-mainline-dl/include/linux/init_task.h
--- linux/include/linux/init_task.h	2014-02-17 14:45:52.301603636 -0500
+++ sched-deadline-mainline-dl/include/linux/init_task.h	2012-11-21 15:11:30.000000000 -0500
@@ -10,6 +10,7 @@
 #include <linux/pid_namespace.h>
 #include <linux/user_namespace.h>
 #include <linux/securebits.h>
+#include <linux/rbtree.h>
 #include <net/net_namespace.h>
 
 #ifdef CONFIG_SMP
@@ -143,6 +144,14 @@ extern struct task_group root_task_group
 
 #define INIT_TASK_COMM "swapper"
 
+#ifdef CONFIG_RT_MUTEXES
+# define INIT_RT_MUTEXES(tsk)						\
+	.pi_waiters = RB_ROOT,						\
+	.pi_waiters_leftmost = NULL,
+#else
+# define INIT_RT_MUTEXES(tsk)
+#endif
+
 /*
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
@@ -210,6 +219,7 @@ extern struct task_group root_task_group
 	INIT_TRACE_RECURSION						\
 	INIT_TASK_RCU_PREEMPT(tsk)					\
 	INIT_CPUSET_SEQ							\
+	INIT_RT_MUTEXES(tsk)						\
 }
 
 
diff -Nurp linux/include/linux/math128.h sched-deadline-mainline-dl/include/linux/math128.h
--- linux/include/linux/math128.h	1969-12-31 19:00:00.000000000 -0500
+++ sched-deadline-mainline-dl/include/linux/math128.h	2012-11-21 15:11:30.000000000 -0500
@@ -0,0 +1,180 @@
+#ifndef _LINUX_MATH128_H
+#define _LINUX_MATH128_H
+
+#include <linux/types.h>
+
+typedef union {
+	struct {
+#if __BYTE_ORDER__  == __ORDER_LITTLE_ENDIAN__
+		u64 lo, hi;
+#else
+		u64 hi, lo;
+#endif
+	};
+#ifdef __SIZEOF_INT128__ /* gcc-4.6+ */
+	unsigned __int128 val;
+#endif
+} u128;
+
+#define U128_INIT(_hi, _lo) (u128){{ .hi = (_hi), .lo = (_lo) }}
+
+#include <asm/math128.h>
+
+/*
+ * Make usage of __int128 dependent on arch code so they can
+ * judge if gcc is doing the right thing for them and can over-ride
+ * any funnies.
+ */
+
+#ifndef ARCH_HAS_INT128
+
+#ifndef add_u128
+static inline u128 add_u128(u128 a, u128 b)
+{
+	a.hi += b.hi;
+	a.lo += b.lo;
+	if (a.lo < b.lo)
+		a.hi++;
+
+	return a;
+}
+#endif /* add_u128 */
+
+#ifndef mul_u64_u64
+extern u128 mul_u64_u64(u64 a, u64 b);
+#endif
+
+#ifndef mul_u64_u32_shr
+static inline u64 mul_u64_u32_shr(u64 a, u32 mul, unsigned int shift)
+{
+	u32 ah, al;
+	u64 t1, t2;
+
+	ah = a >> 32;
+	al = a;
+
+	t1 = ((u64)al * mul) >> shift;
+	t2 = ((u64)ah * mul) << (32 - shift);
+
+	return t1 + t2;
+}
+#endif /* mul_u64_u32_shr */
+
+#ifndef shl_u128
+static inline u128 shl_u128(u128 x, unsigned int n)
+{
+	u128 res;
+
+	if (!n)
+		return x;
+
+	if (n < 64) {
+		res.hi = x.hi << n;
+		res.hi |= x.lo >> (64 - n);
+		res.lo = x.lo << n;
+	} else {
+		res.lo = 0;
+		res.hi = x.lo << (n - 64);
+	}
+
+	return res;
+}
+#endif /* shl_u128 */
+
+#ifndef shr_u128
+static inline u128 shr_u128(u128 x, unsigned int n)
+{
+	u128 res;
+
+	if (!n)
+		return x;
+
+	if (n < 64) {
+		res.lo = x.lo >> n;
+		res.lo |= x.hi << (64 - n);
+		res.hi = x.hi >> n;
+	} else {
+		res.hi = 0;
+		res.lo = x.hi >> (n - 64);
+	}
+
+	return res;
+}
+#endif /* shr_u128 */
+
+#ifndef cmp_u128
+static inline int cmp_u128(u128 a, u128 b)
+{
+	if (a.hi > b.hi)
+		return 1;
+	if (a.hi < b.hi)
+		return -1;
+	if (a.lo > b.lo)
+		return 1;
+	if (a.lo < b.lo)
+		return -1;
+
+	return 0;
+}
+#endif /* cmp_u128 */
+
+#else /* ARCH_HAS_INT128 */
+
+#ifndef add_u128
+static inline u128 add_u128(u128 a, u128 b)
+{
+	a.val += b.val;
+	return a;
+}
+#endif /* add_u128 */
+
+#ifndef mul_u64_u64
+static inline u128 mul_u64_u64(u64 a, u64 b)
+{
+	u128 res;
+
+	res.val = a;
+	res.val *= b;
+
+	return res;
+}
+#define mul_u64_u64 mul_u64_u64
+#endif
+
+#ifndef mul_u64_u32_shr
+static inline u64 mul_u64_u32_shr(u64 a, u32 mul, unsigned int shift)
+{
+	return (u64)(((unsigned __int128)a * mul) >> shift);
+}
+#endif /* mul_u64_u32_shr */
+
+#ifndef shl_u128
+static inline u128 shl_u128(u128 x, unsigned int n)
+{
+	x.val <<= n;
+	return x;
+}
+#endif /* shl_u128 */
+
+#ifndef shr_u128
+static inline u128 shr_u128(u128 x, unsigned int n)
+{
+	x.val >>= n;
+	return x;
+}
+#endif /* shr_u128 */
+
+#ifndef cmp_u128
+static inline int cmp_u128(u128 a, u128 b)
+{
+	if (a.val < b.val)
+		return -1;
+	if (a.val > b.val)
+		return 1;
+	return 0;
+}
+#endif /* cmp_u128 */
+
+#endif /* ARCH_HAS_INT128 */
+
+#endif /* _LINUX_MATH128_H */
diff -Nurp linux/include/linux/rtmutex.h sched-deadline-mainline-dl/include/linux/rtmutex.h
--- linux/include/linux/rtmutex.h	2014-02-17 14:45:55.308677415 -0500
+++ sched-deadline-mainline-dl/include/linux/rtmutex.h	2012-11-21 15:11:30.000000000 -0500
@@ -13,7 +13,7 @@
 #define __LINUX_RT_MUTEX_H
 
 #include <linux/linkage.h>
-#include <linux/plist.h>
+#include <linux/rbtree.h>
 #include <linux/spinlock_types.h>
 
 extern int max_lock_depth; /* for sysctl */
@@ -22,12 +22,14 @@ extern int max_lock_depth; /* for sysctl
  * The rt_mutex structure
  *
  * @wait_lock:	spinlock to protect the structure
- * @wait_list:	pilist head to enqueue waiters in priority order
+ * @waiters:	rbtree root to enqueue waiters in priority order
+ * @waiters_leftmost: top waiter
  * @owner:	the mutex owner
  */
 struct rt_mutex {
 	raw_spinlock_t		wait_lock;
-	struct plist_head	wait_list;
+	struct rb_root          waiters;
+	struct rb_node          *waiters_leftmost;
 	struct task_struct	*owner;
 #ifdef CONFIG_DEBUG_RT_MUTEXES
 	int			save_state;
@@ -66,7 +68,7 @@ struct hrtimer_sleeper;
 
 #define __RT_MUTEX_INITIALIZER(mutexname) \
 	{ .wait_lock = __RAW_SPIN_LOCK_UNLOCKED(mutexname.wait_lock) \
-	, .wait_list = PLIST_HEAD_INIT(mutexname.wait_list) \
+	, .waiters = RB_ROOT \
 	, .owner = NULL \
 	__DEBUG_RT_MUTEX_INITIALIZER(mutexname)}
 
@@ -98,12 +100,4 @@ extern int rt_mutex_trylock(struct rt_mu
 
 extern void rt_mutex_unlock(struct rt_mutex *lock);
 
-#ifdef CONFIG_RT_MUTEXES
-# define INIT_RT_MUTEXES(tsk)						\
-	.pi_waiters	= PLIST_HEAD_INIT(tsk.pi_waiters),	\
-	INIT_RT_MUTEX_DEBUG(tsk)
-#else
-# define INIT_RT_MUTEXES(tsk)
-#endif
-
 #endif
diff -Nurp linux/include/linux/sched.h sched-deadline-mainline-dl/include/linux/sched.h
--- linux/include/linux/sched.h	2014-02-17 14:45:55.321677734 -0500
+++ sched-deadline-mainline-dl/include/linux/sched.h	2012-11-21 15:11:30.000000000 -0500
@@ -16,6 +16,7 @@ struct sched_param {
 #include <linux/types.h>
 #include <linux/timex.h>
 #include <linux/jiffies.h>
+#include <linux/plist.h>
 #include <linux/rbtree.h>
 #include <linux/thread_info.h>
 #include <linux/cpumask.h>
@@ -54,6 +55,58 @@ struct sched_param {
 
 #include <asm/processor.h>
 
+/*
+ * Extended scheduling parameters data structure.
+ *
+ * This is needed because the original struct sched_param can not be
+ * altered without introducing ABI issues with legacy applications
+ * (e.g., in sched_getparam()).
+ *
+ * However, the possibility of specifying more than just a priority for
+ * the tasks may be useful for a wide variety of application fields, e.g.,
+ * multimedia, streaming, automation and control, and many others.
+ *
+ * This variant (sched_param2) is meant at describing a so-called
+ * sporadic time-constrained task. In such model a task is specified by:
+ *  - the activation period or minimum instance inter-arrival time;
+ *  - the maximum (or average, depending on the actual scheduling
+ *    discipline) computation time of all instances, a.k.a. runtime;
+ *  - the deadline (relative to the actual activation time) of each
+ *    instance.
+ * Very briefly, a periodic (sporadic) task asks for the execution of
+ * some specific computation --which is typically called an instance--
+ * (at most) every period. Moreover, each instance typically lasts no more
+ * than the runtime and must be completed by time instant t equal to
+ * the instance activation time + the deadline.
+ *
+ * This is reflected by the actual fields of the sched_param2 structure:
+ *
+ *  @sched_priority     task's priority (might still be useful)
+ *  @sched_deadline     representative of the task's deadline
+ *  @sched_runtime      representative of the task's runtime
+ *  @sched_period       representative of the task's period
+ *  @sched_flags        for customizing the scheduler behaviour
+ *
+ * Given this task model, there are a multiplicity of scheduling algorithms
+ * and policies, that can be used to ensure all the tasks will make their
+ * timing constraints.
+ *
+ * @__unused		padding to allow future expansion without ABI issues
+ *
+ * As of now, the SCHED_DEADLINE policy (sched_dl scheduling class) is the
+ * only user of this new interface. More information about the algorithm
+ * available in the scheduling class file or in Documentation/.
+ */
+struct sched_param2 {
+	int sched_priority;
+	unsigned int sched_flags;
+	u64 sched_runtime;
+	u64 sched_deadline;
+	u64 sched_period;
+
+	u64 __unused[12];
+};
+
 struct exec_domain;
 struct futex_pi_state;
 struct robust_list_head;
@@ -1043,6 +1096,7 @@ struct sched_domain;
 #else
 #define ENQUEUE_WAKING		0
 #endif
+#define ENQUEUE_REPLENISH	8
 
 #define DEQUEUE_SLEEP		1
 
@@ -1077,6 +1131,7 @@ struct sched_class {
 	void (*set_curr_task) (struct rq *rq);
 	void (*task_tick) (struct rq *rq, struct task_struct *p, int queued);
 	void (*task_fork) (struct task_struct *p);
+	void (*task_dead) (struct task_struct *p);
 
 	void (*switched_from) (struct rq *this_rq, struct task_struct *task);
 	void (*switched_to) (struct rq *this_rq, struct task_struct *task);
@@ -1172,6 +1227,65 @@ struct sched_rt_entity {
 #endif
 };
 
+#ifdef CONFIG_SCHEDSTATS
+struct sched_stats_dl {
+	u64			last_dmiss;
+	u64			last_rorun;
+	u64			dmiss_max;
+	u64			rorun_max;
+};
+#endif
+
+struct sched_dl_entity {
+	struct rb_node	rb_node;
+
+	/*
+	 * Original scheduling parameters. Copied here from sched_param2
+	 * during sched_setscheduler2(), they will remain the same until
+	 * the next sched_setscheduler2().
+	 */
+	u64 dl_runtime;		/* maximum runtime for each instance	*/
+	u64 dl_deadline;	/* relative deadline of each instance	*/
+	u64 dl_period;		/* separation of two instances (period) */
+	u64 dl_bw;		/* dl_runtime / dl_deadline		*/
+
+	/*
+	 * Actual scheduling parameters. Initialized with the values above,
+	 * they are continously updated during task execution. Note that
+	 * the remaining runtime could be < 0 in case we are in overrun.
+	 */
+	s64 runtime;		/* remaining runtime for this instance	*/
+	u64 deadline;		/* absolute deadline for this instance	*/
+	unsigned int flags;	/* specifying the scheduler behaviour	*/
+
+	/*
+	 * Some bool flags:
+	 *
+	 * @dl_throttled tells if we exhausted the runtime. If so, the
+	 * task has to wait for a replenishment to be performed at the
+	 * next firing of dl_timer.
+	 *
+	 * @dl_new tells if a new instance arrived. If so we must
+	 * start executing it with full runtime and reset its absolute
+	 * deadline;
+	 *
+	 * @dl_boosted tells if we are boosted due to DI. If so we are
+	 * outside bandwidth enforcement mechanism (but only until we
+	 * exit the critical section).
+	 */
+	int dl_throttled, dl_new, dl_boosted;
+
+	/*
+	 * Bandwidth enforcement timer. Each -deadline task has its
+	 * own bandwidth to be enforced, thus we need one timer per task.
+	 */
+	struct hrtimer dl_timer;
+
+#ifdef CONFIG_SCHEDSTATS
+	struct sched_stats_dl stats;
+#endif
+};
+
 /*
  * default timeslice is 100 msecs (used only for SCHED_RR tasks).
  * Timeslices get refilled after they expire.
@@ -1208,6 +1322,7 @@ struct task_struct {
 #ifdef CONFIG_CGROUP_SCHED
 	struct task_group *sched_task_group;
 #endif
+	struct sched_dl_entity dl;
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 	/* list of struct preempt_notifier: */
@@ -1250,6 +1365,7 @@ struct task_struct {
 	struct list_head tasks;
 #ifdef CONFIG_SMP
 	struct plist_node pushable_tasks;
+	struct rb_node pushable_dl_tasks;
 #endif
 
 	struct mm_struct *mm, *active_mm;
@@ -1390,9 +1506,12 @@ struct task_struct {
 
 #ifdef CONFIG_RT_MUTEXES
 	/* PI waiters blocked on a rt_mutex held by this task */
-	struct plist_head pi_waiters;
+	struct rb_root pi_waiters;
+	struct rb_node *pi_waiters_leftmost;
 	/* Deadlock detection and priority inheritance handling */
 	struct rt_mutex_waiter *pi_blocked_on;
+	/* Top pi_waiters task */
+	struct task_struct *pi_top_task;
 #endif
 
 #ifdef CONFIG_DEBUG_MUTEXES
@@ -1564,6 +1683,10 @@ struct task_struct {
  * user-space.  This allows kernel threads to set their
  * priority to a value higher than any user task. Note:
  * MAX_RT_PRIO must not be smaller than MAX_USER_RT_PRIO.
+ *
+ * SCHED_DEADLINE tasks has negative priorities, reflecting
+ * the fact that any of them has higher prio than RT and
+ * NORMAL/BATCH tasks.
  */
 
 #define MAX_USER_RT_PRIO	100
@@ -1572,9 +1695,23 @@ struct task_struct {
 #define MAX_PRIO		(MAX_RT_PRIO + 40)
 #define DEFAULT_PRIO		(MAX_RT_PRIO + 20)
 
+#define MAX_DL_PRIO		0
+
+static inline int dl_prio(int prio)
+{
+	if (unlikely(prio < MAX_DL_PRIO))
+		return 1;
+	return 0;
+}
+
+static inline int dl_task(struct task_struct *p)
+{
+	return dl_prio(p->prio);
+}
+
 static inline int rt_prio(int prio)
 {
-	if (unlikely(prio < MAX_RT_PRIO))
+	if ((unsigned)prio < MAX_RT_PRIO)
 		return 1;
 	return 0;
 }
@@ -2019,6 +2156,12 @@ int sched_rt_handler(struct ctl_table *t
 		void __user *buffer, size_t *lenp,
 		loff_t *ppos);
 
+extern int sysctl_sched_dl_runtime;
+
+int sched_dl_handler(struct ctl_table *table, int write,
+		void __user *buffer, size_t *lenp,
+		loff_t *ppos);
+
 #ifdef CONFIG_SCHED_AUTOGROUP
 extern unsigned int sysctl_sched_autogroup_enabled;
 
@@ -2044,6 +2187,7 @@ extern unsigned int sysctl_sched_cfs_ban
 #ifdef CONFIG_RT_MUTEXES
 extern int rt_mutex_getprio(struct task_struct *p);
 extern void rt_mutex_setprio(struct task_struct *p, int prio);
+extern struct task_struct *rt_mutex_get_top_task(struct task_struct *task);
 extern void rt_mutex_adjust_pi(struct task_struct *p);
 static inline bool tsk_is_pi_blocked(struct task_struct *tsk)
 {
@@ -2072,6 +2216,8 @@ extern int sched_setscheduler(struct tas
 			      const struct sched_param *);
 extern int sched_setscheduler_nocheck(struct task_struct *, int,
 				      const struct sched_param *);
+extern int sched_setscheduler2(struct task_struct *, int,
+				 const struct sched_param2 *);
 extern struct task_struct *idle_task(int cpu);
 /**
  * is_idle_task - is the specified task an idle task?
@@ -2151,7 +2297,7 @@ extern void wake_up_new_task(struct task
 #else
  static inline void kick_process(struct task_struct *tsk) { }
 #endif
-extern void sched_fork(struct task_struct *p);
+extern int sched_fork(struct task_struct *p);
 extern void sched_dead(struct task_struct *p);
 
 extern void proc_caches_init(void);
diff -Nurp linux/include/linux/syscalls.h sched-deadline-mainline-dl/include/linux/syscalls.h
--- linux/include/linux/syscalls.h	2014-02-17 14:45:55.960693412 -0500
+++ sched-deadline-mainline-dl/include/linux/syscalls.h	2012-11-21 15:11:30.000000000 -0500
@@ -38,6 +38,7 @@ struct rlimit;
 struct rlimit64;
 struct rusage;
 struct sched_param;
+struct sched_param2;
 struct sel_arg_struct;
 struct semaphore;
 struct sembuf;
@@ -328,11 +329,17 @@ asmlinkage long sys_clock_nanosleep(cloc
 asmlinkage long sys_nice(int increment);
 asmlinkage long sys_sched_setscheduler(pid_t pid, int policy,
 					struct sched_param __user *param);
+asmlinkage long sys_sched_setscheduler2(pid_t pid, int policy,
+					struct sched_param2 __user *param);
 asmlinkage long sys_sched_setparam(pid_t pid,
 					struct sched_param __user *param);
+asmlinkage long sys_sched_setparam2(pid_t pid,
+					struct sched_param2 __user *param);
 asmlinkage long sys_sched_getscheduler(pid_t pid);
 asmlinkage long sys_sched_getparam(pid_t pid,
 					struct sched_param __user *param);
+asmlinkage long sys_sched_getparam2(pid_t pid,
+					struct sched_param2 __user *param);
 asmlinkage long sys_sched_setaffinity(pid_t pid, unsigned int len,
 					unsigned long __user *user_mask_ptr);
 asmlinkage long sys_sched_getaffinity(pid_t pid, unsigned int len,
diff -Nurp linux/include/uapi/linux/sched.h sched-deadline-mainline-dl/include/uapi/linux/sched.h
--- linux/include/uapi/linux/sched.h	2014-02-17 14:46:00.180796950 -0500
+++ sched-deadline-mainline-dl/include/uapi/linux/sched.h	2012-11-21 15:11:30.000000000 -0500
@@ -39,6 +39,7 @@
 #define SCHED_BATCH		3
 /* SCHED_ISO: reserved but not implemented yet */
 #define SCHED_IDLE		5
+#define SCHED_DEADLINE		6
 /* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */
 #define SCHED_RESET_ON_FORK     0x40000000
 
diff -Nurp linux/kernel/fork.c sched-deadline-mainline-dl/kernel/fork.c
--- linux/kernel/fork.c	2014-02-17 14:46:02.856862605 -0500
+++ sched-deadline-mainline-dl/kernel/fork.c	2012-11-21 15:11:30.000000000 -0500
@@ -1092,8 +1092,10 @@ static void rt_mutex_init_task(struct ta
 {
 	raw_spin_lock_init(&p->pi_lock);
 #ifdef CONFIG_RT_MUTEXES
-	plist_head_init(&p->pi_waiters);
+	p->pi_waiters = RB_ROOT;
+	p->pi_waiters_leftmost = NULL;
 	p->pi_blocked_on = NULL;
+	p->pi_top_task = NULL;
 #endif
 }
 
@@ -1287,7 +1289,9 @@ static struct task_struct *copy_process(
 #endif
 
 	/* Perform scheduler related setup. Assign this task to a CPU. */
-	sched_fork(p);
+	retval = sched_fork(p);
+	if (retval)
+		goto bad_fork_cleanup_policy;
 
 	retval = perf_event_init_task(p);
 	if (retval)
diff -Nurp linux/kernel/futex.c sched-deadline-mainline-dl/kernel/futex.c
--- linux/kernel/futex.c	2014-02-17 14:46:02.869862923 -0500
+++ sched-deadline-mainline-dl/kernel/futex.c	2012-11-21 15:11:30.000000000 -0500
@@ -2296,6 +2296,8 @@ static int futex_wait_requeue_pi(u32 __u
 	 * code while we sleep on uaddr.
 	 */
 	debug_rt_mutex_init_waiter(&rt_waiter);
+	RB_CLEAR_NODE(&rt_waiter.pi_tree_entry);
+	RB_CLEAR_NODE(&rt_waiter.tree_entry);
 	rt_waiter.task = NULL;
 
 	ret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, VERIFY_WRITE);
diff -Nurp linux/kernel/hrtimer.c sched-deadline-mainline-dl/kernel/hrtimer.c
--- linux/kernel/hrtimer.c	2014-02-17 14:46:03.221871560 -0500
+++ sched-deadline-mainline-dl/kernel/hrtimer.c	2012-11-21 15:11:30.000000000 -0500
@@ -1586,7 +1586,7 @@ long hrtimer_nanosleep(struct timespec *
 	unsigned long slack;
 
 	slack = current->timer_slack_ns;
-	if (rt_task(current))
+	if (dl_task(current) || rt_task(current))
 		slack = 0;
 
 	hrtimer_init_on_stack(&t.timer, clockid, mode);
diff -Nurp linux/kernel/rtmutex.c sched-deadline-mainline-dl/kernel/rtmutex.c
--- linux/kernel/rtmutex.c	2014-02-17 14:46:04.496902841 -0500
+++ sched-deadline-mainline-dl/kernel/rtmutex.c	2012-11-21 15:11:30.000000000 -0500
@@ -90,10 +90,104 @@ static inline void mark_rt_mutex_waiters
 }
 #endif
 
+static inline int
+rt_mutex_waiter_less(struct rt_mutex_waiter *left,
+		     struct rt_mutex_waiter *right)
+{
+	if (left->task->prio < right->task->prio)
+		return 1;
+
+	/*
+	 * If both tasks are dl_task(), we check their deadlines.
+	 */
+	if (dl_prio(left->task->prio) && dl_prio(right->task->prio))
+		return (left->task->dl.deadline < right->task->dl.deadline);
+
+	return 0;
+}
+
+static void
+rt_mutex_enqueue(struct rt_mutex *lock, struct rt_mutex_waiter *waiter)
+{
+	struct rb_node **link = &lock->waiters.rb_node;
+	struct rb_node *parent = NULL;
+	struct rt_mutex_waiter *entry;
+	int leftmost = 1;
+
+	while (*link) {
+		parent = *link;
+		entry = rb_entry(parent, struct rt_mutex_waiter, tree_entry);
+		if (rt_mutex_waiter_less(waiter, entry)) {
+			link = &parent->rb_left;
+		} else {
+			link = &parent->rb_right;
+			leftmost = 0;
+		}
+	}
+
+	if (leftmost)
+		lock->waiters_leftmost = &waiter->tree_entry;
+
+	rb_link_node(&waiter->tree_entry, parent, link);
+	rb_insert_color(&waiter->tree_entry, &lock->waiters);
+}
+
+static void
+rt_mutex_dequeue(struct rt_mutex *lock, struct rt_mutex_waiter *waiter)
+{
+	if (RB_EMPTY_NODE(&waiter->tree_entry))
+		return;
+
+	if (lock->waiters_leftmost == &waiter->tree_entry)
+		lock->waiters_leftmost = rb_next(&waiter->tree_entry);
+
+	rb_erase(&waiter->tree_entry, &lock->waiters);
+	RB_CLEAR_NODE(&waiter->tree_entry);
+}
+
+static void
+rt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)
+{
+	struct rb_node **link = &task->pi_waiters.rb_node;
+	struct rb_node *parent = NULL;
+	struct rt_mutex_waiter *entry;
+	int leftmost = 1;
+
+	while (*link) {
+		parent = *link;
+		entry = rb_entry(parent, struct rt_mutex_waiter, pi_tree_entry);
+		if (rt_mutex_waiter_less(waiter, entry)) {
+			link = &parent->rb_left;
+		} else {
+			link = &parent->rb_right;
+			leftmost = 0;
+		}
+	}
+
+	if (leftmost)
+		task->pi_waiters_leftmost = &waiter->pi_tree_entry;
+
+	rb_link_node(&waiter->pi_tree_entry, parent, link);
+	rb_insert_color(&waiter->pi_tree_entry, &task->pi_waiters);
+}
+
+static void
+rt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)
+{
+	if (RB_EMPTY_NODE(&waiter->pi_tree_entry))
+		return;
+
+	if (task->pi_waiters_leftmost == &waiter->pi_tree_entry)
+		task->pi_waiters_leftmost = rb_next(&waiter->pi_tree_entry);
+
+	rb_erase(&waiter->pi_tree_entry, &task->pi_waiters);
+	RB_CLEAR_NODE(&waiter->pi_tree_entry);
+}
+
 /*
- * Calculate task priority from the waiter list priority
+ * Calculate task priority from the waiter tree priority
  *
- * Return task->normal_prio when the waiter list is empty or when
+ * Return task->normal_prio when the waiter tree is empty or when
  * the waiter is not allowed to do priority boosting
  */
 int rt_mutex_getprio(struct task_struct *task)
@@ -101,10 +195,18 @@ int rt_mutex_getprio(struct task_struct
 	if (likely(!task_has_pi_waiters(task)))
 		return task->normal_prio;
 
-	return min(task_top_pi_waiter(task)->pi_list_entry.prio,
+	return min(task_top_pi_waiter(task)->task->prio,
 		   task->normal_prio);
 }
 
+struct task_struct *rt_mutex_get_top_task(struct task_struct *task)
+{
+	if (likely(!task_has_pi_waiters(task)))
+		return NULL;
+
+	return task_top_pi_waiter(task)->task;
+}
+
 /*
  * Adjust the priority of a task, after its pi_waiters got modified.
  *
@@ -114,7 +216,7 @@ static void __rt_mutex_adjust_prio(struc
 {
 	int prio = rt_mutex_getprio(task);
 
-	if (task->prio != prio)
+	if (task->prio != prio || dl_prio(prio))
 		rt_mutex_setprio(task, prio);
 }
 
@@ -219,7 +321,7 @@ static int rt_mutex_adjust_prio_chain(st
 	 * When deadlock detection is off then we check, if further
 	 * priority adjustment is necessary.
 	 */
-	if (!detect_deadlock && waiter->list_entry.prio == task->prio)
+	if (!detect_deadlock && waiter->task->prio == task->prio)
 		goto out_unlock_pi;
 
 	lock = waiter->lock;
@@ -240,9 +342,9 @@ static int rt_mutex_adjust_prio_chain(st
 	top_waiter = rt_mutex_top_waiter(lock);
 
 	/* Requeue the waiter */
-	plist_del(&waiter->list_entry, &lock->wait_list);
-	waiter->list_entry.prio = task->prio;
-	plist_add(&waiter->list_entry, &lock->wait_list);
+	rt_mutex_dequeue(lock, waiter);
+	waiter->task->prio = task->prio;
+	rt_mutex_enqueue(lock, waiter);
 
 	/* Release the task */
 	raw_spin_unlock_irqrestore(&task->pi_lock, flags);
@@ -266,17 +368,15 @@ static int rt_mutex_adjust_prio_chain(st
 
 	if (waiter == rt_mutex_top_waiter(lock)) {
 		/* Boost the owner */
-		plist_del(&top_waiter->pi_list_entry, &task->pi_waiters);
-		waiter->pi_list_entry.prio = waiter->list_entry.prio;
-		plist_add(&waiter->pi_list_entry, &task->pi_waiters);
+		rt_mutex_dequeue_pi(task, top_waiter);
+		rt_mutex_enqueue_pi(task, waiter);
 		__rt_mutex_adjust_prio(task);
 
 	} else if (top_waiter == waiter) {
 		/* Deboost the owner */
-		plist_del(&waiter->pi_list_entry, &task->pi_waiters);
+		rt_mutex_dequeue_pi(task, waiter);
 		waiter = rt_mutex_top_waiter(lock);
-		waiter->pi_list_entry.prio = waiter->list_entry.prio;
-		plist_add(&waiter->pi_list_entry, &task->pi_waiters);
+		rt_mutex_enqueue_pi(task, waiter);
 		__rt_mutex_adjust_prio(task);
 	}
 
@@ -341,7 +441,7 @@ static int try_to_take_rt_mutex(struct r
 	 * 3) it is top waiter
 	 */
 	if (rt_mutex_has_waiters(lock)) {
-		if (task->prio >= rt_mutex_top_waiter(lock)->list_entry.prio) {
+		if (task->prio >= rt_mutex_top_waiter(lock)->task->prio) {
 			if (!waiter || waiter != rt_mutex_top_waiter(lock))
 				return 0;
 		}
@@ -355,7 +455,7 @@ static int try_to_take_rt_mutex(struct r
 
 		/* remove the queued waiter. */
 		if (waiter) {
-			plist_del(&waiter->list_entry, &lock->wait_list);
+			rt_mutex_dequeue(lock, waiter);
 			task->pi_blocked_on = NULL;
 		}
 
@@ -365,8 +465,7 @@ static int try_to_take_rt_mutex(struct r
 		 */
 		if (rt_mutex_has_waiters(lock)) {
 			top = rt_mutex_top_waiter(lock);
-			top->pi_list_entry.prio = top->list_entry.prio;
-			plist_add(&top->pi_list_entry, &task->pi_waiters);
+			rt_mutex_enqueue_pi(task, top);
 		}
 		raw_spin_unlock_irqrestore(&task->pi_lock, flags);
 	}
@@ -402,13 +501,11 @@ static int task_blocks_on_rt_mutex(struc
 	__rt_mutex_adjust_prio(task);
 	waiter->task = task;
 	waiter->lock = lock;
-	plist_node_init(&waiter->list_entry, task->prio);
-	plist_node_init(&waiter->pi_list_entry, task->prio);
-
+	
 	/* Get the top priority waiter on the lock */
 	if (rt_mutex_has_waiters(lock))
 		top_waiter = rt_mutex_top_waiter(lock);
-	plist_add(&waiter->list_entry, &lock->wait_list);
+	rt_mutex_enqueue(lock, waiter);
 
 	task->pi_blocked_on = waiter;
 
@@ -419,8 +516,8 @@ static int task_blocks_on_rt_mutex(struc
 
 	if (waiter == rt_mutex_top_waiter(lock)) {
 		raw_spin_lock_irqsave(&owner->pi_lock, flags);
-		plist_del(&top_waiter->pi_list_entry, &owner->pi_waiters);
-		plist_add(&waiter->pi_list_entry, &owner->pi_waiters);
+		rt_mutex_dequeue_pi(owner, top_waiter);
+		rt_mutex_enqueue_pi(owner, waiter);
 
 		__rt_mutex_adjust_prio(owner);
 		if (owner->pi_blocked_on)
@@ -472,7 +569,7 @@ static void wakeup_next_waiter(struct rt
 	 * boosted mode and go back to normal after releasing
 	 * lock->wait_lock.
 	 */
-	plist_del(&waiter->pi_list_entry, &current->pi_waiters);
+	rt_mutex_dequeue_pi(current, waiter);
 
 	rt_mutex_set_owner(lock, NULL);
 
@@ -496,7 +593,7 @@ static void remove_waiter(struct rt_mute
 	int chain_walk = 0;
 
 	raw_spin_lock_irqsave(&current->pi_lock, flags);
-	plist_del(&waiter->list_entry, &lock->wait_list);
+	rt_mutex_dequeue(lock, waiter);
 	current->pi_blocked_on = NULL;
 	raw_spin_unlock_irqrestore(&current->pi_lock, flags);
 
@@ -507,13 +604,13 @@ static void remove_waiter(struct rt_mute
 
 		raw_spin_lock_irqsave(&owner->pi_lock, flags);
 
-		plist_del(&waiter->pi_list_entry, &owner->pi_waiters);
+		rt_mutex_dequeue_pi(owner, waiter);
 
 		if (rt_mutex_has_waiters(lock)) {
 			struct rt_mutex_waiter *next;
 
 			next = rt_mutex_top_waiter(lock);
-			plist_add(&next->pi_list_entry, &owner->pi_waiters);
+			rt_mutex_enqueue_pi(owner, next);
 		}
 		__rt_mutex_adjust_prio(owner);
 
@@ -523,8 +620,6 @@ static void remove_waiter(struct rt_mute
 		raw_spin_unlock_irqrestore(&owner->pi_lock, flags);
 	}
 
-	WARN_ON(!plist_node_empty(&waiter->pi_list_entry));
-
 	if (!chain_walk)
 		return;
 
@@ -551,7 +646,8 @@ void rt_mutex_adjust_pi(struct task_stru
 	raw_spin_lock_irqsave(&task->pi_lock, flags);
 
 	waiter = task->pi_blocked_on;
-	if (!waiter || waiter->list_entry.prio == task->prio) {
+	if (!waiter || (waiter->task->prio == task->prio &&
+			!dl_prio(task->prio))) {
 		raw_spin_unlock_irqrestore(&task->pi_lock, flags);
 		return;
 	}
@@ -624,6 +720,8 @@ rt_mutex_slowlock(struct rt_mutex *lock,
 	int ret = 0;
 
 	debug_rt_mutex_init_waiter(&waiter);
+	RB_CLEAR_NODE(&waiter.pi_tree_entry);
+	RB_CLEAR_NODE(&waiter.tree_entry);
 
 	raw_spin_lock(&lock->wait_lock);
 
@@ -890,7 +988,8 @@ void __rt_mutex_init(struct rt_mutex *lo
 {
 	lock->owner = NULL;
 	raw_spin_lock_init(&lock->wait_lock);
-	plist_head_init(&lock->wait_list);
+	lock->waiters = RB_ROOT;
+	lock->waiters_leftmost = NULL;
 
 	debug_rt_mutex_init(lock, name);
 }
diff -Nurp linux/kernel/rtmutex_common.h sched-deadline-mainline-dl/kernel/rtmutex_common.h
--- linux/kernel/rtmutex_common.h	2014-02-17 14:46:04.496902841 -0500
+++ sched-deadline-mainline-dl/kernel/rtmutex_common.h	2012-11-21 15:11:30.000000000 -0500
@@ -40,13 +40,13 @@ extern void schedule_rt_mutex_test(struc
  * This is the control structure for tasks blocked on a rt_mutex,
  * which is allocated on the kernel stack on of the blocked task.
  *
- * @list_entry:		pi node to enqueue into the mutex waiters list
- * @pi_list_entry:	pi node to enqueue into the mutex owner waiters list
+ * @tree_entry:		pi node to enqueue into the mutex waiters tree
+ * @pi_tree_entry:	pi node to enqueue into the mutex owner waiters tree
  * @task:		task reference to the blocked task
  */
 struct rt_mutex_waiter {
-	struct plist_node	list_entry;
-	struct plist_node	pi_list_entry;
+	struct rb_node          tree_entry;
+	struct rb_node          pi_tree_entry;
 	struct task_struct	*task;
 	struct rt_mutex		*lock;
 #ifdef CONFIG_DEBUG_RT_MUTEXES
@@ -57,11 +57,11 @@ struct rt_mutex_waiter {
 };
 
 /*
- * Various helpers to access the waiters-plist:
+ * Various helpers to access the waiters-tree:
  */
 static inline int rt_mutex_has_waiters(struct rt_mutex *lock)
 {
-	return !plist_head_empty(&lock->wait_list);
+	return !RB_EMPTY_ROOT(&lock->waiters);
 }
 
 static inline struct rt_mutex_waiter *
@@ -69,8 +69,8 @@ rt_mutex_top_waiter(struct rt_mutex *loc
 {
 	struct rt_mutex_waiter *w;
 
-	w = plist_first_entry(&lock->wait_list, struct rt_mutex_waiter,
-			       list_entry);
+	w = rb_entry(lock->waiters_leftmost, struct rt_mutex_waiter,
+		     tree_entry);
 	BUG_ON(w->lock != lock);
 
 	return w;
@@ -78,14 +78,14 @@ rt_mutex_top_waiter(struct rt_mutex *loc
 
 static inline int task_has_pi_waiters(struct task_struct *p)
 {
-	return !plist_head_empty(&p->pi_waiters);
+	return !RB_EMPTY_ROOT(&p->pi_waiters);
 }
 
 static inline struct rt_mutex_waiter *
 task_top_pi_waiter(struct task_struct *p)
 {
-	return plist_first_entry(&p->pi_waiters, struct rt_mutex_waiter,
-				  pi_list_entry);
+	return rb_entry(p->pi_waiters_leftmost, struct rt_mutex_waiter,
+			pi_tree_entry);
 }
 
 /*
diff -Nurp linux/kernel/rtmutex-debug.c sched-deadline-mainline-dl/kernel/rtmutex-debug.c
--- linux/kernel/rtmutex-debug.c	2014-02-17 14:46:04.495902816 -0500
+++ sched-deadline-mainline-dl/kernel/rtmutex-debug.c	2012-11-21 15:11:30.000000000 -0500
@@ -23,7 +23,7 @@
 #include <linux/kallsyms.h>
 #include <linux/syscalls.h>
 #include <linux/interrupt.h>
-#include <linux/plist.h>
+#include <linux/rbtree.h>
 #include <linux/fs.h>
 #include <linux/debug_locks.h>
 
@@ -56,7 +56,7 @@ static void printk_lock(struct rt_mutex
 
 void rt_mutex_debug_task_free(struct task_struct *task)
 {
-	DEBUG_LOCKS_WARN_ON(!plist_head_empty(&task->pi_waiters));
+	DEBUG_LOCKS_WARN_ON(!RB_EMPTY_ROOT(&task->pi_waiters));
 	DEBUG_LOCKS_WARN_ON(task->pi_blocked_on);
 }
 
@@ -153,16 +153,12 @@ void debug_rt_mutex_proxy_unlock(struct
 void debug_rt_mutex_init_waiter(struct rt_mutex_waiter *waiter)
 {
 	memset(waiter, 0x11, sizeof(*waiter));
-	plist_node_init(&waiter->list_entry, MAX_PRIO);
-	plist_node_init(&waiter->pi_list_entry, MAX_PRIO);
 	waiter->deadlock_task_pid = NULL;
 }
 
 void debug_rt_mutex_free_waiter(struct rt_mutex_waiter *waiter)
 {
 	put_pid(waiter->deadlock_task_pid);
-	DEBUG_LOCKS_WARN_ON(!plist_node_empty(&waiter->list_entry));
-	DEBUG_LOCKS_WARN_ON(!plist_node_empty(&waiter->pi_list_entry));
 	memset(waiter, 0x22, sizeof(*waiter));
 }
 
diff -Nurp linux/kernel/sched/core.c sched-deadline-mainline-dl/kernel/sched/core.c
--- linux/kernel/sched/core.c	2014-02-17 14:46:04.520903430 -0500
+++ sched-deadline-mainline-dl/kernel/sched/core.c	2012-11-21 15:11:30.000000000 -0500
@@ -287,6 +287,14 @@ __read_mostly int scheduler_running;
  */
 int sysctl_sched_rt_runtime = 950000;
 
+/*
+ * Sub-quota or rt bandwidth available for all -deadline tasks
+ * on each CPU.
+ *
+ * default: 40%
+ */
+int sysctl_sched_dl_runtime = 400000;
+
 
 
 /*
@@ -849,7 +857,9 @@ static inline int normal_prio(struct tas
 {
 	int prio;
 
-	if (task_has_rt_policy(p))
+	if (task_has_dl_policy(p))
+		prio = MAX_DL_PRIO-1;
+	else if (task_has_rt_policy(p))
 		prio = MAX_RT_PRIO-1 - p->rt_priority;
 	else
 		prio = __normal_prio(p);
@@ -1528,6 +1538,13 @@ static void __sched_fork(struct task_str
 	memset(&p->se.statistics, 0, sizeof(p->se.statistics));
 #endif
 
+	RB_CLEAR_NODE(&p->dl.rb_node);
+	hrtimer_init(&p->dl.dl_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	p->dl.dl_runtime = p->dl.runtime = 0;
+	p->dl.dl_deadline = p->dl.deadline = 0;
+	p->dl.dl_period = 0;
+	p->dl.flags = 0;
+
 	INIT_LIST_HEAD(&p->rt.run_list);
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
@@ -1538,7 +1555,7 @@ static void __sched_fork(struct task_str
 /*
  * fork()/clone()-time setup:
  */
-void sched_fork(struct task_struct *p)
+int sched_fork(struct task_struct *p)
 {
 	unsigned long flags;
 	int cpu = get_cpu();
@@ -1560,7 +1577,7 @@ void sched_fork(struct task_struct *p)
 	 * Revert to default priority/policy on fork if requested.
 	 */
 	if (unlikely(p->sched_reset_on_fork)) {
-		if (task_has_rt_policy(p)) {
+		if (task_has_dl_policy(p) || task_has_rt_policy(p)) {
 			p->policy = SCHED_NORMAL;
 			p->static_prio = NICE_TO_PRIO(0);
 			p->rt_priority = 0;
@@ -1577,8 +1594,14 @@ void sched_fork(struct task_struct *p)
 		p->sched_reset_on_fork = 0;
 	}
 
-	if (!rt_prio(p->prio))
+	if (dl_prio(p->prio)) {
+		put_cpu();
+		return -EAGAIN;
+	} else if (rt_prio(p->prio)) {
+		p->sched_class = &rt_sched_class;
+	} else {
 		p->sched_class = &fair_sched_class;
+	}
 
 	if (p->sched_class->task_fork)
 		p->sched_class->task_fork(p);
@@ -1607,12 +1630,104 @@ void sched_fork(struct task_struct *p)
 #endif
 #ifdef CONFIG_SMP
 	plist_node_init(&p->pushable_tasks, MAX_PRIO);
+	RB_CLEAR_NODE(&p->pushable_dl_tasks);
 #endif
 
 	put_cpu();
+	return 0;
+}
+
+unsigned long to_ratio(u64 period, u64 runtime)
+{
+	if (runtime == RUNTIME_INF)
+		return 1ULL << 20;
+
+	/*
+	 * Doing this here saves a lot of checks in all
+	 * the calling paths, and returning zero seems
+	 * safe for them anyway.
+	 */
+	if (period == 0)
+		return 0;
+
+	return div64_u64(runtime << 20, period);
+}
+
+static inline
+void __dl_clear(struct dl_bw *dl_b, u64 tsk_bw)
+{
+	dl_b->total_bw -= tsk_bw;
+}
+
+static inline
+void __dl_add(struct dl_bw *dl_b, u64 tsk_bw)
+{
+	dl_b->total_bw += tsk_bw;
+}
+
+static inline
+bool __dl_overflow(struct dl_bw *dl_b, int cpus, u64 old_bw, u64 new_bw)
+{
+	return dl_b->bw != -1 &&
+	       dl_b->bw * cpus < dl_b->total_bw - old_bw + new_bw;
 }
 
 /*
+ * We must be sure that accepting a new task (or allowing changing the
+ * parameters of an existing one) is consistent with the bandwidth
+ * constraints. If yes, this function also accordingly updates the currently
+ * allocated bandwidth to reflect the new situation.
+ *
+ * This function is called while holding p's rq->lock.
+ */
+static int dl_overflow(struct task_struct *p, int policy,
+		       const struct sched_param2 *param2)
+{
+#ifdef CONFIG_SMP
+	struct dl_bw *dl_b = &task_rq(p)->rd->dl_bw;
+#else
+	struct dl_bw *dl_b = &task_rq(p)->dl.dl_bw;
+#endif
+	u64 period = param2->sched_period;
+	u64 runtime = param2->sched_runtime;
+	u64 new_bw = dl_policy(policy) ? to_ratio(period, runtime) : 0;
+#ifdef CONFIG_SMP
+	int cpus = cpumask_weight(task_rq(p)->rd->span);
+#else
+	int cpus = 1;
+#endif
+	int err = -1;
+
+	if (new_bw == p->dl.dl_bw)
+		return 0;
+
+	/*
+	 * Either if a task, enters, leave, or stays -deadline but changes
+	 * its parameters, we may need to update accordingly the total
+	 * allocated bandwidth of the container.
+	 */
+	raw_spin_lock(&dl_b->lock);
+	if (dl_policy(policy) && !task_has_dl_policy(p) &&
+	    !__dl_overflow(dl_b, cpus, 0, new_bw)) {
+		__dl_add(dl_b, new_bw);
+		err = 0;
+	} else if (dl_policy(policy) && task_has_dl_policy(p) &&
+		   !__dl_overflow(dl_b, cpus, p->dl.dl_bw, new_bw)) {
+		__dl_clear(dl_b, p->dl.dl_bw);
+		__dl_add(dl_b, new_bw);
+		err = 0;
+	} else if (!dl_policy(policy) && task_has_dl_policy(p)) {
+		__dl_clear(dl_b, p->dl.dl_bw);
+		err = 0;
+	}
+	raw_spin_unlock(&dl_b->lock);
+
+	return err;
+}
+
+extern void init_dl_bw(struct dl_bw *dl_b);
+
+/*
  * wake_up_new_task - wake up a newly created task for the first time.
  *
  * This function will do some initial scheduler statistics housekeeping
@@ -1774,6 +1889,9 @@ static void finish_task_switch(struct rq
 	if (mm)
 		mmdrop(mm);
 	if (unlikely(prev_state == TASK_DEAD)) {
+		if (prev->sched_class->task_dead)
+			prev->sched_class->task_dead(prev);
+
 		/*
 		 * Remove function-return probe instances associated with this
 		 * task and put them back on the free list.
@@ -3452,11 +3570,11 @@ EXPORT_SYMBOL(sleep_on_timeout);
  */
 void rt_mutex_setprio(struct task_struct *p, int prio)
 {
-	int oldprio, on_rq, running;
+	int oldprio, on_rq, running, enqueue_flag = 0;
 	struct rq *rq;
 	const struct sched_class *prev_class;
 
-	BUG_ON(prio < 0 || prio > MAX_PRIO);
+	BUG_ON(prio > MAX_PRIO);
 
 	rq = __task_rq_lock(p);
 
@@ -3479,6 +3597,7 @@ void rt_mutex_setprio(struct task_struct
 	}
 
 	trace_sched_pi_setprio(p, prio);
+	p->pi_top_task = rt_mutex_get_top_task(p);
 	oldprio = p->prio;
 	prev_class = p->sched_class;
 	on_rq = p->on_rq;
@@ -3488,17 +3607,42 @@ void rt_mutex_setprio(struct task_struct
 	if (running)
 		p->sched_class->put_prev_task(rq, p);
 
-	if (rt_prio(prio))
+	/*
+	 * Boosting condition are:
+	 * 1. -rt task is running and holds mutex A
+	 *      --> -dl task blocks on mutex A
+	 *
+	 * 2. -dl task is running and holds mutex A
+	 *      --> -dl task blocks on mutex A and could preempt the
+	 *          running task
+	 */
+	if (dl_prio(prio)) {
+		if (!dl_prio(p->normal_prio) || (p->pi_top_task &&
+			dl_entity_preempt(&p->pi_top_task->dl, &p->dl))) {
+			p->dl.dl_boosted = 1;
+			p->dl.dl_throttled = 0;
+			enqueue_flag = ENQUEUE_REPLENISH;
+		} else
+			p->dl.dl_boosted = 0;
+		p->sched_class = &dl_sched_class;
+	} else if (rt_prio(prio)) {
+		if (dl_prio(oldprio))
+			p->dl.dl_boosted = 0;
+		if (oldprio < prio)
+			enqueue_flag = ENQUEUE_HEAD;
 		p->sched_class = &rt_sched_class;
-	else
+	} else {
+		if (dl_prio(oldprio))
+			p->dl.dl_boosted = 0;
 		p->sched_class = &fair_sched_class;
+	}
 
 	p->prio = prio;
 
 	if (running)
 		p->sched_class->set_curr_task(rq);
 	if (on_rq)
-		enqueue_task(rq, p, oldprio < prio ? ENQUEUE_HEAD : 0);
+		enqueue_task(rq, p, enqueue_flag);
 
 	check_class_changed(rq, p, prev_class, oldprio);
 out_unlock:
@@ -3522,9 +3666,9 @@ void set_user_nice(struct task_struct *p
 	 * The RT priorities are set via sched_setscheduler(), but we still
 	 * allow the 'normal' nice value to be set - but as expected
 	 * it wont have any effect on scheduling until the task is
-	 * SCHED_FIFO/SCHED_RR:
+	 * SCHED_DEADLINE, SCHED_FIFO or SCHED_RR:
 	 */
-	if (task_has_rt_policy(p)) {
+	if (task_has_dl_policy(p) || task_has_rt_policy(p)) {
 		p->static_prio = NICE_TO_PRIO(nice);
 		goto out_unlock;
 	}
@@ -3680,7 +3824,9 @@ __setscheduler(struct rq *rq, struct tas
 	p->normal_prio = normal_prio(p);
 	/* we are holding p->pi_lock already */
 	p->prio = rt_mutex_getprio(p);
-	if (rt_prio(p->prio))
+	if (dl_prio(p->prio))
+		p->sched_class = &dl_sched_class;
+	else if (rt_prio(p->prio))
 		p->sched_class = &rt_sched_class;
 	else
 		p->sched_class = &fair_sched_class;
@@ -3688,6 +3834,62 @@ __setscheduler(struct rq *rq, struct tas
 }
 
 /*
+ * This function initializes the sched_dl_entity of a newly becoming
+ * SCHED_DEADLINE task.
+ *
+ * Only the static values are considered here, the actual runtime and the
+ * absolute deadline will be properly calculated when the task is enqueued
+ * for the first time with its new policy.
+ */
+static void
+__setparam_dl(struct task_struct *p, const struct sched_param2 *param2)
+{
+	struct sched_dl_entity *dl_se = &p->dl;
+
+	init_dl_task_timer(dl_se);
+	dl_se->dl_runtime = param2->sched_runtime;
+	dl_se->dl_deadline = param2->sched_deadline;
+	if (param2->sched_period != 0)
+		dl_se->dl_period = param2->sched_period;
+	else
+		dl_se->dl_period = dl_se->dl_deadline;
+	dl_se->dl_bw = to_ratio(dl_se->dl_period, dl_se->dl_runtime);
+	dl_se->flags = param2->sched_flags;
+	dl_se->dl_throttled = 0;
+	dl_se->dl_new = 1;
+}
+
+static void
+__getparam_dl(struct task_struct *p, struct sched_param2 *param2)
+{
+	struct sched_dl_entity *dl_se = &p->dl;
+
+	param2->sched_priority = p->rt_priority;
+	param2->sched_runtime = dl_se->dl_runtime;
+	param2->sched_deadline = dl_se->dl_deadline;
+	param2->sched_period = dl_se->dl_period;
+	param2->sched_flags = dl_se->flags;
+}
+
+/*
+ * This function validates the new parameters of a -deadline task.
+ * We ask for the deadline not being zero, and greater or equal
+ * than the runtime, as well as the period of being zero or
+ * greater than deadline. Furthermore, we have to be sure that
+ * user parameters are above the internal resolution (1us); we
+ * check sched_runtime only since it is always the smaller one.
+ */
+static bool
+__checkparam_dl(const struct sched_param2 *prm)
+{
+	return prm && prm->sched_deadline != 0 &&
+	       (prm->sched_period == 0 ||
+	       (s64)(prm->sched_period - prm->sched_deadline) >= 0) &&
+	       (s64)(prm->sched_deadline - prm->sched_runtime) >= 0 &&
+	       prm->sched_runtime >= (2 << (DL_SCALE - 1));
+}
+
+/*
  * check the target process has a UID that matches the current process's
  */
 static bool check_same_owner(struct task_struct *p)
@@ -3704,7 +3906,8 @@ static bool check_same_owner(struct task
 }
 
 static int __sched_setscheduler(struct task_struct *p, int policy,
-				const struct sched_param *param, bool user)
+				const struct sched_param2 *param,
+				bool user)
 {
 	int retval, oldprio, oldpolicy = -1, on_rq, running;
 	unsigned long flags;
@@ -3723,7 +3926,8 @@ recheck:
 		reset_on_fork = !!(policy & SCHED_RESET_ON_FORK);
 		policy &= ~SCHED_RESET_ON_FORK;
 
-		if (policy != SCHED_FIFO && policy != SCHED_RR &&
+		if (policy != SCHED_DEADLINE &&
+				policy != SCHED_FIFO && policy != SCHED_RR &&
 				policy != SCHED_NORMAL && policy != SCHED_BATCH &&
 				policy != SCHED_IDLE)
 			return -EINVAL;
@@ -3738,7 +3942,8 @@ recheck:
 	    (p->mm && param->sched_priority > MAX_USER_RT_PRIO-1) ||
 	    (!p->mm && param->sched_priority > MAX_RT_PRIO-1))
 		return -EINVAL;
-	if (rt_policy(policy) != (param->sched_priority != 0))
+	if ((dl_policy(policy) && !__checkparam_dl(param)) ||
+	    (rt_policy(policy) != (param->sched_priority != 0)))
 		return -EINVAL;
 
 	/*
@@ -3804,13 +4009,14 @@ recheck:
 	 * If not changing anything there's no need to proceed further:
 	 */
 	if (unlikely(policy == p->policy && (!rt_policy(policy) ||
-			param->sched_priority == p->rt_priority))) {
+			param->sched_priority == p->rt_priority) &&
+			!dl_policy(policy))) {
 		task_rq_unlock(rq, p, &flags);
 		return 0;
 	}
 
-#ifdef CONFIG_RT_GROUP_SCHED
 	if (user) {
+#ifdef CONFIG_RT_GROUP_SCHED
 		/*
 		 * Do not allow realtime tasks into groups that have no runtime
 		 * assigned.
@@ -3821,8 +4027,34 @@ recheck:
 			task_rq_unlock(rq, p, &flags);
 			return -EPERM;
 		}
-	}
 #endif
+#ifdef CONFIG_SMP
+		if (dl_bandwidth_enabled() && dl_policy(policy)) {
+			cpumask_t *span = rq->rd->span;
+			cpumask_t act_affinity;
+
+			/*
+			 * cpus_allowed mask is statically initialized with
+			 * CPU_MASK_ALL, span is instead dynamic. Here we
+			 * compute the "dynamic" affinity of a task.
+			 */
+			cpumask_and(&act_affinity, &p->cpus_allowed,
+				    cpu_active_mask);
+
+			/*
+			 * Don't allow tasks with an affinity mask smaller than
+			 * the entire root_domain to become SCHED_DEADLINE. We
+			 * will also fail if there's no bandwidth available.
+			 */
+			if (!cpumask_equal(&act_affinity, span) ||
+			    		   rq->rd->dl_bw.bw == 0) {
+				__task_rq_unlock(rq);
+				raw_spin_unlock_irqrestore(&p->pi_lock, flags);
+				return -EPERM;
+			}
+		}
+#endif
+	}
 
 	/* recheck policy now with rq lock held */
 	if (unlikely(oldpolicy != -1 && oldpolicy != p->policy)) {
@@ -3830,6 +4062,19 @@ recheck:
 		task_rq_unlock(rq, p, &flags);
 		goto recheck;
 	}
+
+	/*
+	 * If setscheduling to SCHED_DEADLINE (or changing the parameters
+	 * of a SCHED_DEADLINE task) we need to check if enough bandwidth
+	 * is available.
+	 */
+	if ((dl_policy(policy) || dl_task(p)) &&
+	    dl_overflow(p, policy, param)) {
+		__task_rq_unlock(rq);
+		raw_spin_unlock_irqrestore(&p->pi_lock, flags);
+		return -EBUSY;
+	}
+
 	on_rq = p->on_rq;
 	running = task_current(rq, p);
 	if (on_rq)
@@ -3841,7 +4086,11 @@ recheck:
 
 	oldprio = p->prio;
 	prev_class = p->sched_class;
-	__setscheduler(rq, p, policy, param->sched_priority);
+	if (dl_policy(policy)) {
+		__setparam_dl(p, param);
+		__setscheduler(rq, p, policy, param->sched_priority);
+	} else
+		__setscheduler(rq, p, policy, param->sched_priority);
 
 	if (running)
 		p->sched_class->set_curr_task(rq);
@@ -3867,10 +4116,20 @@ recheck:
 int sched_setscheduler(struct task_struct *p, int policy,
 		       const struct sched_param *param)
 {
-	return __sched_setscheduler(p, policy, param, true);
+	struct sched_param2 param2 = {
+		.sched_priority = param->sched_priority
+	};
+	return __sched_setscheduler(p, policy, &param2, true);
 }
 EXPORT_SYMBOL_GPL(sched_setscheduler);
 
+int sched_setscheduler2(struct task_struct *p, int policy,
+			  const struct sched_param2 *param2)
+{
+	return __sched_setscheduler(p, policy, param2, true);
+}
+EXPORT_SYMBOL_GPL(sched_setscheduler2);
+
 /**
  * sched_setscheduler_nocheck - change the scheduling policy and/or RT priority of a thread from kernelspace.
  * @p: the task in question.
@@ -3885,7 +4144,10 @@ EXPORT_SYMBOL_GPL(sched_setscheduler);
 int sched_setscheduler_nocheck(struct task_struct *p, int policy,
 			       const struct sched_param *param)
 {
-	return __sched_setscheduler(p, policy, param, false);
+	struct sched_param2 param2 = {
+		.sched_priority = param->sched_priority
+	};
+	return __sched_setscheduler(p, policy, &param2, false);
 }
 
 static int
@@ -3910,6 +4172,34 @@ do_sched_setscheduler(pid_t pid, int pol
 	return retval;
 }
 
+static int
+do_sched_setscheduler2(pid_t pid, int policy,
+			 struct sched_param2 __user *param2)
+{
+	struct sched_param2 lparam2;
+	struct task_struct *p;
+	int retval;
+
+	if (!param2 || pid < 0)
+		return -EINVAL;
+
+	memset(&lparam2, 0, sizeof(struct sched_param2));
+	if (copy_from_user(&lparam2, param2, sizeof(struct sched_param2)))
+		return -EFAULT;
+
+	rcu_read_lock();
+	retval = -ESRCH;
+	p = find_process_by_pid(pid);
+	if (p != NULL) {
+		if (dl_policy(policy))
+			lparam2.sched_priority = 0;
+		retval = sched_setscheduler2(p, policy, &lparam2);
+	}
+	rcu_read_unlock();
+
+	return retval;
+}
+
 /**
  * sys_sched_setscheduler - set/change the scheduler policy and RT priority
  * @pid: the pid in question.
@@ -3927,6 +4217,21 @@ SYSCALL_DEFINE3(sched_setscheduler, pid_
 }
 
 /**
+ * sys_sched_setscheduler2 - same as above, but with extended sched_param
+ * @pid: the pid in question.
+ * @policy: new policy (could use extended sched_param).
+ * @param: structure containg the extended parameters.
+ */
+SYSCALL_DEFINE3(sched_setscheduler2, pid_t, pid, int, policy,
+		struct sched_param2 __user *, param2)
+{
+	if (policy < 0)
+		return -EINVAL;
+
+	return do_sched_setscheduler2(pid, policy, param2);
+}
+
+/**
  * sys_sched_setparam - set/change the RT priority of a thread
  * @pid: the pid in question.
  * @param: structure containing the new RT priority.
@@ -3937,6 +4242,17 @@ SYSCALL_DEFINE2(sched_setparam, pid_t, p
 }
 
 /**
+ * sys_sched_setparam2 - same as above, but with extended sched_param
+ * @pid: the pid in question.
+ * @param2: structure containing the extended parameters.
+ */
+SYSCALL_DEFINE2(sched_setparam2, pid_t, pid,
+		struct sched_param2 __user *, param2)
+{
+	return do_sched_setscheduler2(pid, -1, param2);
+}
+
+/**
  * sys_sched_getscheduler - get the policy (scheduling class) of a thread
  * @pid: the pid in question.
  */
@@ -4000,6 +4316,48 @@ out_unlock:
 	return retval;
 }
 
+/**
+ * sys_sched_getparam2 - same as above, but with extended sched_param
+ * @pid: the pid in question.
+ * @param2: structure containing the extended parameters.
+ */
+SYSCALL_DEFINE2(sched_getparam2, pid_t, pid,
+		struct sched_param2 __user *, param2)
+{
+	struct sched_param2 lp;
+	struct task_struct *p;
+	int retval;
+
+	if (!param2 || pid < 0)
+		return -EINVAL;
+
+	rcu_read_lock();
+	p = find_process_by_pid(pid);
+	retval = -ESRCH;
+	if (!p)
+		goto out_unlock;
+
+	retval = security_task_getscheduler(p);
+	if (retval)
+		goto out_unlock;
+
+	if (task_has_dl_policy(p))
+		__getparam_dl(p, &lp);
+	else
+		lp.sched_priority = p->rt_priority;
+	rcu_read_unlock();
+
+	retval = copy_to_user(param2, &lp,
+			sizeof(struct sched_param2)) ? -EFAULT : 0;
+
+	return retval;
+
+out_unlock:
+	rcu_read_unlock();
+	return retval;
+
+}
+
 long sched_setaffinity(pid_t pid, const struct cpumask *in_mask)
 {
 	cpumask_var_t cpus_allowed, new_mask;
@@ -4036,6 +4394,24 @@ long sched_setaffinity(pid_t pid, const
 	if (retval)
 		goto out_unlock;
 
+	/*
+	 * Since bandwidth control happens on root_domain basis,
+	 * if admission test is enabled, we only admit -deadline
+	 * tasks allowed to run on all the CPUs in the task's
+	 * root_domain.
+	 */
+#ifdef CONFIG_SMP
+	if (task_has_dl_policy(p)) {
+		const struct cpumask *span = task_rq(p)->rd->span;
+
+		if (dl_bandwidth_enabled() &&
+		    !cpumask_equal(in_mask, span)) {
+			retval = -EBUSY;
+			goto out_unlock;
+		}
+	}
+#endif
+
 	cpuset_cpus_allowed(p, cpus_allowed);
 	cpumask_and(new_mask, in_mask, cpus_allowed);
 again:
@@ -4391,6 +4767,7 @@ SYSCALL_DEFINE1(sched_get_priority_max,
 	case SCHED_RR:
 		ret = MAX_USER_RT_PRIO-1;
 		break;
+	case SCHED_DEADLINE:
 	case SCHED_NORMAL:
 	case SCHED_BATCH:
 	case SCHED_IDLE:
@@ -4416,6 +4793,7 @@ SYSCALL_DEFINE1(sched_get_priority_min,
 	case SCHED_RR:
 		ret = 1;
 		break;
+	case SCHED_DEADLINE:
 	case SCHED_NORMAL:
 	case SCHED_BATCH:
 	case SCHED_IDLE:
@@ -4671,6 +5049,42 @@ out:
 EXPORT_SYMBOL_GPL(set_cpus_allowed_ptr);
 
 /*
+ * When dealing with a -deadline task, we have to check if moving it to
+ * a new CPU is possible or not. In fact, this is only true iff there
+ * is enough bandwidth available on such CPU, otherwise we want the
+ * whole migration progedure to fail over.
+ */
+static inline
+bool set_task_cpu_dl(struct task_struct *p, unsigned int cpu)
+{
+	struct dl_bw *dl_b = &task_rq(p)->rd->dl_bw;
+	struct dl_bw *cpu_b = &cpu_rq(cpu)->rd->dl_bw;
+	int ret = 1;
+	u64 bw;
+
+	if (dl_b == cpu_b)
+		return 1;
+
+	raw_spin_lock(&dl_b->lock);
+	raw_spin_lock(&cpu_b->lock);
+
+	bw = cpu_b->bw * cpumask_weight(cpu_rq(cpu)->rd->span);
+	if (dl_bandwidth_enabled() &&
+	    bw < cpu_b->total_bw + p->dl.dl_bw) {
+		ret = 0;
+		goto unlock;
+	}
+	dl_b->total_bw -= p->dl.dl_bw;
+	cpu_b->total_bw += p->dl.dl_bw;
+
+unlock:
+	raw_spin_unlock(&cpu_b->lock);
+	raw_spin_unlock(&dl_b->lock);
+
+	return ret;
+}
+
+/*
  * Move (not current) task off this cpu, onto dest cpu. We're doing
  * this because either it can't run here any more (set_cpus_allowed()
  * away from this CPU, or CPU going down), or because we're
@@ -4702,6 +5116,13 @@ static int __migrate_task(struct task_st
 		goto fail;
 
 	/*
+	 * If p is -deadline, proceed only if there is enough
+	 * bandwidth available on dest_cpu
+	 */
+	if (unlikely(dl_task(p)) && !set_task_cpu_dl(p, dest_cpu))
+		goto fail;
+
+	/*
 	 * If we're not on a rq, the next wake-up will ensure we're
 	 * placed properly.
 	 */
@@ -5328,6 +5749,8 @@ static void free_rootdomain(struct rcu_h
 	struct root_domain *rd = container_of(rcu, struct root_domain, rcu);
 
 	cpupri_cleanup(&rd->cpupri);
+	cpudl_cleanup(&rd->cpudl);
+	free_cpumask_var(rd->dlo_mask);
 	free_cpumask_var(rd->rto_mask);
 	free_cpumask_var(rd->online);
 	free_cpumask_var(rd->span);
@@ -5379,8 +5802,14 @@ static int init_rootdomain(struct root_d
 		goto out;
 	if (!alloc_cpumask_var(&rd->online, GFP_KERNEL))
 		goto free_span;
-	if (!alloc_cpumask_var(&rd->rto_mask, GFP_KERNEL))
+	if (!alloc_cpumask_var(&rd->dlo_mask, GFP_KERNEL))
 		goto free_online;
+	if (!alloc_cpumask_var(&rd->rto_mask, GFP_KERNEL))
+		goto free_dlo_mask;
+
+	init_dl_bw(&rd->dl_bw);
+	if (cpudl_init(&rd->cpudl) != 0)
+		goto free_dlo_mask;
 
 	if (cpupri_init(&rd->cpupri) != 0)
 		goto free_rto_mask;
@@ -5388,6 +5817,8 @@ static int init_rootdomain(struct root_d
 
 free_rto_mask:
 	free_cpumask_var(rd->rto_mask);
+free_dlo_mask:
+	free_cpumask_var(rd->dlo_mask);
 free_online:
 	free_cpumask_var(rd->online);
 free_span:
@@ -6713,6 +7144,7 @@ void __init sched_init_smp(void)
 	free_cpumask_var(non_isolated_cpus);
 
 	init_sched_rt_class();
+	init_sched_dl_class();
 }
 #else
 void __init sched_init_smp(void)
@@ -6784,6 +7216,8 @@ void __init sched_init(void)
 
 	init_rt_bandwidth(&def_rt_bandwidth,
 			global_rt_period(), global_rt_runtime());
+	init_dl_bandwidth(&def_dl_bandwidth,
+			global_rt_period(), global_dl_runtime());
 
 #ifdef CONFIG_RT_GROUP_SCHED
 	init_rt_bandwidth(&root_task_group.rt_bandwidth,
@@ -6814,6 +7248,7 @@ void __init sched_init(void)
 		rq->calc_load_update = jiffies + LOAD_FREQ;
 		init_cfs_rq(&rq->cfs);
 		init_rt_rq(&rq->rt, rq);
+		init_dl_rq(&rq->dl, rq);
 #ifdef CONFIG_FAIR_GROUP_SCHED
 		root_task_group.shares = ROOT_TASK_GROUP_LOAD;
 		INIT_LIST_HEAD(&rq->leaf_cfs_rq_list);
@@ -6881,10 +7316,6 @@ void __init sched_init(void)
 	INIT_HLIST_HEAD(&init_task.preempt_notifiers);
 #endif
 
-#ifdef CONFIG_RT_MUTEXES
-	plist_head_init(&init_task.pi_waiters);
-#endif
-
 	/*
 	 * The boot idle thread does lazy MMU switching as well:
 	 */
@@ -6994,7 +7425,7 @@ void normalize_rt_tasks(void)
 		p->se.statistics.block_start	= 0;
 #endif
 
-		if (!rt_task(p)) {
+		if (!dl_task(p) && !rt_task(p)) {
 			/*
 			 * Renice negative nice level userspace
 			 * tasks back to 0:
@@ -7180,15 +7611,92 @@ void sched_move_task(struct task_struct
 }
 #endif /* CONFIG_CGROUP_SCHED */
 
-#if defined(CONFIG_RT_GROUP_SCHED) || defined(CONFIG_CFS_BANDWIDTH)
-static unsigned long to_ratio(u64 period, u64 runtime)
+static u64 actual_dl_runtime(void)
 {
-	if (runtime == RUNTIME_INF)
-		return 1ULL << 20;
+	u64 dl_runtime = global_dl_runtime();
+	u64 rt_runtime = global_rt_runtime();
+	u64 period = global_rt_period();
 
-	return div64_u64(runtime << 20, period);
+	/*
+	 * We want to calculate the sub-quota of rt_bw actually available
+	 * for -dl tasks. It is a percentage of percentage. By default 95%
+	 * of system bandwidth is allocate to -rt tasks; among this, a 40%
+	 * quota is reserved for -dl tasks. To have the actual quota a simple
+	 * multiplication is needed: .95 * .40 = .38 (38% of system bandwidth
+	 * for deadline tasks).
+	 * What follows is basically the same, but using unsigned integers.
+	 *
+	 *                   dl_runtime   rt_runtime
+	 * actual_runtime =  ---------- * ---------- * period
+	 *                     period       period
+	 */
+	if (dl_runtime == RUNTIME_INF)
+		return RUNTIME_INF;
+
+	return div64_u64 (dl_runtime * rt_runtime, period);
 }
+
+static int check_dl_bw(void)
+{
+	int i;
+	u64 period = global_rt_period();
+	u64 dl_actual_runtime = actual_dl_runtime();
+	u64 new_bw = to_ratio(period, dl_actual_runtime);
+
+	/*
+	 * Here we want to check the bandwidth not being set to some
+	 * value smaller than the currently allocated bandwidth in
+	 * any of the root_domains.
+	 *
+	 * FIXME: Cycling on all the CPUs is overdoing, but simpler than
+	 * cycling on root_domains... Discussion on different/better
+	 * solutions is welcome!
+	 */
+	for_each_possible_cpu(i) {
+#ifdef CONFIG_SMP
+		struct dl_bw *dl_b = &cpu_rq(i)->rd->dl_bw;
+#else
+		struct dl_bw *dl_b = &cpu_rq(i)->dl.dl_bw;
 #endif
+		raw_spin_lock(&dl_b->lock);
+		if (new_bw < dl_b->total_bw) {
+			raw_spin_unlock(&dl_b->lock);
+			return -EBUSY;
+		}
+		raw_spin_unlock(&dl_b->lock);
+	}
+
+	return 0;
+}
+
+static void update_dl_bw(void)
+{
+	u64 new_bw;
+	int i;
+
+	def_dl_bandwidth.dl_runtime = global_dl_runtime();
+	if (global_dl_runtime() == RUNTIME_INF ||
+	    global_rt_runtime() == RUNTIME_INF)
+		new_bw = -1;
+	else {
+		new_bw = to_ratio(global_rt_period(),
+				  actual_dl_runtime());
+	}
+	/*
+	 * FIXME: As above...
+	 */
+	for_each_possible_cpu(i) {
+#ifdef CONFIG_SMP
+		struct dl_bw *dl_b = &cpu_rq(i)->rd->dl_bw;
+#else
+		struct dl_bw *dl_b = &cpu_rq(i)->dl.dl_bw;
+#endif
+
+		raw_spin_lock(&dl_b->lock);
+		dl_b->bw = new_bw;
+		raw_spin_unlock(&dl_b->lock);
+	}
+}
 
 #ifdef CONFIG_RT_GROUP_SCHED
 /*
@@ -7381,6 +7889,14 @@ static int sched_rt_global_constraints(v
 	if (runtime > period && runtime != RUNTIME_INF)
 		return -EINVAL;
 
+	/*
+	 * Check if changing rt_bw could have negative effects
+	 * on dl_bw
+	 */
+	ret = check_dl_bw();
+	if (ret)
+		return ret;
+
 	mutex_lock(&rt_constraints_mutex);
 	read_lock(&tasklist_lock);
 	ret = __rt_schedulable(NULL, 0, 0);
@@ -7403,7 +7919,7 @@ int sched_rt_can_attach(struct task_grou
 static int sched_rt_global_constraints(void)
 {
 	unsigned long flags;
-	int i;
+	int i, ret;
 
 	if (sysctl_sched_rt_period <= 0)
 		return -EINVAL;
@@ -7415,7 +7931,16 @@ static int sched_rt_global_constraints(v
 	if (sysctl_sched_rt_runtime == 0)
 		return -EBUSY;
 
+	/*
+	 * Check if changing rt_bw could have negative effects
+	 * on dl_bw
+	 */
+	ret = check_dl_bw();
+	if (ret)
+		return ret;
+
 	raw_spin_lock_irqsave(&def_rt_bandwidth.rt_runtime_lock, flags);
+
 	for_each_possible_cpu(i) {
 		struct rt_rq *rt_rq = &cpu_rq(i)->rt;
 
@@ -7429,6 +7954,31 @@ static int sched_rt_global_constraints(v
 }
 #endif /* CONFIG_RT_GROUP_SCHED */
 
+static bool __sched_dl_global_constraints(u64 runtime, u64 period)
+{
+	if (!period || (runtime != RUNTIME_INF && runtime > period))
+		return -EINVAL;
+
+	return 0;
+}
+
+static int sched_dl_global_constraints(void)
+{
+	u64 period = global_rt_period();
+	u64 dl_actual_runtime = actual_dl_runtime();
+	int ret;
+
+	ret = __sched_dl_global_constraints(dl_actual_runtime, period);
+	if (ret)
+		return ret;
+
+	ret = check_dl_bw();
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
 int sched_rt_handler(struct ctl_table *table, int write,
 		void __user *buffer, size_t *lenp,
 		loff_t *ppos)
@@ -7436,6 +7986,7 @@ int sched_rt_handler(struct ctl_table *t
 	int ret;
 	int old_period, old_runtime;
 	static DEFINE_MUTEX(mutex);
+	unsigned long flags;
 
 	mutex_lock(&mutex);
 	old_period = sysctl_sched_rt_period;
@@ -7445,6 +7996,8 @@ int sched_rt_handler(struct ctl_table *t
 
 	if (!ret && write) {
 		ret = sched_rt_global_constraints();
+		raw_spin_lock_irqsave(&def_dl_bandwidth.dl_runtime_lock,
+				      flags);
 		if (ret) {
 			sysctl_sched_rt_period = old_period;
 			sysctl_sched_rt_runtime = old_runtime;
@@ -7452,7 +8005,44 @@ int sched_rt_handler(struct ctl_table *t
 			def_rt_bandwidth.rt_runtime = global_rt_runtime();
 			def_rt_bandwidth.rt_period =
 				ns_to_ktime(global_rt_period());
+
+			update_dl_bw();
 		}
+		raw_spin_unlock_irqrestore(&def_dl_bandwidth.dl_runtime_lock,
+					   flags);
+	}
+	mutex_unlock(&mutex);
+
+	return ret;
+}
+
+int sched_dl_handler(struct ctl_table *table, int write,
+		void __user *buffer, size_t *lenp,
+		loff_t *ppos)
+{
+	int ret;
+	int old_runtime;
+	static DEFINE_MUTEX(mutex);
+	unsigned long flags;
+
+	mutex_lock(&mutex);
+	old_runtime = sysctl_sched_dl_runtime;
+
+	ret = proc_dointvec(table, write, buffer, lenp, ppos);
+
+	if (!ret && write) {
+		raw_spin_lock_irqsave(&def_dl_bandwidth.dl_runtime_lock,
+				      flags);
+
+		ret = sched_dl_global_constraints();
+		if (ret) {
+			sysctl_sched_dl_runtime = old_runtime;
+		} else {
+			update_dl_bw();
+		}
+
+		raw_spin_unlock_irqrestore(&def_dl_bandwidth.dl_runtime_lock,
+					   flags);
 	}
 	mutex_unlock(&mutex);
 
diff -Nurp linux/kernel/sched/cpudeadline.c sched-deadline-mainline-dl/kernel/sched/cpudeadline.c
--- linux/kernel/sched/cpudeadline.c	1969-12-31 19:00:00.000000000 -0500
+++ sched-deadline-mainline-dl/kernel/sched/cpudeadline.c	2012-11-21 15:11:30.000000000 -0500
@@ -0,0 +1,208 @@
+/*
+ *  kernel/sched/cpudl.c
+ *
+ *  Global CPU deadline management
+ *
+ *  Author: Juri Lelli <j.lelli@sssup.it>
+ *
+ *  This program is free software; you can redistribute it and/or
+ *  modify it under the terms of the GNU General Public License
+ *  as published by the Free Software Foundation; version 2
+ *  of the License.
+ */
+
+#include <linux/gfp.h>
+#include <linux/kernel.h>
+#include "cpudeadline.h"
+
+static inline int parent(int i)
+{
+	return (i - 1) >> 1;
+}
+
+static inline int left_child(int i)
+{
+	return (i << 1) + 1;
+}
+
+static inline int right_child(int i)
+{
+	return (i << 1) + 2;
+}
+
+static inline int dl_time_before(u64 a, u64 b)
+{
+	return (s64)(a - b) < 0;
+}
+
+void cpudl_exchange(struct cpudl *cp, int a, int b)
+{
+	int cpu_a = cp->elements[a].cpu, cpu_b = cp->elements[b].cpu;
+
+	swap(cp->elements[a], cp->elements[b]);
+	swap(cp->cpu_to_idx[cpu_a], cp->cpu_to_idx[cpu_b]);
+}
+
+void cpudl_heapify(struct cpudl *cp, int idx)
+{
+	int l, r, largest;
+
+	/* adapted from lib/prio_heap.c */
+	while(1) {
+		l = left_child(idx);
+		r = right_child(idx);
+		largest = idx;
+
+		if ((l < cp->size) && dl_time_before(cp->elements[idx].dl,
+							cp->elements[l].dl))
+			largest = l;
+		if ((r < cp->size) && dl_time_before(cp->elements[largest].dl,
+							cp->elements[r].dl))
+			largest = r;
+		if (largest == idx)
+			break;
+
+		/* Push idx down the heap one level and bump one up */
+		cpudl_exchange(cp, largest, idx);
+		idx = largest;
+	}
+}
+
+void cpudl_change_key(struct cpudl *cp, int idx, u64 new_dl)
+{
+	WARN_ON(idx > num_present_cpus() && idx != -1);
+
+	if (dl_time_before(new_dl, cp->elements[idx].dl)) {
+		cp->elements[idx].dl = new_dl;
+		cpudl_heapify(cp, idx);
+	} else {
+		cp->elements[idx].dl = new_dl;
+		while (idx > 0 && dl_time_before(cp->elements[parent(idx)].dl,
+					cp->elements[idx].dl)) {
+			cpudl_exchange(cp, idx, parent(idx));
+			idx = parent(idx);
+		}
+	}
+}
+
+static inline int cpudl_maximum(struct cpudl *cp)
+{
+	return cp->elements[0].cpu;
+}
+
+/*
+ * cpudl_find - find the best (later-dl) CPU in the system
+ * @cp: the cpudl max-heap context
+ * @p: the task
+ * @later_mask: a mask to fill in with the selected CPUs (or NULL)
+ *
+ * Returns: int - best CPU (heap maximum if suitable)
+ */
+int cpudl_find(struct cpudl *cp, struct task_struct *p,
+	       struct cpumask *later_mask)
+{
+	int best_cpu = -1;
+	const struct sched_dl_entity *dl_se = &p->dl;
+
+	if (later_mask && cpumask_and(later_mask, cp->free_cpus,
+			&p->cpus_allowed) && cpumask_and(later_mask,
+			later_mask, cpu_active_mask)) {
+		best_cpu = cpumask_any(later_mask);
+		goto out;
+	} else if (cpumask_test_cpu(cpudl_maximum(cp), &p->cpus_allowed) &&
+			dl_time_before(dl_se->deadline, cp->elements[0].dl)) {
+		best_cpu = cpudl_maximum(cp);
+		if (later_mask)
+			cpumask_set_cpu(best_cpu, later_mask);
+	}
+
+out:
+	WARN_ON(best_cpu > num_present_cpus() && best_cpu != -1);
+
+	return best_cpu;
+}
+
+/*
+ * cpudl_set - update the cpudl max-heap
+ * @cp: the cpudl max-heap context
+ * @cpu: the target cpu
+ * @dl: the new earliest deadline for this cpu
+ *
+ * Notes: assumes cpu_rq(cpu)->lock is locked
+ *
+ * Returns: (void)
+ */
+void cpudl_set(struct cpudl *cp, int cpu, u64 dl, int is_valid)
+{
+	int old_idx, new_cpu;
+	unsigned long flags;
+
+	WARN_ON(cpu > num_present_cpus());
+
+	raw_spin_lock_irqsave(&cp->lock, flags);
+	old_idx = cp->cpu_to_idx[cpu];
+	if (!is_valid) {
+		/* remove item */
+		new_cpu = cp->elements[cp->size - 1].cpu;
+		cp->elements[old_idx].dl = cp->elements[cp->size - 1].dl;
+		cp->elements[old_idx].cpu = new_cpu;
+		cp->size--;
+		cp->cpu_to_idx[new_cpu] = old_idx;
+		cp->cpu_to_idx[cpu] = IDX_INVALID;
+		while (old_idx > 0 && dl_time_before(
+				cp->elements[parent(old_idx)].dl,
+				cp->elements[old_idx].dl)) {
+			cpudl_exchange(cp, old_idx, parent(old_idx));
+			old_idx = parent(old_idx);
+		}
+		cpumask_set_cpu(cpu, cp->free_cpus);
+                cpudl_heapify(cp, old_idx);
+
+		goto out;
+	}
+
+	if (old_idx == IDX_INVALID) {
+		cp->size++;
+		cp->elements[cp->size - 1].dl = 0;
+		cp->elements[cp->size - 1].cpu = cpu;
+		cp->cpu_to_idx[cpu] = cp->size - 1;
+		cpudl_change_key(cp, cp->size - 1, dl);
+		cpumask_clear_cpu(cpu, cp->free_cpus);
+	} else {
+		cpudl_change_key(cp, old_idx, dl);
+	}
+
+out:
+	raw_spin_unlock_irqrestore(&cp->lock, flags);
+}
+
+/*
+ * cpudl_init - initialize the cpudl structure
+ * @cp: the cpudl max-heap context
+ */
+int cpudl_init(struct cpudl *cp)
+{
+	int i;
+
+	memset(cp, 0, sizeof(*cp));
+	raw_spin_lock_init(&cp->lock);
+	cp->size = 0;
+	for (i = 0; i < NR_CPUS; i++)
+		cp->cpu_to_idx[i] = IDX_INVALID;
+	if (!alloc_cpumask_var(&cp->free_cpus, GFP_KERNEL))
+		return -ENOMEM;
+	cpumask_setall(cp->free_cpus);
+
+	return 0;
+}
+
+/*
+ * cpudl_cleanup - clean up the cpudl structure
+ * @cp: the cpudl max-heap context
+ */
+void cpudl_cleanup(struct cpudl *cp)
+{
+	/*
+	 * nothing to do for the moment
+	 */
+}
diff -Nurp linux/kernel/sched/cpudeadline.h sched-deadline-mainline-dl/kernel/sched/cpudeadline.h
--- linux/kernel/sched/cpudeadline.h	1969-12-31 19:00:00.000000000 -0500
+++ sched-deadline-mainline-dl/kernel/sched/cpudeadline.h	2012-11-21 15:11:30.000000000 -0500
@@ -0,0 +1,33 @@
+#ifndef _LINUX_CPUDL_H
+#define _LINUX_CPUDL_H
+
+#include <linux/sched.h>
+
+#define IDX_INVALID     -1
+
+struct array_item {
+	u64 dl;
+	int cpu;
+};
+
+struct cpudl {
+	raw_spinlock_t lock;
+	int size;
+	int cpu_to_idx[NR_CPUS];
+	struct array_item elements[NR_CPUS];
+	cpumask_var_t free_cpus;
+};
+
+
+#ifdef CONFIG_SMP
+int cpudl_find(struct cpudl *cp, struct task_struct *p,
+	       struct cpumask *later_mask);
+void cpudl_set(struct cpudl *cp, int cpu, u64 dl, int is_valid);
+int cpudl_init(struct cpudl *cp);
+void cpudl_cleanup(struct cpudl *cp);
+#else
+#define cpudl_set(cp, cpu, dl) do { } while (0)
+#define cpudl_init() do { } while (0)
+#endif /* CONFIG_SMP */
+
+#endif /* _LINUX_CPUDL_H */
diff -Nurp linux/kernel/sched/deadline.c sched-deadline-mainline-dl/kernel/sched/deadline.c
--- linux/kernel/sched/deadline.c	1969-12-31 19:00:00.000000000 -0500
+++ sched-deadline-mainline-dl/kernel/sched/deadline.c	2012-11-21 15:11:30.000000000 -0500
@@ -0,0 +1,1655 @@
+/*
+ * Deadline Scheduling Class (SCHED_DEADLINE)
+ *
+ * Earliest Deadline First (EDF) + Constant Bandwidth Server (CBS).
+ *
+ * Tasks that periodically executes their instances for less than their
+ * runtime won't miss any of their deadlines.
+ * Tasks that are not periodic or sporadic or that tries to execute more
+ * than their reserved bandwidth will be slowed down (and may potentially
+ * miss some of their deadlines), and won't affect any other task.
+ *
+ * Copyright (C) 2012 Dario Faggioli <raistlin@linux.it>,
+ *                    Juri Lelli <juri.lelli@gmail.com>,
+ *                    Michael Trimarchi <michael@amarulasolutions.com>,
+ *                    Fabio Checconi <fchecconi@gmail.com>
+ */
+#include "sched.h"
+
+#include <linux/slab.h>
+
+struct dl_bandwidth def_dl_bandwidth;
+
+static inline struct task_struct *dl_task_of(struct sched_dl_entity *dl_se)
+{
+	return container_of(dl_se, struct task_struct, dl);
+}
+
+static inline struct rq *rq_of_dl_rq(struct dl_rq *dl_rq)
+{
+	return container_of(dl_rq, struct rq, dl);
+}
+
+static inline struct dl_rq *dl_rq_of_se(struct sched_dl_entity *dl_se)
+{
+	struct task_struct *p = dl_task_of(dl_se);
+	struct rq *rq = task_rq(p);
+
+	return &rq->dl;
+}
+
+static inline int on_dl_rq(struct sched_dl_entity *dl_se)
+{
+	return !RB_EMPTY_NODE(&dl_se->rb_node);
+}
+
+static inline int is_leftmost(struct task_struct *p, struct dl_rq *dl_rq)
+{
+	struct sched_dl_entity *dl_se = &p->dl;
+
+	return dl_rq->rb_leftmost == &dl_se->rb_node;
+}
+
+void init_dl_bandwidth(struct dl_bandwidth *dl_b, u64 period, u64 runtime)
+{
+	raw_spin_lock_init(&dl_b->dl_runtime_lock);
+	dl_b->dl_runtime = runtime;
+}
+
+extern unsigned long to_ratio(u64 period, u64 runtime);
+
+void init_dl_bw(struct dl_bw *dl_b)
+{
+	raw_spin_lock_init(&dl_b->lock);
+	raw_spin_lock(&def_dl_bandwidth.dl_runtime_lock);
+	if (global_dl_runtime() == RUNTIME_INF)
+		dl_b->bw = -1;
+	else
+		dl_b->bw = to_ratio(global_rt_period(), global_dl_runtime());
+	raw_spin_unlock(&def_dl_bandwidth.dl_runtime_lock);
+	dl_b->total_bw = 0;
+}
+
+void init_dl_rq(struct dl_rq *dl_rq, struct rq *rq)
+{
+	dl_rq->rb_root = RB_ROOT;
+
+#ifdef CONFIG_SMP
+	/* zero means no -deadline tasks */
+	dl_rq->earliest_dl.curr = dl_rq->earliest_dl.next = 0;
+
+	dl_rq->dl_nr_migratory = 0;
+	dl_rq->overloaded = 0;
+	dl_rq->pushable_dl_tasks_root = RB_ROOT;
+#else
+	init_dl_bw(&dl_rq->dl_bw);
+#endif
+}
+
+#ifdef CONFIG_SMP
+
+static inline int dl_overloaded(struct rq *rq)
+{
+	return atomic_read(&rq->rd->dlo_count);
+}
+
+static inline void dl_set_overload(struct rq *rq)
+{
+	if (!rq->online)
+		return;
+
+	cpumask_set_cpu(rq->cpu, rq->rd->dlo_mask);
+	/*
+	 * Must be visible before the overload count is
+	 * set (as in sched_rt.c).
+	 */
+	wmb();
+	atomic_inc(&rq->rd->dlo_count);
+}
+
+static inline void dl_clear_overload(struct rq *rq)
+{
+	if (!rq->online)
+		return;
+
+	atomic_dec(&rq->rd->dlo_count);
+	cpumask_clear_cpu(rq->cpu, rq->rd->dlo_mask);
+}
+
+static void update_dl_migration(struct dl_rq *dl_rq)
+{
+	if (dl_rq->dl_nr_migratory && dl_rq->dl_nr_total > 1) {
+		if (!dl_rq->overloaded) {
+			dl_set_overload(rq_of_dl_rq(dl_rq));
+			dl_rq->overloaded = 1;
+		}
+	} else if (dl_rq->overloaded) {
+		dl_clear_overload(rq_of_dl_rq(dl_rq));
+		dl_rq->overloaded = 0;
+	}
+}
+
+static void inc_dl_migration(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)
+{
+	struct task_struct *p = dl_task_of(dl_se);
+	dl_rq = &rq_of_dl_rq(dl_rq)->dl;
+
+	dl_rq->dl_nr_total++;
+	if (p->nr_cpus_allowed > 1)
+		dl_rq->dl_nr_migratory++;
+
+	update_dl_migration(dl_rq);
+}
+
+static void dec_dl_migration(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)
+{
+	struct task_struct *p = dl_task_of(dl_se);
+	dl_rq = &rq_of_dl_rq(dl_rq)->dl;
+
+	dl_rq->dl_nr_total--;
+	if (p->nr_cpus_allowed > 1)
+		dl_rq->dl_nr_migratory--;
+
+	update_dl_migration(dl_rq);
+}
+
+/*
+ * The list of pushable -deadline task is not a plist, like in
+ * sched_rt.c, it is an rb-tree with tasks ordered by deadline.
+ */
+static void enqueue_pushable_dl_task(struct rq *rq, struct task_struct *p)
+{
+	struct dl_rq *dl_rq = &rq->dl;
+	struct rb_node **link = &dl_rq->pushable_dl_tasks_root.rb_node;
+	struct rb_node *parent = NULL;
+	struct task_struct *entry;
+	int leftmost = 1;
+
+	BUG_ON(!RB_EMPTY_NODE(&p->pushable_dl_tasks));
+
+	while (*link) {
+		parent = *link;
+		entry = rb_entry(parent, struct task_struct,
+				 pushable_dl_tasks);
+		if (dl_entity_preempt(&p->dl, &entry->dl))
+			link = &parent->rb_left;
+		else {
+			link = &parent->rb_right;
+			leftmost = 0;
+		}
+	}
+
+	if (leftmost)
+		dl_rq->pushable_dl_tasks_leftmost = &p->pushable_dl_tasks;
+
+	rb_link_node(&p->pushable_dl_tasks, parent, link);
+	rb_insert_color(&p->pushable_dl_tasks, &dl_rq->pushable_dl_tasks_root);
+}
+
+static void dequeue_pushable_dl_task(struct rq *rq, struct task_struct *p)
+{
+	struct dl_rq *dl_rq = &rq->dl;
+
+	if (RB_EMPTY_NODE(&p->pushable_dl_tasks))
+		return;
+
+	if (dl_rq->pushable_dl_tasks_leftmost == &p->pushable_dl_tasks) {
+		struct rb_node *next_node;
+
+		next_node = rb_next(&p->pushable_dl_tasks);
+		dl_rq->pushable_dl_tasks_leftmost = next_node;
+	}
+
+	rb_erase(&p->pushable_dl_tasks, &dl_rq->pushable_dl_tasks_root);
+	RB_CLEAR_NODE(&p->pushable_dl_tasks);
+}
+
+static inline int has_pushable_dl_tasks(struct rq *rq)
+{
+	return !RB_EMPTY_ROOT(&rq->dl.pushable_dl_tasks_root);
+}
+
+static int push_dl_task(struct rq *rq);
+
+#else
+
+static inline
+void enqueue_pushable_dl_task(struct rq *rq, struct task_struct *p)
+{
+}
+
+static inline
+void dequeue_pushable_dl_task(struct rq *rq, struct task_struct *p)
+{
+}
+
+static inline
+void inc_dl_migration(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)
+{
+}
+
+static inline
+void dec_dl_migration(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)
+{
+}
+
+#endif /* CONFIG_SMP */
+
+static void enqueue_task_dl(struct rq *rq, struct task_struct *p, int flags);
+static void __dequeue_task_dl(struct rq *rq, struct task_struct *p, int flags);
+static void check_preempt_curr_dl(struct rq *rq, struct task_struct *p,
+				  int flags);
+
+/*
+ * We are being explicitly informed that a new instance is starting,
+ * and this means that:
+ *  - the absolute deadline of the entity has to be placed at
+ *    current time + relative deadline;
+ *  - the runtime of the entity has to be set to the maximum value.
+ *
+ * The capability of specifying such event is useful whenever a -deadline
+ * entity wants to (try to!) synchronize its behaviour with the scheduler's
+ * one, and to (try to!) reconcile itself with its own scheduling
+ * parameters.
+ */
+static inline void setup_new_dl_entity(struct sched_dl_entity *dl_se,
+				       struct sched_dl_entity *pi_se)
+{
+	struct dl_rq *dl_rq = dl_rq_of_se(dl_se);
+	struct rq *rq = rq_of_dl_rq(dl_rq);
+
+	WARN_ON(!dl_se->dl_new || dl_se->dl_throttled);
+
+	/*
+	 * We use the regular wall clock time to set deadlines in the
+	 * future; in fact, we must consider execution overheads (time
+	 * spent on hardirq context, etc.).
+	 */
+	dl_se->deadline = rq->clock + pi_se->dl_deadline;
+	dl_se->runtime = pi_se->dl_runtime;
+	dl_se->dl_new = 0;
+}
+
+/*
+ * Pure Earliest Deadline First (EDF) scheduling does not deal with the
+ * possibility of a entity lasting more than what it declared, and thus
+ * exhausting its runtime.
+ *
+ * Here we are interested in making runtime overrun possible, but we do
+ * not want a entity which is misbehaving to affect the scheduling of all
+ * other entities.
+ * Therefore, a budgeting strategy called Constant Bandwidth Server (CBS)
+ * is used, in order to confine each entity within its own bandwidth.
+ *
+ * This function deals exactly with that, and ensures that when the runtime
+ * of a entity is replenished, its deadline is also postponed. That ensures
+ * the overrunning entity can't interfere with other entity in the system and
+ * can't make them miss their deadlines. Reasons why this kind of overruns
+ * could happen are, typically, a entity voluntarily trying to overcome its
+ * runtime, or it just underestimated it during sched_setscheduler_ex().
+ */
+static void replenish_dl_entity(struct sched_dl_entity *dl_se,
+				struct sched_dl_entity *pi_se)
+{
+	struct dl_rq *dl_rq = dl_rq_of_se(dl_se);
+	struct rq *rq = rq_of_dl_rq(dl_rq);
+
+	BUG_ON(pi_se->dl_runtime <= 0);
+
+	/*
+	 * This could be the case for a !-dl task that is boosted.
+	 * Just go with full inherited parameters.
+	 */
+	if (dl_se->dl_deadline == 0) {
+		dl_se->deadline = rq->clock + pi_se->dl_deadline;
+		dl_se->runtime = pi_se->dl_runtime;
+	}
+
+	/*
+	 * We keep moving the deadline away until we get some
+	 * available runtime for the entity. This ensures correct
+	 * handling of situations where the runtime overrun is
+	 * arbitrary large.
+	 */
+	while (dl_se->runtime <= 0) {
+		dl_se->deadline += pi_se->dl_period;
+		dl_se->runtime += pi_se->dl_runtime;
+	}
+
+	/*
+	 * At this point, the deadline really should be "in
+	 * the future" with respect to rq->clock. If it's
+	 * not, we are, for some reason, lagging too much!
+	 * Anyway, after having warn userspace abut that,
+	 * we still try to keep the things running by
+	 * resetting the deadline and the budget of the
+	 * entity.
+	 */
+	if (dl_time_before(dl_se->deadline, rq->clock)) {
+		static bool lag_once = false;
+
+		if (!lag_once) {
+			lag_once = true;
+			printk_sched("sched: DL replenish lagged to much\n");
+		}
+		dl_se->deadline = rq->clock + pi_se->dl_deadline;
+		dl_se->runtime = pi_se->dl_runtime;
+	}
+}
+
+/*
+ * Here we check if --at time t-- an entity (which is probably being
+ * [re]activated or, in general, enqueued) can use its remaining runtime
+ * and its current deadline _without_ exceeding the bandwidth it is
+ * assigned (function returns true if it can't). We are in fact applying
+ * one of the CBS rules: when a task wakes up, if the residual runtime
+ * over residual deadline fits within the allocated bandwidth, then we
+ * can keep the current (absolute) deadline and residual budget without
+ * disrupting the schedulability of the system. Otherwise, we should
+ * refill the runtime and set the deadline a period in the future,
+ * because keeping the current (absolute) deadline of the task would
+ * result in breaking guarantees promised to other tasks (refer to
+ * Documentation/scheduler/sched-deadline.txt for more informations).
+ *
+ * This function returns true if:
+ *
+ *   runtime / (deadline - t) > dl_runtime / dl_period ,
+ *
+ * IOW we can't recycle current parameters.
+ *
+ * Notice that the bandwidth check is done against the period. For
+ * task with deadline equal to period this is the same of using
+ * dl_deadline instead of dl_period in the equation above.
+ */
+static bool dl_entity_overflow(struct sched_dl_entity *dl_se,
+			       struct sched_dl_entity *pi_se, u64 t)
+{
+	u64 left, right;
+
+	/*
+	 * left and right are the two sides of the equation above,
+	 * after a bit of shuffling to use multiplications instead
+	 * of divisions.
+	 *
+	 * Note that none of the time values involved in the two
+	 * multiplications are absolute: dl_deadline and dl_runtime
+	 * are the relative deadline and the maximum runtime of each
+	 * instance, runtime is the runtime left for the last instance
+	 * and (deadline - t), since t is rq->clock, is the time left
+	 * to the (absolute) deadline. Even if overflowing the u64 type
+	 * is very unlikely to occur in both cases, here we scale down
+	 * as we want to avoid that risk at all. Scaling down by 10
+	 * means that we reduce granularity to 1us. We are fine with it,
+	 * since this is only a true/false check and, anyway, thinking
+	 * of anything below microseconds resolution is actually fiction
+	 * (but still we want to give the user that illusion >;).
+	 */
+	left = (pi_se->dl_period >> DL_SCALE) * (dl_se->runtime >> DL_SCALE);
+	right = ((dl_se->deadline - t) >> DL_SCALE) *
+		(pi_se->dl_runtime >> DL_SCALE);
+
+	return dl_time_before(right, left);
+}
+
+/*
+ * When a -deadline entity is queued back on the runqueue, its runtime and
+ * deadline might need updating.
+ *
+ * The policy here is that we update the deadline of the entity only if:
+ *  - the current deadline is in the past,
+ *  - using the remaining runtime with the current deadline would make
+ *    the entity exceed its bandwidth.
+ */
+static void update_dl_entity(struct sched_dl_entity *dl_se,
+			     struct sched_dl_entity *pi_se)
+{
+	struct dl_rq *dl_rq = dl_rq_of_se(dl_se);
+	struct rq *rq = rq_of_dl_rq(dl_rq);
+
+	/*
+	 * The arrival of a new instance needs special treatment, i.e.,
+	 * the actual scheduling parameters have to be "renewed".
+	 */
+	if (dl_se->dl_new) {
+		setup_new_dl_entity(dl_se, pi_se);
+		return;
+	}
+
+	if (dl_time_before(dl_se->deadline, rq->clock) ||
+	    dl_entity_overflow(dl_se, pi_se, rq->clock)) {
+		dl_se->deadline = rq->clock + pi_se->dl_deadline;
+		dl_se->runtime = pi_se->dl_runtime;
+	}
+}
+
+/*
+ * If the entity depleted all its runtime, and if we want it to sleep
+ * while waiting for some new execution time to become available, we
+ * set the bandwidth enforcement timer to the replenishment instant
+ * and try to activate it.
+ *
+ * Notice that it is important for the caller to know if the timer
+ * actually started or not (i.e., the replenishment instant is in
+ * the future or in the past).
+ */
+static int start_dl_timer(struct sched_dl_entity *dl_se, bool boosted)
+{
+	struct dl_rq *dl_rq = dl_rq_of_se(dl_se);
+	struct rq *rq = rq_of_dl_rq(dl_rq);
+	ktime_t now, act;
+	ktime_t soft, hard;
+	unsigned long range;
+	s64 delta;
+
+	if (boosted)
+		return 0;
+	/*
+	 * We want the timer to fire at the deadline, but considering
+	 * that it is actually coming from rq->clock and not from
+	 * hrtimer's time base reading.
+	 */
+	act = ns_to_ktime(dl_se->deadline);
+	now = hrtimer_cb_get_time(&dl_se->dl_timer);
+	delta = ktime_to_ns(now) - rq->clock;
+	act = ktime_add_ns(act, delta);
+
+	/*
+	 * If the expiry time already passed, e.g., because the value
+	 * chosen as the deadline is too small, don't even try to
+	 * start the timer in the past!
+	 */
+	if (ktime_us_delta(act, now) < 0)
+		return 0;
+
+	hrtimer_set_expires(&dl_se->dl_timer, act);
+
+	soft = hrtimer_get_softexpires(&dl_se->dl_timer);
+	hard = hrtimer_get_expires(&dl_se->dl_timer);
+	range = ktime_to_ns(ktime_sub(hard, soft));
+	__hrtimer_start_range_ns(&dl_se->dl_timer, soft,
+				 range, HRTIMER_MODE_ABS, 0);
+
+	return hrtimer_active(&dl_se->dl_timer);
+}
+
+/*
+ * This is the bandwidth enforcement timer callback. If here, we know
+ * a task is not on its dl_rq, since the fact that the timer was running
+ * means the task is throttled and needs a runtime replenishment.
+ *
+ * However, what we actually do depends on the fact the task is active,
+ * (it is on its rq) or has been removed from there by a call to
+ * dequeue_task_dl(). In the former case we must issue the runtime
+ * replenishment and add the task back to the dl_rq; in the latter, we just
+ * do nothing but clearing dl_throttled, so that runtime and deadline
+ * updating (and the queueing back to dl_rq) will be done by the
+ * next call to enqueue_task_dl().
+ */
+static enum hrtimer_restart dl_task_timer(struct hrtimer *timer)
+{
+	struct sched_dl_entity *dl_se = container_of(timer,
+						     struct sched_dl_entity,
+						     dl_timer);
+	struct task_struct *p = dl_task_of(dl_se);
+	struct rq *rq = task_rq(p);
+	raw_spin_lock(&rq->lock);
+
+	/*
+	 * We need to take care of a possible races here. In fact, the
+	 * task might have changed its scheduling policy to something
+	 * different from SCHED_DEADLINE or changed its reservation
+	 * parameters (through sched_setscheduler()).
+	 */
+	if (!dl_task(p) || dl_se->dl_new)
+		goto unlock;
+
+	dl_se->dl_throttled = 0;
+	if (p->on_rq) {
+		enqueue_task_dl(rq, p, ENQUEUE_REPLENISH);
+		if (task_has_dl_policy(rq->curr))
+			check_preempt_curr_dl(rq, p, 0);
+		else
+			resched_task(rq->curr);
+#ifdef CONFIG_SMP
+		/*
+		 * Queueing this task back might have overloaded rq,
+		 * check if we need to kick someone away.
+		 */
+		if (has_pushable_dl_tasks(rq))
+			push_dl_task(rq);
+#endif
+	}
+unlock:
+	raw_spin_unlock(&rq->lock);
+
+	return HRTIMER_NORESTART;
+}
+
+void init_dl_task_timer(struct sched_dl_entity *dl_se)
+{
+	struct hrtimer *timer = &dl_se->dl_timer;
+
+	if (hrtimer_active(timer)) {
+		hrtimer_try_to_cancel(timer);
+		return;
+	}
+
+	hrtimer_init(timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	timer->function = dl_task_timer;
+}
+
+static
+int dl_runtime_exceeded(struct rq *rq, struct sched_dl_entity *dl_se)
+{
+	int dmiss = dl_time_before(dl_se->deadline, rq->clock);
+	int rorun = dl_se->runtime <= 0;
+
+	if (!rorun && !dmiss)
+		return 0;
+
+	/*
+	 * Record statistics about last and maximum deadline
+	 * misses and runtime overruns.
+	 */
+	if (dmiss) {
+		u64 damount = rq->clock - dl_se->deadline;
+
+		schedstat_set(dl_se->stats.last_dmiss, damount);
+		schedstat_set(dl_se->stats.dmiss_max,
+			      max(dl_se->stats.dmiss_max, damount));
+	}
+	if (rorun) {
+		u64 ramount = -dl_se->runtime;
+
+		schedstat_set(dl_se->stats.last_rorun, ramount);
+		schedstat_set(dl_se->stats.rorun_max,
+			      max(dl_se->stats.rorun_max, ramount));
+	}
+
+	/*
+	 * If we are beyond our current deadline and we are still
+	 * executing, then we have already used some of the runtime of
+	 * the next instance. Thus, if we do not account that, we are
+	 * stealing bandwidth from the system at each deadline miss!
+	 */
+	if (dmiss) {
+		dl_se->runtime = rorun ? dl_se->runtime : 0;
+		dl_se->runtime -= rq->clock - dl_se->deadline;
+	}
+
+	return 1;
+}
+
+/*
+ * Update the current task's runtime statistics (provided it is still
+ * a -deadline task and has not been removed from the dl_rq).
+ */
+static void update_curr_dl(struct rq *rq)
+{
+	struct task_struct *curr = rq->curr;
+	struct sched_dl_entity *dl_se = &curr->dl;
+	u64 delta_exec;
+
+	if (!dl_task(curr) || !on_dl_rq(dl_se))
+		return;
+
+	/*
+	 * Consumed budget is computed considering the time as
+	 * observed by schedulable tasks (excluding time spent
+	 * in hardirq context, etc.)
+	 */
+	delta_exec = rq->clock_task - curr->se.exec_start;
+	if (unlikely((s64)delta_exec < 0))
+		delta_exec = 0;
+
+	schedstat_set(curr->se.statistics.exec_max,
+		      max(curr->se.statistics.exec_max, delta_exec));
+
+	curr->se.sum_exec_runtime += delta_exec;
+	schedstat_add(&rq->dl, exec_clock, delta_exec);
+	account_group_exec_runtime(curr, delta_exec);
+
+	curr->se.exec_start = rq->clock_task;
+	cpuacct_charge(curr, delta_exec);
+
+	sched_rt_avg_update(rq, delta_exec);
+
+	dl_se->runtime -= delta_exec;
+	if (dl_runtime_exceeded(rq, dl_se)) {
+		__dequeue_task_dl(rq, curr, 0);
+		if (likely(start_dl_timer(dl_se, curr->dl.dl_boosted)))
+			dl_se->dl_throttled = 1;
+		else
+			enqueue_task_dl(rq, curr, ENQUEUE_REPLENISH);
+
+		if (!is_leftmost(curr, &rq->dl))
+			resched_task(curr);
+	}
+}
+
+#ifdef CONFIG_SMP
+
+static struct task_struct *pick_next_earliest_dl_task(struct rq *rq, int cpu);
+
+static inline u64 next_deadline(struct rq *rq)
+{
+	struct task_struct *next = pick_next_earliest_dl_task(rq, rq->cpu);
+
+	if (next && dl_prio(next->prio))
+		return next->dl.deadline;
+	else
+		return 0;
+}
+
+static void inc_dl_deadline(struct dl_rq *dl_rq, u64 deadline)
+{
+	struct rq *rq = rq_of_dl_rq(dl_rq);
+
+	if (dl_rq->earliest_dl.curr == 0 ||
+	    dl_time_before(deadline, dl_rq->earliest_dl.curr)) {
+		/*
+		 * If the dl_rq had no -deadline tasks, or if the new task
+		 * has shorter deadline than the current one on dl_rq, we
+		 * know that the previous earliest becomes our next earliest,
+		 * as the new task becomes the earliest itself.
+		 */
+		dl_rq->earliest_dl.next = dl_rq->earliest_dl.curr;
+		dl_rq->earliest_dl.curr = deadline;
+		cpudl_set(&rq->rd->cpudl, rq->cpu, deadline, 1);
+	} else if (dl_rq->earliest_dl.next == 0 ||
+		   dl_time_before(deadline, dl_rq->earliest_dl.next)) {
+		/*
+		 * On the other hand, if the new -deadline task has a
+		 * a later deadline than the earliest one on dl_rq, but
+		 * it is earlier than the next (if any), we must
+		 * recompute the next-earliest.
+		 */
+		dl_rq->earliest_dl.next = next_deadline(rq);
+	}
+}
+
+static void dec_dl_deadline(struct dl_rq *dl_rq, u64 deadline)
+{
+	struct rq *rq = rq_of_dl_rq(dl_rq);
+
+	/*
+	 * Since we may have removed our earliest (and/or next earliest)
+	 * task we must recompute them.
+	 */
+	if (!dl_rq->dl_nr_running) {
+		dl_rq->earliest_dl.curr = 0;
+		dl_rq->earliest_dl.next = 0;
+		cpudl_set(&rq->rd->cpudl, rq->cpu, 0, 0);
+	} else {
+		struct rb_node *leftmost = dl_rq->rb_leftmost;
+		struct sched_dl_entity *entry;
+
+		entry = rb_entry(leftmost, struct sched_dl_entity, rb_node);
+		dl_rq->earliest_dl.curr = entry->deadline;
+		dl_rq->earliest_dl.next = next_deadline(rq);
+		cpudl_set(&rq->rd->cpudl, rq->cpu, entry->deadline, 1);
+	}
+}
+
+#else
+
+static inline void inc_dl_deadline(struct dl_rq *dl_rq, u64 deadline) {}
+static inline void dec_dl_deadline(struct dl_rq *dl_rq, u64 deadline) {}
+
+#endif /* CONFIG_SMP */
+
+static inline
+void inc_dl_tasks(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)
+{
+	int prio = dl_task_of(dl_se)->prio;
+	u64 deadline = dl_se->deadline;
+
+	WARN_ON(!dl_prio(prio));
+	dl_rq->dl_nr_running++;
+
+	inc_dl_deadline(dl_rq, deadline);
+	inc_dl_migration(dl_se, dl_rq);
+}
+
+static inline
+void dec_dl_tasks(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)
+{
+	int prio = dl_task_of(dl_se)->prio;
+
+	WARN_ON(!dl_prio(prio));
+	WARN_ON(!dl_rq->dl_nr_running);
+	dl_rq->dl_nr_running--;
+
+	dec_dl_deadline(dl_rq, dl_se->deadline);
+	dec_dl_migration(dl_se, dl_rq);
+}
+
+static void __enqueue_dl_entity(struct sched_dl_entity *dl_se)
+{
+	struct dl_rq *dl_rq = dl_rq_of_se(dl_se);
+	struct rb_node **link = &dl_rq->rb_root.rb_node;
+	struct rb_node *parent = NULL;
+	struct sched_dl_entity *entry;
+	int leftmost = 1;
+
+	BUG_ON(!RB_EMPTY_NODE(&dl_se->rb_node));
+
+	while (*link) {
+		parent = *link;
+		entry = rb_entry(parent, struct sched_dl_entity, rb_node);
+		if (dl_time_before(dl_se->deadline, entry->deadline))
+			link = &parent->rb_left;
+		else {
+			link = &parent->rb_right;
+			leftmost = 0;
+		}
+	}
+
+	if (leftmost)
+		dl_rq->rb_leftmost = &dl_se->rb_node;
+
+	rb_link_node(&dl_se->rb_node, parent, link);
+	rb_insert_color(&dl_se->rb_node, &dl_rq->rb_root);
+
+	inc_dl_tasks(dl_se, dl_rq);
+}
+
+static void __dequeue_dl_entity(struct sched_dl_entity *dl_se)
+{
+	struct dl_rq *dl_rq = dl_rq_of_se(dl_se);
+
+	if (RB_EMPTY_NODE(&dl_se->rb_node))
+		return;
+
+	if (dl_rq->rb_leftmost == &dl_se->rb_node) {
+		struct rb_node *next_node;
+
+		next_node = rb_next(&dl_se->rb_node);
+		dl_rq->rb_leftmost = next_node;
+	}
+
+	rb_erase(&dl_se->rb_node, &dl_rq->rb_root);
+	RB_CLEAR_NODE(&dl_se->rb_node);
+
+	dec_dl_tasks(dl_se, dl_rq);
+}
+
+static void
+enqueue_dl_entity(struct sched_dl_entity *dl_se,
+		  struct sched_dl_entity *pi_se, int flags)
+{
+	BUG_ON(on_dl_rq(dl_se));
+
+	/*
+	 * If this is a wakeup or a new instance, the scheduling
+	 * parameters of the task might need updating. Otherwise,
+	 * we want a replenishment of its runtime.
+	 */
+	if (!dl_se->dl_new && flags & ENQUEUE_REPLENISH)
+		replenish_dl_entity(dl_se, pi_se);
+	else
+		update_dl_entity(dl_se, pi_se);
+
+	__enqueue_dl_entity(dl_se);
+}
+
+static void dequeue_dl_entity(struct sched_dl_entity *dl_se)
+{
+	__dequeue_dl_entity(dl_se);
+}
+
+static void enqueue_task_dl(struct rq *rq, struct task_struct *p, int flags)
+{
+	struct task_struct *pi_task = p->pi_top_task;
+	struct sched_dl_entity *pi_se = &p->dl;
+
+	/*
+	 * Use the scheduling parameters of the top pi-waiter
+	 * task if we have one and its (relative) deadline is
+	 * smaller than our one... OTW we keep our runtime and
+	 * deadline.
+	 */
+	if (pi_task && p->dl.dl_boosted && dl_prio(pi_task->normal_prio))
+		pi_se = &pi_task->dl;
+
+	/*
+	 * If p is throttled, we do nothing. In fact, if it exhausted
+	 * its budget it needs a replenishment and, since it now is on
+	 * its rq, the bandwidth timer callback (which clearly has not
+	 * run yet) will take care of this.
+	 */
+	if (p->dl.dl_throttled)
+		return;
+
+	enqueue_dl_entity(&p->dl, pi_se, flags);
+
+	if (!task_current(rq, p) && p->nr_cpus_allowed > 1)
+		enqueue_pushable_dl_task(rq, p);
+
+	inc_nr_running(rq);
+}
+
+static void __dequeue_task_dl(struct rq *rq, struct task_struct *p, int flags)
+{
+	dequeue_dl_entity(&p->dl);
+	dequeue_pushable_dl_task(rq, p);
+}
+
+static void dequeue_task_dl(struct rq *rq, struct task_struct *p, int flags)
+{
+	update_curr_dl(rq);
+	__dequeue_task_dl(rq, p, flags);
+
+	dec_nr_running(rq);
+}
+
+/*
+ * Yield task semantic for -deadline tasks is:
+ *
+ *   get off from the CPU until our next instance, with
+ *   a new runtime.
+ */
+static void yield_task_dl(struct rq *rq)
+{
+	struct task_struct *p = rq->curr;
+
+	/*
+	 * We make the task go to sleep until its current deadline by
+	 * forcing its runtime to zero. This way, update_curr_dl() stops
+	 * it and the bandwidth timer will wake it up and will give it
+	 * new scheduling parameters (thanks to dl_new=1).
+	 */
+	if (p->dl.runtime > 0) {
+		rq->curr->dl.dl_new = 1;
+		p->dl.runtime = 0;
+	}
+	update_curr_dl(rq);
+}
+
+#ifdef CONFIG_SMP
+
+static int find_later_rq(struct task_struct *task);
+
+static int
+select_task_rq_dl(struct task_struct *p, int sd_flag, int flags)
+{
+	struct task_struct *curr;
+	struct rq *rq;
+	int cpu;
+
+	cpu = task_cpu(p);
+
+	if (sd_flag != SD_BALANCE_WAKE && sd_flag != SD_BALANCE_FORK)
+		goto out;
+
+	rq = cpu_rq(cpu);
+
+	rcu_read_lock();
+	curr = ACCESS_ONCE(rq->curr); /* unlocked access */
+
+	/*
+	 * If we are dealing with a -deadline task, we must
+	 * decide where to wake it up.
+	 * If it has a later deadline and the current task
+	 * on this rq can't move (provided the waking task
+	 * can!) we prefer to send it somewhere else. On the
+	 * other hand, if it has a shorter deadline, we
+	 * try to make it stay here, it might be important.
+	 */
+	if (unlikely(dl_task(curr)) &&
+	    (curr->nr_cpus_allowed < 2 ||
+	     !dl_entity_preempt(&p->dl, &curr->dl)) &&
+	    (p->nr_cpus_allowed > 1)) {
+		int target = find_later_rq(p);
+
+		if (target != -1)
+			cpu = target;
+	}
+	rcu_read_unlock();
+
+out:
+	return cpu;
+}
+
+static void check_preempt_equal_dl(struct rq *rq, struct task_struct *p)
+{
+	/*
+	 * Current can't be migrated, useless to reschedule,
+	 * let's hope p can move out.
+	 */
+	if (rq->curr->nr_cpus_allowed == 1 ||
+	    cpudl_find(&rq->rd->cpudl, rq->curr, NULL) == -1)
+		return;
+
+	/*
+	 * p is migratable, so let's not schedule it and
+	 * see if it is pushed or pulled somewhere else.
+	 */
+	if (p->nr_cpus_allowed != 1 &&
+	    cpudl_find(&rq->rd->cpudl, p, NULL) != -1)
+		return;
+
+	resched_task(rq->curr);
+}
+
+#endif /* CONFIG_SMP */
+
+/*
+ * Only called when both the current and waking task are -deadline
+ * tasks.
+ */
+static void check_preempt_curr_dl(struct rq *rq, struct task_struct *p,
+				  int flags)
+{
+	if (dl_entity_preempt(&p->dl, &rq->curr->dl)) {
+		resched_task(rq->curr);
+		return;
+	}
+
+#ifdef CONFIG_SMP
+	/*
+	 * In the unlikely case current and p have the same deadline
+	 * let us try to decide what's the best thing to do...
+	 */
+	if ((p->dl.deadline == rq->curr->dl.deadline) &&
+	    !test_tsk_need_resched(rq->curr))
+		check_preempt_equal_dl(rq, p);
+#endif /* CONFIG_SMP */
+}
+
+#ifdef CONFIG_SCHED_HRTICK
+static void start_hrtick_dl(struct rq *rq, struct task_struct *p)
+{
+	s64 delta = p->dl.dl_runtime - p->dl.runtime;
+
+	if (delta > 10000)
+		hrtick_start(rq, delta);
+}
+#else
+static void start_hrtick_dl(struct rq *rq, struct task_struct *p)
+{
+}
+#endif
+
+#ifdef CONFIG_SCHED_DEBUG
+struct sched_dl_entity *__pick_dl_last_entity(struct dl_rq *dl_rq)
+{
+	struct rb_node *last = rb_last(&dl_rq->rb_root);
+
+	if (!last)
+		return NULL;
+
+	return rb_entry(last, struct sched_dl_entity, rb_node);
+}
+#endif /* CONFIG_SCHED_DEBUG */
+
+static struct sched_dl_entity *pick_next_dl_entity(struct rq *rq,
+						   struct dl_rq *dl_rq)
+{
+	struct rb_node *left = dl_rq->rb_leftmost;
+
+	if (!left)
+		return NULL;
+
+	return rb_entry(left, struct sched_dl_entity, rb_node);
+}
+
+struct task_struct *pick_next_task_dl(struct rq *rq)
+{
+	struct sched_dl_entity *dl_se;
+	struct task_struct *p;
+	struct dl_rq *dl_rq;
+
+	dl_rq = &rq->dl;
+
+	if (unlikely(!dl_rq->dl_nr_running))
+		return NULL;
+
+	dl_se = pick_next_dl_entity(rq, dl_rq);
+	BUG_ON(!dl_se);
+
+	p = dl_task_of(dl_se);
+	p->se.exec_start = rq->clock_task;
+
+	/* Running task will never be pushed. */
+	if (p)
+		dequeue_pushable_dl_task(rq, p);
+
+#ifdef CONFIG_SCHED_HRTICK
+	if (hrtick_enabled(rq))
+		start_hrtick_dl(rq, p);
+#endif
+
+#ifdef CONFIG_SMP
+	rq->post_schedule = has_pushable_dl_tasks(rq);
+#endif /* CONFIG_SMP */
+
+	return p;
+}
+
+static void put_prev_task_dl(struct rq *rq, struct task_struct *p)
+{
+	update_curr_dl(rq);
+
+	if (on_dl_rq(&p->dl) && p->nr_cpus_allowed > 1)
+		enqueue_pushable_dl_task(rq, p);
+}
+
+static void task_tick_dl(struct rq *rq, struct task_struct *p, int queued)
+{
+	update_curr_dl(rq);
+
+#ifdef CONFIG_SCHED_HRTICK
+	if (hrtick_enabled(rq) && queued && p->dl.runtime > 0)
+		start_hrtick_dl(rq, p);
+#endif
+}
+
+static void task_fork_dl(struct task_struct *p)
+{
+	/*
+	 * SCHED_DEADLINE tasks cannot fork and this is achieved through
+	 * sched_fork()
+	 */
+}
+
+static void task_dead_dl(struct task_struct *p)
+{
+	struct hrtimer *timer = &p->dl.dl_timer;
+#ifdef CONFIG_SMP
+	struct dl_bw *dl_b = &task_rq(p)->rd->dl_bw;
+#else
+	struct dl_bw *dl_b = &task_rq(p)->dl.dl_bw;
+#endif
+
+	/*
+	 * Since we are TASK_DEAD we won't slip out of the domain!
+	 */
+	raw_spin_lock_irq(&dl_b->lock);
+	dl_b->total_bw -= p->dl.dl_bw;
+	raw_spin_unlock_irq(&dl_b->lock);
+
+	hrtimer_cancel(timer);
+}
+
+static void set_curr_task_dl(struct rq *rq)
+{
+	struct task_struct *p = rq->curr;
+
+	p->se.exec_start = rq->clock_task;
+
+	/* You can't push away the running task */
+	dequeue_pushable_dl_task(rq, p);
+}
+
+#ifdef CONFIG_SMP
+
+/* Only try algorithms three times */
+#define DL_MAX_TRIES 3
+
+static int pick_dl_task(struct rq *rq, struct task_struct *p, int cpu)
+{
+	if (!task_running(rq, p) &&
+	    (cpu < 0 || cpumask_test_cpu(cpu, &p->cpus_allowed)) &&
+	    (p->nr_cpus_allowed > 1))
+		return 1;
+
+	return 0;
+}
+
+/* Returns the second earliest -deadline task, NULL otherwise */
+static struct task_struct *pick_next_earliest_dl_task(struct rq *rq, int cpu)
+{
+	struct rb_node *next_node = rq->dl.rb_leftmost;
+	struct sched_dl_entity *dl_se;
+	struct task_struct *p = NULL;
+
+next_node:
+	next_node = rb_next(next_node);
+	if (next_node) {
+		dl_se = rb_entry(next_node, struct sched_dl_entity, rb_node);
+		p = dl_task_of(dl_se);
+
+		if (pick_dl_task(rq, p, cpu))
+			return p;
+
+		goto next_node;
+	}
+
+	return NULL;
+}
+
+static DEFINE_PER_CPU(cpumask_var_t, local_cpu_mask_dl);
+
+static int find_later_rq(struct task_struct *task)
+{
+	struct sched_domain *sd;
+	struct cpumask *later_mask = __get_cpu_var(local_cpu_mask_dl);
+	int this_cpu = smp_processor_id();
+	int best_cpu, cpu = task_cpu(task);
+
+	/* Make sure the mask is initialized first */
+	if (unlikely(!later_mask))
+		return -1;
+
+	if (task->nr_cpus_allowed == 1)
+		return -1;
+
+	best_cpu = cpudl_find(&task_rq(task)->rd->cpudl,
+			task, later_mask);
+	if (best_cpu == -1)
+		return -1;
+
+	/*
+	 * If we are here, some target has been found,
+	 * the most suitable of which is cached in best_cpu.
+	 * This is, among the runqueues where the current tasks
+	 * have later deadlines than the task's one, the rq
+	 * with the latest possible one.
+	 *
+	 * Now we check how well this matches with task's
+	 * affinity and system topology.
+	 *
+	 * The last cpu where the task run is our first
+	 * guess, since it is most likely cache-hot there.
+	 */
+	if (cpumask_test_cpu(cpu, later_mask))
+		return cpu;
+	/*
+	 * Check if this_cpu is to be skipped (i.e., it is
+	 * not in the mask) or not.
+	 */
+	if (!cpumask_test_cpu(this_cpu, later_mask))
+		this_cpu = -1;
+
+	rcu_read_lock();
+	for_each_domain(cpu, sd) {
+		if (sd->flags & SD_WAKE_AFFINE) {
+
+			/*
+			 * If possible, preempting this_cpu is
+			 * cheaper than migrating.
+			 */
+			if (this_cpu != -1 &&
+			    cpumask_test_cpu(this_cpu, sched_domain_span(sd)))
+				return this_cpu;
+
+			/*
+			 * Last chance: if best_cpu is valid and is
+			 * in the mask, that becomes our choice.
+			 */
+			if (best_cpu < nr_cpu_ids &&
+			    cpumask_test_cpu(best_cpu, sched_domain_span(sd)))
+				return best_cpu;
+		}
+	}
+	rcu_read_unlock();
+
+	/*
+	 * At this point, all our guesses failed, we just return
+	 * 'something', and let the caller sort the things out.
+	 */
+	if (this_cpu != -1)
+		return this_cpu;
+
+	cpu = cpumask_any(later_mask);
+	if (cpu < nr_cpu_ids)
+		return cpu;
+
+	return -1;
+}
+
+/* Locks the rq it finds */
+static struct rq *find_lock_later_rq(struct task_struct *task, struct rq *rq)
+{
+	struct rq *later_rq = NULL;
+	int tries;
+	int cpu;
+
+	for (tries = 0; tries < DL_MAX_TRIES; tries++) {
+		cpu = find_later_rq(task);
+
+		if ((cpu == -1) || (cpu == rq->cpu))
+			break;
+
+		later_rq = cpu_rq(cpu);
+
+		/* Retry if something changed. */
+		if (double_lock_balance(rq, later_rq)) {
+			if (unlikely(task_rq(task) != rq ||
+				     !cpumask_test_cpu(later_rq->cpu,
+						       &task->cpus_allowed) ||
+				     task_running(rq, task) ||
+				     !task->on_rq)) {
+				double_unlock_balance(rq, later_rq);
+				later_rq = NULL;
+				break;
+			}
+		}
+
+		/*
+		 * If the rq we found has no -deadline task, or
+		 * its earliest one has a later deadline than our
+		 * task, the rq is a good one.
+		 */
+		if (!later_rq->dl.dl_nr_running ||
+		    dl_time_before(task->dl.deadline,
+				   later_rq->dl.earliest_dl.curr))
+			break;
+
+		/* Otherwise we try again. */
+		double_unlock_balance(rq, later_rq);
+		later_rq = NULL;
+	}
+
+	return later_rq;
+}
+
+static struct task_struct *pick_next_pushable_dl_task(struct rq *rq)
+{
+	struct task_struct *p;
+
+	if (!has_pushable_dl_tasks(rq))
+		return NULL;
+
+	p = rb_entry(rq->dl.pushable_dl_tasks_leftmost,
+		     struct task_struct, pushable_dl_tasks);
+
+	BUG_ON(rq->cpu != task_cpu(p));
+	BUG_ON(task_current(rq, p));
+	BUG_ON(p->nr_cpus_allowed <= 1);
+
+	BUG_ON(!p->on_rq);
+	BUG_ON(!dl_task(p));
+
+	return p;
+}
+
+/*
+ * See if the non running -deadline tasks on this rq
+ * can be sent to some other CPU where they can preempt
+ * and start executing.
+ */
+static int push_dl_task(struct rq *rq)
+{
+	struct task_struct *next_task;
+	struct rq *later_rq;
+
+	if (!rq->dl.overloaded)
+		return 0;
+
+	next_task = pick_next_pushable_dl_task(rq);
+	if (!next_task)
+		return 0;
+
+retry:
+	if (unlikely(next_task == rq->curr)) {
+		WARN_ON(1);
+		return 0;
+	}
+
+	/*
+	 * If next_task preempts rq->curr, and rq->curr
+	 * can move away, it makes sense to just reschedule
+	 * without going further in pushing next_task.
+	 */
+	if (dl_task(rq->curr) &&
+	    dl_time_before(next_task->dl.deadline, rq->curr->dl.deadline) &&
+	    rq->curr->nr_cpus_allowed > 1) {
+		resched_task(rq->curr);
+		return 0;
+	}
+
+	/* We might release rq lock */
+	get_task_struct(next_task);
+
+	/* Will lock the rq it'll find */
+	later_rq = find_lock_later_rq(next_task, rq);
+	if (!later_rq) {
+		struct task_struct *task;
+
+		/*
+		 * We must check all this again, since
+		 * find_lock_later_rq releases rq->lock and it is
+		 * then possible that next_task has migrated.
+		 */
+		task = pick_next_pushable_dl_task(rq);
+		if (task_cpu(next_task) == rq->cpu && task == next_task) {
+			/*
+			 * The task is still there. We don't try
+			 * again, some other cpu will pull it when ready.
+			 */
+			dequeue_pushable_dl_task(rq, next_task);
+			goto out;
+		}
+
+		if (!task)
+			/* No more tasks */
+			goto out;
+
+		put_task_struct(next_task);
+		next_task = task;
+		goto retry;
+	}
+
+	deactivate_task(rq, next_task, 0);
+	set_task_cpu(next_task, later_rq->cpu);
+	activate_task(later_rq, next_task, 0);
+
+	resched_task(later_rq->curr);
+
+	double_unlock_balance(rq, later_rq);
+
+out:
+	put_task_struct(next_task);
+
+	return 1;
+}
+
+static void push_dl_tasks(struct rq *rq)
+{
+	/* Terminates as it moves a -deadline task */
+	while (push_dl_task(rq))
+		;
+}
+
+static int pull_dl_task(struct rq *this_rq)
+{
+	int this_cpu = this_rq->cpu, ret = 0, cpu;
+	struct task_struct *p;
+	struct rq *src_rq;
+	u64 dmin = LONG_MAX;
+
+	if (likely(!dl_overloaded(this_rq)))
+		return 0;
+
+	for_each_cpu(cpu, this_rq->rd->dlo_mask) {
+		if (this_cpu == cpu)
+			continue;
+
+		src_rq = cpu_rq(cpu);
+
+		/*
+		 * It looks racy, abd it is! However, as in sched_rt.c,
+		 * we are fine with this.
+		 */
+		if (this_rq->dl.dl_nr_running &&
+		    dl_time_before(this_rq->dl.earliest_dl.curr,
+				   src_rq->dl.earliest_dl.next))
+			continue;
+
+		/* Might drop this_rq->lock */
+		double_lock_balance(this_rq, src_rq);
+
+		/*
+		 * If there are no more pullable tasks on the
+		 * rq, we're done with it.
+		 */
+		if (src_rq->dl.dl_nr_running <= 1)
+			goto skip;
+
+		p = pick_next_earliest_dl_task(src_rq, this_cpu);
+
+		/*
+		 * We found a task to be pulled if:
+		 *  - it preempts our current (if there's one),
+		 *  - it will preempt the last one we pulled (if any).
+		 */
+		if (p && dl_time_before(p->dl.deadline, dmin) &&
+		    (!this_rq->dl.dl_nr_running ||
+		     dl_time_before(p->dl.deadline,
+				    this_rq->dl.earliest_dl.curr))) {
+			WARN_ON(p == src_rq->curr);
+			WARN_ON(!p->on_rq);
+
+			/*
+			 * Then we pull iff p has actually an earlier
+			 * deadline than the current task of its runqueue.
+			 */
+			if (dl_time_before(p->dl.deadline,
+					   src_rq->curr->dl.deadline))
+				goto skip;
+
+			ret = 1;
+
+			deactivate_task(src_rq, p, 0);
+			set_task_cpu(p, this_cpu);
+			activate_task(this_rq, p, 0);
+			dmin = p->dl.deadline;
+
+			/* Is there any other task even earlier? */
+		}
+skip:
+		double_unlock_balance(this_rq, src_rq);
+	}
+
+	return ret;
+}
+
+static void pre_schedule_dl(struct rq *rq, struct task_struct *prev)
+{
+	/* Try to pull other tasks here */
+	if (dl_task(prev))
+		pull_dl_task(rq);
+}
+
+static void post_schedule_dl(struct rq *rq)
+{
+	push_dl_tasks(rq);
+}
+
+/*
+ * Since the task is not running and a reschedule is not going to happen
+ * anytime soon on its runqueue, we try pushing it away now.
+ */
+static void task_woken_dl(struct rq *rq, struct task_struct *p)
+{
+	if (!task_running(rq, p) &&
+	    !test_tsk_need_resched(rq->curr) &&
+	    has_pushable_dl_tasks(rq) &&
+	    p->nr_cpus_allowed > 1 &&
+	    dl_task(rq->curr) &&
+	    (rq->curr->nr_cpus_allowed < 2 ||
+	     dl_entity_preempt(&rq->curr->dl, &p->dl))) {
+		push_dl_tasks(rq);
+	}
+}
+
+static void set_cpus_allowed_dl(struct task_struct *p,
+				const struct cpumask *new_mask)
+{
+	struct rq *rq;
+	int weight;
+
+	BUG_ON(!dl_task(p));
+
+	/*
+	 * Update only if the task is actually running (i.e.,
+	 * it is on the rq AND it is not throttled).
+	 */
+	if (!on_dl_rq(&p->dl))
+		return;
+
+	weight = cpumask_weight(new_mask);
+
+	/*
+	 * Only update if the process changes its state from whether it
+	 * can migrate or not.
+	 */
+	if ((p->nr_cpus_allowed > 1) == (weight > 1))
+		return;
+
+	rq = task_rq(p);
+
+	/*
+	 * The process used to be able to migrate OR it can now migrate
+	 */
+	if (weight <= 1) {
+		if (!task_current(rq, p))
+			dequeue_pushable_dl_task(rq, p);
+		BUG_ON(!rq->dl.dl_nr_migratory);
+		rq->dl.dl_nr_migratory--;
+	} else {
+		if (!task_current(rq, p))
+			enqueue_pushable_dl_task(rq, p);
+		rq->dl.dl_nr_migratory++;
+	}
+	
+	update_dl_migration(&rq->dl);
+}
+
+/* Assumes rq->lock is held */
+static void rq_online_dl(struct rq *rq)
+{
+	if (rq->dl.overloaded)
+		dl_set_overload(rq);
+
+	if (rq->dl.dl_nr_running > 0)
+		cpudl_set(&rq->rd->cpudl, rq->cpu, rq->dl.earliest_dl.curr, 1);
+}
+
+/* Assumes rq->lock is held */
+static void rq_offline_dl(struct rq *rq)
+{
+	if (rq->dl.overloaded)
+		dl_clear_overload(rq);
+
+	cpudl_set(&rq->rd->cpudl, rq->cpu, 0, 0);
+}
+
+void init_sched_dl_class(void)
+{
+	unsigned int i;
+
+	for_each_possible_cpu(i)
+		zalloc_cpumask_var_node(&per_cpu(local_cpu_mask_dl, i),
+					GFP_KERNEL, cpu_to_node(i));
+}
+
+#endif /* CONFIG_SMP */
+
+static void switched_from_dl(struct rq *rq, struct task_struct *p)
+{
+	if (hrtimer_active(&p->dl.dl_timer) && !dl_policy(p->policy))
+		hrtimer_try_to_cancel(&p->dl.dl_timer);
+
+#ifdef CONFIG_SMP
+	/*
+	 * Since this might be the only -deadline task on the rq,
+	 * this is the right place to try to pull some other one
+	 * from an overloaded cpu, if any.
+	 */
+	if (!rq->dl.dl_nr_running)
+		pull_dl_task(rq);
+#endif
+}
+
+/*
+ * When switching to -deadline, we may overload the rq, then
+ * we try to push someone off, if possible.
+ */
+static void switched_to_dl(struct rq *rq, struct task_struct *p)
+{
+	int check_resched = 1;
+
+	/*
+	 * If p is throttled, don't consider the possibility
+	 * of preempting rq->curr, the check will be done right
+	 * after its runtime will get replenished.
+	 */
+	if (unlikely(p->dl.dl_throttled))
+		return;
+
+	if (!p->on_rq || rq->curr != p) {
+#ifdef CONFIG_SMP
+		if (rq->dl.overloaded && push_dl_task(rq) && rq != task_rq(p))
+			/* Only reschedule if pushing failed */
+			check_resched = 0;
+#endif /* CONFIG_SMP */
+		if (check_resched && task_has_dl_policy(rq->curr))
+			check_preempt_curr_dl(rq, p, 0);
+	}
+}
+
+/*
+ * If the scheduling parameters of a -deadline task changed,
+ * a push or pull operation might be needed.
+ */
+static void prio_changed_dl(struct rq *rq, struct task_struct *p,
+			    int oldprio)
+{
+	if (p->on_rq || rq->curr == p) {
+#ifdef CONFIG_SMP
+		/*
+		 * This might be too much, but unfortunately
+		 * we don't have the old deadline value, and
+		 * we can't argue if the task is increasing
+		 * or lowering its prio, so...
+		 */
+		if (!rq->dl.overloaded)
+			pull_dl_task(rq);
+
+		/*
+		 * If we now have a earlier deadline task than p,
+		 * then reschedule, provided p is still on this
+		 * runqueue.
+		 */
+		if (dl_time_before(rq->dl.earliest_dl.curr, p->dl.deadline) &&
+		    rq->curr == p)
+			resched_task(p);
+#else
+		/*
+		 * Again, we don't know if p has a earlier
+		 * or later deadline, so let's blindly set a
+		 * (maybe not needed) rescheduling point.
+		 */
+		resched_task(p);
+#endif /* CONFIG_SMP */
+	} else
+		switched_to_dl(rq, p);
+}
+
+const struct sched_class dl_sched_class = {
+	.next			= &rt_sched_class,
+	.enqueue_task		= enqueue_task_dl,
+	.dequeue_task		= dequeue_task_dl,
+	.yield_task		= yield_task_dl,
+
+	.check_preempt_curr	= check_preempt_curr_dl,
+
+	.pick_next_task		= pick_next_task_dl,
+	.put_prev_task		= put_prev_task_dl,
+
+#ifdef CONFIG_SMP
+	.select_task_rq		= select_task_rq_dl,
+
+	.set_cpus_allowed       = set_cpus_allowed_dl,
+	.rq_online              = rq_online_dl,
+	.rq_offline             = rq_offline_dl,
+	.pre_schedule		= pre_schedule_dl,
+	.post_schedule		= post_schedule_dl,
+	.task_woken		= task_woken_dl,
+#endif
+
+	.set_curr_task		= set_curr_task_dl,
+	.task_tick		= task_tick_dl,
+	.task_fork              = task_fork_dl,
+	.task_dead		= task_dead_dl,
+
+	.prio_changed           = prio_changed_dl,
+	.switched_from		= switched_from_dl,
+	.switched_to		= switched_to_dl,
+};
+
+#ifdef CONFIG_SCHED_DEBUG
+extern void print_dl_rq(struct seq_file *m, int cpu, struct dl_rq *dl_rq);
+
+void print_dl_stats(struct seq_file *m, int cpu)
+{
+	struct dl_rq *dl_rq = &cpu_rq(cpu)->dl;
+
+	rcu_read_lock();
+	print_dl_rq(m, cpu, dl_rq);
+	rcu_read_unlock();
+}
+#endif /* CONFIG_SCHED_DEBUG */
diff -Nurp linux/kernel/sched/debug.c sched-deadline-mainline-dl/kernel/sched/debug.c
--- linux/kernel/sched/debug.c	2014-02-17 14:46:04.688907551 -0500
+++ sched-deadline-mainline-dl/kernel/sched/debug.c	2012-11-21 15:11:30.000000000 -0500
@@ -242,6 +242,45 @@ void print_rt_rq(struct seq_file *m, int
 #undef P
 }
 
+extern struct sched_dl_entity *__pick_dl_last_entity(struct dl_rq *dl_rq);
+extern void print_dl_stats(struct seq_file *m, int cpu);
+
+void print_dl_rq(struct seq_file *m, int cpu, struct dl_rq *dl_rq)
+{
+	s64 min_deadline = -1, max_deadline = -1;
+	struct rq *rq = cpu_rq(cpu);
+	struct sched_dl_entity *last;
+	unsigned long flags;
+
+	SEQ_printf(m, "\ndl_rq[%d]:\n", cpu);
+
+	raw_spin_lock_irqsave(&rq->lock, flags);
+	if (dl_rq->rb_leftmost)
+		min_deadline = (rb_entry(dl_rq->rb_leftmost,
+					 struct sched_dl_entity,
+					 rb_node))->deadline;
+	last = __pick_dl_last_entity(dl_rq);
+	if (last)
+		max_deadline = last->deadline;
+	raw_spin_unlock_irqrestore(&rq->lock, flags);
+
+#define P(x) \
+	SEQ_printf(m, "  .%-30s: %Ld\n", #x, (long long)(dl_rq->x))
+#define __PN(x) \
+	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", #x, SPLIT_NS(x))
+#define PN(x) \
+	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", #x, SPLIT_NS(dl_rq->x))
+
+	P(dl_nr_running);
+	PN(exec_clock);
+	__PN(min_deadline);
+	__PN(max_deadline);
+
+#undef PN
+#undef __PN
+#undef P
+}
+
 extern __read_mostly int sched_clock_running;
 
 static void print_cpu(struct seq_file *m, int cpu)
@@ -309,6 +348,7 @@ do {									\
 	spin_lock_irqsave(&sched_debug_lock, flags);
 	print_cfs_stats(m, cpu);
 	print_rt_stats(m, cpu);
+	print_dl_stats(m, cpu);
 
 	rcu_read_lock();
 	print_rq(m, rq, cpu);
@@ -460,6 +500,12 @@ void proc_sched_show_task(struct task_st
 	P(se.statistics.nr_wakeups_affine_attempts);
 	P(se.statistics.nr_wakeups_passive);
 	P(se.statistics.nr_wakeups_idle);
+	if (dl_task(p)) {
+		PN(dl.stats.last_dmiss);
+		PN(dl.stats.dmiss_max);
+		PN(dl.stats.last_rorun);
+		PN(dl.stats.rorun_max);
+	}
 
 	{
 		u64 avg_atom, avg_per_cpu;
diff -Nurp linux/kernel/sched/Makefile sched-deadline-mainline-dl/kernel/sched/Makefile
--- linux/kernel/sched/Makefile	2014-02-17 14:46:04.510903184 -0500
+++ sched-deadline-mainline-dl/kernel/sched/Makefile	2012-11-21 15:11:30.000000000 -0500
@@ -11,8 +11,8 @@ ifneq ($(CONFIG_SCHED_OMIT_FRAME_POINTER
 CFLAGS_core.o := $(PROFILING) -fno-omit-frame-pointer
 endif
 
-obj-y += core.o clock.o cputime.o idle_task.o fair.o rt.o stop_task.o
-obj-$(CONFIG_SMP) += cpupri.o
+obj-y += core.o clock.o cputime.o idle_task.o fair.o rt.o deadline.o stop_task.o
+obj-$(CONFIG_SMP) += cpupri.o cpudeadline.o
 obj-$(CONFIG_SCHED_AUTOGROUP) += auto_group.o
 obj-$(CONFIG_SCHEDSTATS) += stats.o
 obj-$(CONFIG_SCHED_DEBUG) += debug.o
diff -Nurp linux/kernel/sched/rt.c sched-deadline-mainline-dl/kernel/sched/rt.c
--- linux/kernel/sched/rt.c	2014-02-17 14:46:05.086917316 -0500
+++ sched-deadline-mainline-dl/kernel/sched/rt.c	2012-11-21 15:11:30.000000000 -0500
@@ -1809,7 +1809,7 @@ static void task_woken_rt(struct rq *rq,
 	    !test_tsk_need_resched(rq->curr) &&
 	    has_pushable_tasks(rq) &&
 	    p->nr_cpus_allowed > 1 &&
-	    rt_task(rq->curr) &&
+	    (dl_task(rq->curr) || rt_task(rq->curr)) &&
 	    (rq->curr->nr_cpus_allowed < 2 ||
 	     rq->curr->prio <= p->prio))
 		push_rt_tasks(rq);
diff -Nurp linux/kernel/sched/sched.h sched-deadline-mainline-dl/kernel/sched/sched.h
--- linux/kernel/sched/sched.h	2014-02-17 14:46:05.087917341 -0500
+++ sched-deadline-mainline-dl/kernel/sched/sched.h	2012-11-21 15:11:30.000000000 -0500
@@ -5,6 +5,7 @@
 #include <linux/stop_machine.h>
 
 #include "cpupri.h"
+#include "cpudeadline.h"
 
 extern __read_mostly int scheduler_running;
 
@@ -43,6 +44,13 @@ extern __read_mostly int scheduler_runni
  */
 #define RUNTIME_INF	((u64)~0ULL)
 
+/*
+ * Single value that decides SCHED_DEADLINE internal math precision.
+ * 10 -> just above 1us
+ * 9  -> just above 0.5us
+ */
+#define DL_SCALE (10)
+
 static inline int rt_policy(int policy)
 {
 	if (policy == SCHED_FIFO || policy == SCHED_RR)
@@ -50,11 +58,37 @@ static inline int rt_policy(int policy)
 	return 0;
 }
 
+static inline int dl_policy(int policy)
+{
+	if (unlikely(policy == SCHED_DEADLINE))
+		return 1;
+	return 0;
+}
+
 static inline int task_has_rt_policy(struct task_struct *p)
 {
 	return rt_policy(p->policy);
 }
 
+static inline int task_has_dl_policy(struct task_struct *p)
+{
+	return dl_policy(p->policy);
+}
+
+static inline bool dl_time_before(u64 a, u64 b)
+{
+	return (s64)(a - b) < 0;
+}
+
+/*
+ * Tells if entity @a should preempt entity @b.
+ */
+static inline bool
+dl_entity_preempt(struct sched_dl_entity *a, struct sched_dl_entity *b)
+{
+	return dl_time_before(a->deadline, b->deadline);
+}
+
 /*
  * This is the priority-queue data structure of the RT scheduling class:
  */
@@ -70,6 +104,49 @@ struct rt_bandwidth {
 	u64			rt_runtime;
 	struct hrtimer		rt_period_timer;
 };
+/*
+ * To keep the bandwidth of -deadline tasks under control we need some
+ * place where:
+ *  - store the maximum -deadline bandwidth of the system;
+ *  - cache the fraction of that bandwidth that is currently allocated.
+ *
+ * This is all done in the data structure below. It is similar to the
+ * one used for RT-throttling (rt_bandwidth), with the main difference
+ * that, since here we are only interested in admission control, we
+ * do not decrease any runtime while the task "executes", neither we
+ * need a timer to replenish it.
+ *
+ * With respect to SMP, the bandwidth is given on a per-CPU basis,
+ * meaning that:
+ *  - dl_bw (< 100%) is the bandwidth of the system on each CPU;
+ *  - dl_total_bw array contains, in the i-eth element, the currently
+ *    allocated bandwidth on the i-eth CPU.
+ * Moreover, groups consume bandwidth on each CPU, while tasks only
+ * consume bandwidth on the CPU they're running on.
+ * Finally, dl_total_bw_cpu is used to cache the index of dl_total_bw
+ * that will be shown the next time the proc or cgroup controls will
+ * be red. It on its turn can be changed by writing on its own
+ * control.
+ */
+struct dl_bandwidth {
+	raw_spinlock_t dl_runtime_lock;
+	u64 dl_runtime;
+};
+
+static inline int dl_bandwidth_enabled(void)
+{
+	return sysctl_sched_dl_runtime >= 0;
+}
+
+struct dl_bw {
+	raw_spinlock_t lock;
+	/* default value */
+	u64 bw;
+	/* allocated */
+	u64 total_bw;
+};
+
+static inline u64 global_dl_runtime(void);
 
 extern struct mutex sched_domains_mutex;
 
@@ -309,6 +386,49 @@ struct rt_rq {
 #endif
 };
 
+/* Deadline class' related fields in a runqueue */
+struct dl_rq {
+	/* runqueue is an rbtree, ordered by deadline */
+	struct rb_root rb_root;
+	struct rb_node *rb_leftmost;
+
+	unsigned long dl_nr_running;
+
+	u64 exec_clock;
+
+#ifdef CONFIG_SMP
+	/*
+	 * Deadline values of the currently executing and the
+	 * earliest ready task on this rq. Caching these facilitates
+	 * the decision wether or not a ready but not running task
+	 * should migrate somewhere else.
+	 */
+	struct {
+		u64 curr;
+		u64 next;
+	} earliest_dl;
+
+	unsigned long dl_nr_migratory;
+	unsigned long dl_nr_total;
+	int overloaded;
+
+	/*
+	 * Tasks on this rq that can be pushed away. They are kept in
+	 * an rb-tree, ordered by tasks' deadlines, with caching
+	 * of the leftmost (earliest deadline) element.
+	 */
+	struct rb_root pushable_dl_tasks_root;
+	struct rb_node *pushable_dl_tasks_leftmost;
+#else
+	struct dl_bw dl_bw;
+#endif
+};
+
+#ifdef CONFIG_SCHED_DEBUG
+struct sched_dl_entity *__pick_dl_last_entity(struct dl_rq *dl_rq);
+void print_dl_stats(struct seq_file *m, int cpu);
+#endif
+
 #ifdef CONFIG_SMP
 
 /*
@@ -327,6 +447,15 @@ struct root_domain {
 	cpumask_var_t online;
 
 	/*
+	 * The bit corresponding to a CPU gets set here if such CPU has more
+	 * than one runnable -deadline task (as it is below for RT tasks).
+	 */
+	cpumask_var_t dlo_mask;
+	atomic_t dlo_count;
+	struct dl_bw dl_bw;
+	struct cpudl cpudl;
+
+	/*
 	 * The "RT overload" flag: it gets set if a CPU has more than
 	 * one runnable RT task.
 	 */
@@ -370,6 +499,7 @@ struct rq {
 
 	struct cfs_rq cfs;
 	struct rt_rq rt;
+	struct dl_rq dl;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* list of leaf cfs_rq on this cpu: */
@@ -661,7 +791,13 @@ static inline u64 global_rt_runtime(void
 	return (u64)sysctl_sched_rt_runtime * NSEC_PER_USEC;
 }
 
+static inline u64 global_dl_runtime(void)
+{
+	if (sysctl_sched_dl_runtime < 0)
+		return RUNTIME_INF;
 
+	return (u64)sysctl_sched_dl_runtime * NSEC_PER_USEC;
+}
 
 static inline int task_current(struct rq *rq, struct task_struct *p)
 {
@@ -841,6 +977,7 @@ enum cpuacct_stat_index {
    for (class = sched_class_highest; class; class = class->next)
 
 extern const struct sched_class stop_sched_class;
+extern const struct sched_class dl_sched_class;
 extern const struct sched_class rt_sched_class;
 extern const struct sched_class fair_sched_class;
 extern const struct sched_class idle_sched_class;
@@ -866,6 +1003,7 @@ extern void update_group_power(struct sc
 extern int update_runtime(struct notifier_block *nfb, unsigned long action, void *hcpu);
 extern void init_sched_rt_class(void);
 extern void init_sched_fair_class(void);
+extern void init_sched_dl_class(void);
 
 extern void resched_task(struct task_struct *p);
 extern void resched_cpu(int cpu);
@@ -873,6 +1011,12 @@ extern void resched_cpu(int cpu);
 extern struct rt_bandwidth def_rt_bandwidth;
 extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime);
 
+extern struct dl_bandwidth def_dl_bandwidth;
+extern void init_dl_bandwidth(struct dl_bandwidth *dl_b, u64 period, u64 runtime);
+extern void init_dl_task_timer(struct sched_dl_entity *dl_se);
+
+unsigned long to_ratio(u64 period, u64 runtime);
+
 extern void update_idle_cpu_load(struct rq *this_rq);
 
 #ifdef CONFIG_CGROUP_CPUACCT
@@ -1151,6 +1295,7 @@ extern void print_rt_stats(struct seq_fi
 
 extern void init_cfs_rq(struct cfs_rq *cfs_rq);
 extern void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq);
+extern void init_dl_rq(struct dl_rq *rt_rq, struct rq *rq);
 
 extern void account_cfs_bandwidth_used(int enabled, int was_enabled);
 
diff -Nurp linux/kernel/sched/stop_task.c sched-deadline-mainline-dl/kernel/sched/stop_task.c
--- linux/kernel/sched/stop_task.c	2014-02-17 14:46:05.137918567 -0500
+++ sched-deadline-mainline-dl/kernel/sched/stop_task.c	2012-11-21 15:11:30.000000000 -0500
@@ -103,7 +103,7 @@ get_rr_interval_stop(struct rq *rq, stru
  * Simple, special scheduling class for the per-CPU stop tasks:
  */
 const struct sched_class stop_sched_class = {
-	.next			= &rt_sched_class,
+	.next			= &dl_sched_class,
 
 	.enqueue_task		= enqueue_task_stop,
 	.dequeue_task		= dequeue_task_stop,
diff -Nurp linux/kernel/sysctl.c sched-deadline-mainline-dl/kernel/sysctl.c
--- linux/kernel/sysctl.c	2014-02-17 14:46:05.392924824 -0500
+++ sched-deadline-mainline-dl/kernel/sysctl.c	2012-11-21 15:11:30.000000000 -0500
@@ -362,6 +362,13 @@ static struct ctl_table kern_table[] = {
 		.mode		= 0644,
 		.proc_handler	= sched_rt_handler,
 	},
+	{
+		.procname	= "sched_dl_runtime_us",
+		.data		= &sysctl_sched_dl_runtime,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= sched_dl_handler,
+	},
 #ifdef CONFIG_SCHED_AUTOGROUP
 	{
 		.procname	= "sched_autogroup_enabled",
diff -Nurp linux/kernel/trace/trace_sched_wakeup.c sched-deadline-mainline-dl/kernel/trace/trace_sched_wakeup.c
--- linux/kernel/trace/trace_sched_wakeup.c	2014-02-17 14:46:06.546953136 -0500
+++ sched-deadline-mainline-dl/kernel/trace/trace_sched_wakeup.c	2012-11-21 15:11:30.000000000 -0500
@@ -27,6 +27,7 @@ static int			wakeup_cpu;
 static int			wakeup_current_cpu;
 static unsigned			wakeup_prio = -1;
 static int			wakeup_rt;
+static int			wakeup_dl;
 
 static arch_spinlock_t wakeup_lock =
 	(arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;
@@ -429,9 +430,17 @@ probe_wakeup(void *ignore, struct task_s
 	tracing_record_cmdline(p);
 	tracing_record_cmdline(current);
 
-	if ((wakeup_rt && !rt_task(p)) ||
-			p->prio >= wakeup_prio ||
-			p->prio >= current->prio)
+	/*
+	 * Semantic is like this:
+	 *  - wakeup tracer handles all tasks in the system, independently
+	 *    from their scheduling class;
+	 *  - wakeup_rt tracer handles tasks belonging to sched_dl and
+	 *    sched_rt class;
+	 *  - wakeup_dl handles tasks belonging to sched_dl class only.
+	 */
+	if ((wakeup_dl && !dl_task(p)) ||
+	    (wakeup_rt && !dl_task(p) && !rt_task(p)) ||
+	    (p->prio >= wakeup_prio || p->prio >= current->prio))
 		return;
 
 	pc = preempt_count();
@@ -443,7 +452,7 @@ probe_wakeup(void *ignore, struct task_s
 	arch_spin_lock(&wakeup_lock);
 
 	/* check for races. */
-	if (!tracer_enabled || p->prio >= wakeup_prio)
+	if (!tracer_enabled || (!dl_task(p) && p->prio >= wakeup_prio))
 		goto out_locked;
 
 	/* reset the trace */
@@ -551,16 +560,25 @@ static int __wakeup_tracer_init(struct t
 
 static int wakeup_tracer_init(struct trace_array *tr)
 {
+	wakeup_dl = 0;
 	wakeup_rt = 0;
 	return __wakeup_tracer_init(tr);
 }
 
 static int wakeup_rt_tracer_init(struct trace_array *tr)
 {
+	wakeup_dl = 0;
 	wakeup_rt = 1;
 	return __wakeup_tracer_init(tr);
 }
 
+static int wakeup_dl_tracer_init(struct trace_array *tr)
+{
+	wakeup_dl = 1;
+	wakeup_rt = 0;
+	return __wakeup_tracer_init(tr);
+}
+
 static void wakeup_tracer_reset(struct trace_array *tr)
 {
 	stop_wakeup_tracer(tr);
@@ -623,6 +641,20 @@ static struct tracer wakeup_rt_tracer __
 	.use_max_tr	= 1,
 };
 
+static struct tracer wakeup_dl_tracer __read_mostly =
+{
+	.name		= "wakeup_dl",
+	.init		= wakeup_dl_tracer_init,
+	.reset		= wakeup_tracer_reset,
+	.start		= wakeup_tracer_start,
+	.stop		= wakeup_tracer_stop,
+	.wait_pipe	= poll_wait_pipe,
+	.print_max	= 1,
+#ifdef CONFIG_FTRACE_SELFTEST
+	.selftest    = trace_selftest_startup_wakeup,
+#endif
+};
+
 __init static int init_wakeup_tracer(void)
 {
 	int ret;
@@ -635,6 +667,10 @@ __init static int init_wakeup_tracer(voi
 	if (ret)
 		return ret;
 
+	ret = register_tracer(&wakeup_dl_tracer);
+	if (ret)
+		return ret;
+
 	return 0;
 }
 device_initcall(init_wakeup_tracer);
diff -Nurp linux/kernel/trace/trace_selftest.c sched-deadline-mainline-dl/kernel/trace/trace_selftest.c
--- linux/kernel/trace/trace_selftest.c	2014-02-17 14:46:06.546953136 -0500
+++ sched-deadline-mainline-dl/kernel/trace/trace_selftest.c	2012-11-21 15:11:30.000000000 -0500
@@ -1028,11 +1028,17 @@ trace_selftest_startup_nop(struct tracer
 #ifdef CONFIG_SCHED_TRACER
 static int trace_wakeup_test_thread(void *data)
 {
-	/* Make this a RT thread, doesn't need to be too high */
-	static const struct sched_param param = { .sched_priority = 5 };
+	/* Make this a -deadline thread */
+	struct sched_param2 paramx = {
+		.sched_priority = 0,
+		.sched_runtime = 100000ULL,
+		.sched_deadline = 10000000ULL,
+		.sched_period = 10000000ULL
+		.sched_flags = 0
+	};
 	struct completion *x = data;
 
-	sched_setscheduler(current, SCHED_FIFO, &param);
+	sched_setscheduler2(current, SCHED_DEADLINE, &paramx);
 
 	/* Make it know we have a new prio */
 	complete(x);
@@ -1046,8 +1052,8 @@ static int trace_wakeup_test_thread(void
 	/* we are awake, now wait to disappear */
 	while (!kthread_should_stop()) {
 		/*
-		 * This is an RT task, do short sleeps to let
-		 * others run.
+		 * This will likely be the system top priority
+		 * task, do short sleeps to let others run.
 		 */
 		msleep(100);
 	}
@@ -1060,21 +1066,21 @@ trace_selftest_startup_wakeup(struct tra
 {
 	unsigned long save_max = tracing_max_latency;
 	struct task_struct *p;
-	struct completion isrt;
+	struct completion is_ready;
 	unsigned long count;
 	int ret;
 
-	init_completion(&isrt);
+	init_completion(&is_ready);
 
-	/* create a high prio thread */
-	p = kthread_run(trace_wakeup_test_thread, &isrt, "ftrace-test");
+	/* create a -deadline thread */
+	p = kthread_run(trace_wakeup_test_thread, &is_ready, "ftrace-test");
 	if (IS_ERR(p)) {
 		printk(KERN_CONT "Failed to create ftrace wakeup test thread ");
 		return -1;
 	}
 
-	/* make sure the thread is running at an RT prio */
-	wait_for_completion(&isrt);
+	/* make sure the thread is running at -deadline policy */
+	wait_for_completion(&is_ready);
 
 	/* start the tracing */
 	ret = tracer_init(trace, tr);
diff -Nurp linux/lib/Makefile sched-deadline-mainline-dl/lib/Makefile
--- linux/lib/Makefile	2014-02-17 14:46:07.056965648 -0500
+++ sched-deadline-mainline-dl/lib/Makefile	2012-11-21 15:11:30.000000000 -0500
@@ -12,7 +12,7 @@ lib-y := ctype.o string.o vsprintf.o cmd
 	 idr.o int_sqrt.o extable.o \
 	 sha1.o md5.o irq_regs.o reciprocal_div.o argv_split.o \
 	 proportions.o flex_proportions.o prio_heap.o ratelimit.o show_mem.o \
-	 is_single_threaded.o plist.o decompress.o
+	 is_single_threaded.o plist.o decompress.o math128.o
 
 lib-$(CONFIG_MMU) += ioremap.o
 lib-$(CONFIG_SMP) += cpumask.o
diff -Nurp linux/lib/math128.c sched-deadline-mainline-dl/lib/math128.c
--- linux/lib/math128.c	1969-12-31 19:00:00.000000000 -0500
+++ sched-deadline-mainline-dl/lib/math128.c	2012-11-21 15:11:30.000000000 -0500
@@ -0,0 +1,40 @@
+#include <linux/math128.h>
+
+#ifndef mul_u64_u64
+/*
+ * a * b = (ah * 2^32 + al) * (bh * 2^32 + bl) =
+ *   ah*bh * 2^64 + (ah*bl + bh*al) * 2^32 + al*bl
+ */
+u128 mul_u64_u64(u64 a, u64 b)
+{
+	u128 t1, t2, t3, t4;
+	u32 ah, al;
+	u32 bh, bl;
+
+	ah = a >> 32;
+	al = a;
+
+	bh = b >> 32;
+	bl = b;
+
+	t1.lo = 0;
+	t1.hi = (u64)ah * bh;
+
+	t2.lo = (u64)ah * bl;
+	t2.hi = t2.lo >> 32;
+	t2.lo <<= 32;
+
+	t3.lo = (u64)al * bh;
+	t3.hi = t3.lo >> 32;
+	t3.lo <<= 32;
+
+	t4.lo = (u64)al * bl;
+	t4.hi = 0;
+
+	t1 = add_u128(t1, t2);
+	t1 = add_u128(t1, t3);
+	t1 = add_u128(t1, t4);
+
+	return t1;
+}
+#endif /* mul_u64_u64 */
